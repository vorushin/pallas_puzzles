{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697f541d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vorushin/pallas_puzzles/blob/master/ragged_dot.ipynb?flush_caches=true\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Pallas Puzzles: Ragged Dot\n",
    "\n",
    "**9 progressive puzzles** building toward a production **ragged_dot** kernel\n",
    "for Mixture-of-Experts. You'll implement scalar prefetch, group metadata,\n",
    "masked stores, and full grouped matmul — the core of MoE dispatch on TPU.\n",
    "\n",
    "Every puzzle runs on **CPU** via `interpret=True` — no TPU needed.\n",
    "\n",
    "**Prerequisites**: Complete **basics.py** first (Pallas foundations and\n",
    "tiled matmul patterns).\n",
    "\n",
    "**Key Pallas docs**: https://docs.jax.dev/en/latest/pallas/index.html\n",
    "\n",
    "| Part | Puzzles | Focus |\n",
    "|------|---------|-------|\n",
    "| III — Scalar Prefetch | 1–5 | Runtime index maps, group metadata, masking |\n",
    "| IV — Ragged Dot | 6–9 | Grouped matmul, tgmm, pipelining |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fad43",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Collapse this section (› arrow next to the heading), then ▶ **Run all cells\n",
    "in section** from the menu to get everything ready in one click."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jax jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb01209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "print(f\"JAX {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5e153",
   "metadata": {},
   "source": [
    "---\n",
    "# Part III: Scalar Prefetch & Group Metadata (Puzzles 1–5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b009da",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 1: Scalar Prefetch — Permuted Batched Matmul\n",
    "\n",
    "**Goal**: Implement a **permuted batched matmul** where the mapping from\n",
    "output group → rhs group is determined at runtime by a permutation array.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In ragged_dot, the tile-to-group mapping is computed at runtime (from\n",
    "`group_sizes`). Standard `BlockSpec` index maps only see grid indices —\n",
    "they can't access runtime arrays.\n",
    "\n",
    "**Scalar prefetch** solves this. With `PrefetchScalarGridSpec`:\n",
    "- Small arrays are loaded into **SMEM** (scalar memory) before the kernel\n",
    "- Index maps receive these SMEM refs as extra arguments\n",
    "- The kernel also receives them as leading arguments\n",
    "\n",
    "```python\n",
    "PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=1,  # first 1 arg is scalar-prefetched\n",
    "    in_specs=[...],\n",
    "    out_specs=...,\n",
    "    grid=(...),\n",
    ")\n",
    "```\n",
    "\n",
    "Index map signature becomes: `lambda grid_idx0, ..., *prefetch_refs: (...)`\n",
    "\n",
    "The kernel signature becomes: `kernel(prefetch_ref0, ..., in_ref0, ..., out_ref, *scratch)`\n",
    "\n",
    "We skip teaching plain `GridSpec` as a separate concept — the simpler\n",
    "`grid=` kwarg to `pallas_call` (used in basics.py Puzzles 1–10) handles the basic\n",
    "case. `PrefetchScalarGridSpec` is introduced now because it's what\n",
    "production kernels use.\n",
    "\n",
    "**Note**: From this point on, puzzles use `grid_spec=` instead of\n",
    "`grid=` for `PrefetchScalarGridSpec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6ba6f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "G = 4\n",
    "M, K, N = 64, 64, 64\n",
    "\n",
    "# --- Reference ---\n",
    "def permuted_matmul_spec(lhs, rhs, perm):\n",
    "    \"\"\"lhs: (G, M, K), rhs: (G, K, N), perm: (G,) → (G, M, N)\n",
    "    out[i] = lhs[i] @ rhs[perm[i]]\n",
    "    \"\"\"\n",
    "    return jnp.stack([lhs[i] @ rhs[perm[i]] for i in range(G)])\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def permuted_matmul_kernel(perm_ref, lhs_ref, rhs_ref, o_ref):\n",
    "    # perm_ref: scalar-prefetched permutation array (in SMEM)\n",
    "    # lhs_ref: (M, K) — current group's lhs\n",
    "    # rhs_ref: (K, N) — permuted group's rhs (loaded via index map)\n",
    "    # o_ref: (M, N) — output tile\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "# --- Index maps ---\n",
    "def lhs_index_map(g, perm_ref):\n",
    "    return (g, 0, 0)\n",
    "\n",
    "def rhs_index_map(g, perm_ref):\n",
    "    # Use the scalar-prefetched perm to look up which rhs group to load\n",
    "    return (perm_ref[g], 0, 0)\n",
    "\n",
    "def out_index_map(g, perm_ref):\n",
    "    return (g, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec2222",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "lhs = jax.random.normal(jax.random.key(14), (G, M, K))\n",
    "rhs = jax.random.normal(jax.random.key(15), (G, K, N))\n",
    "perm = jnp.array([2, 0, 3, 1], dtype=jnp.int32)  # permutation\n",
    "\n",
    "expected = permuted_matmul_spec(lhs, rhs, perm)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    permuted_matmul_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=1,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((None, M, K), lhs_index_map),\n",
    "            pl.BlockSpec((None, K, N), rhs_index_map),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((None, M, N), out_index_map),\n",
    "        grid=(G,),\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((G, M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(perm, lhs, rhs)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    max_err = float(jnp.max(jnp.abs(actual - expected)))\n",
    "    print(f\"FAILED ✗  max error: {max_err:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655e243",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Approach</summary>\n",
    "\n",
    "The index maps handle the permutation using `perm_ref[g]`. By the time the kernel runs, `rhs_ref` already points to the correct permuted group. So the kernel body is identical to basics.py Puzzle 9 — just `lhs_ref[...] @ rhs_ref[...]`.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "o_ref[...] = lhs_ref[...] @ rhs_ref[...]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88def514",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 2: Group Metadata — CSR-style Tile Mapping\n",
    "\n",
    "**Goal**: Implement the `make_group_metadata` function that computes\n",
    "the tile-to-group mapping for ragged_dot. This is **pure JAX** — not a\n",
    "kernel puzzle.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In ragged_dot, `lhs` has shape `(M, K)` where rows are divided into `G`\n",
    "groups of variable sizes. We need to figure out which **tiles** belong to\n",
    "which **groups**.\n",
    "\n",
    "Given `group_sizes = [300, 212, 512]` with `bm = 128`:\n",
    "\n",
    "```\n",
    "Row:  0              300    512         1024\n",
    "      ├── group 0 ───┤├─ g1 ─┤├── group 2 ──┤\n",
    "\n",
    "Tiles (bm=128):\n",
    "      [  0  ][ 128 ][ 256 ][ 384 ][ 512 ][ 640 ][ 768 ][ 896 ]\n",
    "      ├─g0──┤├─g0──┤├g0/g1┤├─g1──┤├─g2──┤├─g2──┤├─g2──┤├─g2──┤\n",
    "                     ^ partial tile: visited by BOTH g0 and g1\n",
    "```\n",
    "\n",
    "Tile at row 256 straddles the group boundary. It gets visited **twice**:\n",
    "once for group 0 (rows 256-299 are valid) and once for group 1 (rows\n",
    "300-383 are valid). The kernel uses a **mask** to only store the valid\n",
    "rows for each visit.\n",
    "\n",
    "**Rule of thumb**: `num_tiles = tiles_m + (number of non-aligned group\n",
    "boundaries)`. Aligned boundaries don't cause extra visits.\n",
    "\n",
    "**Output arrays**:\n",
    "- `group_offsets`: `[0, 300, 512, 1024]` — cumsum with leading 0\n",
    "- `group_ids`: maps each grid index → group id\n",
    "- `m_tile_ids`: maps each grid index → which m-tile to process\n",
    "- `num_tiles`: total number of grid iterations needed\n",
    "\n",
    "The arrays can be longer than `num_tiles` (padded with the last group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_group_metadata_reference(group_sizes, m, bm):\n",
    "    \"\"\"Simple reference implementation — O(m) but correct.\"\"\"\n",
    "    num_groups = len(group_sizes)\n",
    "    group_offsets = jnp.concatenate([jnp.array([0]), jnp.cumsum(group_sizes)])\n",
    "\n",
    "    # Assign each row to a group\n",
    "    row_to_group = jnp.zeros(m, dtype=jnp.int32)\n",
    "    for g in range(num_groups):\n",
    "        start = int(group_offsets[g])\n",
    "        end = int(group_offsets[g + 1])\n",
    "        row_to_group = row_to_group.at[start:end].set(g)\n",
    "\n",
    "    # Assign each tile to group(s)\n",
    "    tiles_m = m // bm\n",
    "    group_ids_list = []\n",
    "    m_tile_ids_list = []\n",
    "\n",
    "    for t in range(tiles_m):\n",
    "        tile_start = t * bm\n",
    "        tile_end = (t + 1) * bm\n",
    "        # Which groups touch this tile?\n",
    "        groups_in_tile = jnp.unique(row_to_group[tile_start:tile_end])\n",
    "        for g in groups_in_tile:\n",
    "            group_ids_list.append(int(g))\n",
    "            m_tile_ids_list.append(t)\n",
    "\n",
    "    num_tiles = len(group_ids_list)\n",
    "\n",
    "    # Pad to max possible length\n",
    "    max_len = tiles_m + num_groups - 1\n",
    "    group_ids = jnp.zeros(max_len, dtype=jnp.int32)\n",
    "    m_tile_ids = jnp.zeros(max_len, dtype=jnp.int32)\n",
    "    group_ids = group_ids.at[:num_tiles].set(jnp.array(group_ids_list, dtype=jnp.int32))\n",
    "    m_tile_ids = m_tile_ids.at[:num_tiles].set(jnp.array(m_tile_ids_list, dtype=jnp.int32))\n",
    "    # Pad remainder with last values\n",
    "    if num_tiles < max_len:\n",
    "        group_ids = group_ids.at[num_tiles:].set(group_ids_list[-1])\n",
    "        m_tile_ids = m_tile_ids.at[num_tiles:].set(m_tile_ids_list[-1])\n",
    "\n",
    "    return (group_offsets.astype(jnp.int32), group_ids, m_tile_ids), num_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761cc024",
   "metadata": {},
   "source": [
    "### Your implementation — decomposed into 5 testable steps\n",
    "\n",
    "We break `make_group_metadata` into independent functions,\n",
    "each tested before combining them.\n",
    "\n",
    "### Step 2a: Group Offsets\n",
    "\n",
    "**Goal**: Compute CSR-style prefix sum `[0, cumsum(group_sizes)]`.\n",
    "\n",
    "```\n",
    "group_sizes = [300, 212, 512]\n",
    "group_offsets = [0, 300, 512, 1024]\n",
    "                 ^    ^    ^     ^\n",
    "                 g0   g1   g2   end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04772649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_offsets(group_sizes):\n",
    "    \"\"\"[0, cumsum(group_sizes)] — maps group id → start row.\n",
    "\n",
    "    Args:\n",
    "        group_sizes: (G,) int32\n",
    "    Returns:\n",
    "        (G+1,) int32\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca08c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "assert jnp.array_equal(\n",
    "    compute_group_offsets(jnp.array([256, 256, 256, 256], dtype=jnp.int32)),\n",
    "    jnp.array([0, 256, 512, 768, 1024], dtype=jnp.int32))\n",
    "assert jnp.array_equal(\n",
    "    compute_group_offsets(jnp.array([300, 212, 512], dtype=jnp.int32)),\n",
    "    jnp.array([0, 300, 512, 1024], dtype=jnp.int32))\n",
    "assert jnp.array_equal(\n",
    "    compute_group_offsets(jnp.array([512, 0, 512], dtype=jnp.int32)),\n",
    "    jnp.array([0, 512, 512, 1024], dtype=jnp.int32))\n",
    "print(\"Step 2a — compute_group_offsets: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28674578",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "return jnp.concatenate([jnp.zeros(1, dtype=jnp.int32), jnp.cumsum(group_sizes)])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437fdf98",
   "metadata": {},
   "source": [
    "### Step 2b: Tiles per Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_tiles(group_sizes, group_offsets, bm):\n",
    "    \"\"\"Number of tile visits per group (boundary tiles counted by both neighbors).\n",
    "\n",
    "    Args:\n",
    "        group_sizes: (G,) int32\n",
    "        group_offsets: (G+1,) int32 from compute_group_offsets\n",
    "        bm: tile size\n",
    "    Returns:\n",
    "        (G,) int32\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Extract group starts and ends from offsets\n",
    "    # 2. Round starts DOWN and ends UP to tile boundaries\n",
    "    # 3. Handle zero-size groups\n",
    "    # 4. Convert rounded range sizes to tile counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b58cb3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "assert jnp.array_equal(\n",
    "    compute_group_tiles(jnp.array([256, 256, 256, 256], dtype=jnp.int32),\n",
    "                        jnp.array([0, 256, 512, 768, 1024], dtype=jnp.int32), 128),\n",
    "    jnp.array([2, 2, 2, 2]))\n",
    "assert jnp.array_equal(\n",
    "    compute_group_tiles(jnp.array([300, 212, 512], dtype=jnp.int32),\n",
    "                        jnp.array([0, 300, 512, 1024], dtype=jnp.int32), 128),\n",
    "    jnp.array([3, 2, 4]))\n",
    "assert jnp.array_equal(\n",
    "    compute_group_tiles(jnp.array([512, 0, 512], dtype=jnp.int32),\n",
    "                        jnp.array([0, 512, 512, 1024], dtype=jnp.int32), 128),\n",
    "    jnp.array([4, 0, 4]))\n",
    "assert jnp.array_equal(\n",
    "    compute_group_tiles(jnp.array([300, 0, 724], dtype=jnp.int32),\n",
    "                        jnp.array([0, 300, 300, 1024], dtype=jnp.int32), 128),\n",
    "    jnp.array([3, 0, 6]))\n",
    "print(\"Step 2b — compute_group_tiles: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5c26c",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group_starts = group_offsets[:-1]\n",
    "group_ends = group_offsets[1:]\n",
    "rounded_starts = (group_starts // bm * bm).astype(jnp.int32)\n",
    "rounded_ends = ((group_ends + bm - 1) // bm * bm).astype(jnp.int32)\n",
    "rounded_sizes = jnp.where(group_sizes == 0, 0, rounded_ends - rounded_starts)\n",
    "return rounded_sizes // bm\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf24ae0",
   "metadata": {},
   "source": [
    "### Step 2c: Group IDs\n",
    "\n",
    "```\n",
    "group_tiles = [3, 2, 4]  →  group_ids = [0,0,0, 1,1, 2,2,2,2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_ids(group_tiles, num_groups, max_len):\n",
    "    \"\"\"Flat array mapping grid index → group id.\n",
    "\n",
    "    Args:\n",
    "        group_tiles: (G,) int32 from compute_group_tiles\n",
    "        num_groups: G\n",
    "        max_len: output array length (padded)\n",
    "    Returns:\n",
    "        (max_len,) int32\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4231d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "assert compute_group_ids(jnp.array([2, 2, 2, 2]), 4, 11)[:8].tolist() == [0, 0, 1, 1, 2, 2, 3, 3]\n",
    "assert compute_group_ids(jnp.array([3, 2, 4]), 3, 10)[:9].tolist() == [0, 0, 0, 1, 1, 2, 2, 2, 2]\n",
    "print(\"Step 2c — compute_group_ids: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfc8a6",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "return jnp.repeat(\n",
    "    jnp.arange(num_groups, dtype=jnp.int32),\n",
    "    group_tiles,\n",
    "    total_repeat_length=max_len,\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68aaa01",
   "metadata": {},
   "source": [
    "### Step 2d: Tile Visits\n",
    "\n",
    "```\n",
    "group_offsets = [0, 300, 512, 1024],  bm = 128\n",
    "Group 1 starts at row 300 → inside tile 2 → extra visit\n",
    "tile_visits = [1, 1, 2, 1, 1, 1, 1, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ffc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tile_visits(group_sizes, group_offsets, tiles_m, bm):\n",
    "    \"\"\"Visit count per tile (1 + extra for each mid-tile group boundary).\n",
    "\n",
    "    Every tile is visited at least once. When a group boundary falls\n",
    "    in the MIDDLE of a tile (not aligned to bm), that tile gets an\n",
    "    extra visit. We need to count how many non-aligned boundaries\n",
    "    land in each tile.\n",
    "\n",
    "    Args:\n",
    "        group_sizes: (G,) int32\n",
    "        group_offsets: (G+1,) int32\n",
    "        tiles_m: M // bm\n",
    "        bm: tile size\n",
    "    Returns:\n",
    "        (tiles_m,) int32\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Find group start positions (from offsets, skip the leading 0)\n",
    "    # 2. Identify which starts are non-aligned (start % bm != 0)\n",
    "    #    AND belong to non-empty groups\n",
    "    # 3. For non-aligned starts, compute which tile they land in (start // bm)\n",
    "    # 4. Count how many non-aligned boundaries per tile (jnp.histogram)\n",
    "    # 5. Result = 1 + extra_visits_per_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4dae1f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "assert compute_tile_visits(\n",
    "    jnp.array([256, 256, 256, 256], dtype=jnp.int32),\n",
    "    jnp.array([0, 256, 512, 768, 1024], dtype=jnp.int32), 8, 128\n",
    ").tolist() == [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([300, 212, 512], dtype=jnp.int32),\n",
    "    jnp.array([0, 300, 512, 1024], dtype=jnp.int32), 8, 128\n",
    ").tolist() == [1, 1, 2, 1, 1, 1, 1, 1]\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([512, 0, 512], dtype=jnp.int32),\n",
    "    jnp.array([0, 512, 512, 1024], dtype=jnp.int32), 8, 128\n",
    ").tolist() == [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([300, 0, 724], dtype=jnp.int32),\n",
    "    jnp.array([0, 300, 300, 1024], dtype=jnp.int32), 8, 128\n",
    ").tolist() == [1, 1, 2, 1, 1, 1, 1, 1]\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([300, 212, 512], dtype=jnp.int32),\n",
    "    jnp.array([0, 300, 512, 1024], dtype=jnp.int32), 8, 128\n",
    ").dtype == jnp.int32, \"compute_tile_visits must return int32, not float32\"\n",
    "print(\"Step 2d — compute_tile_visits: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8810086",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group_starts = group_offsets[:-1]\n",
    "aligned_or_empty = ((group_starts % bm) == 0) | (group_sizes == 0)\n",
    "partial_tile_ids = jnp.where(aligned_or_empty, tiles_m + 1, group_starts // bm)\n",
    "extra_visits = jnp.histogram(\n",
    "    partial_tile_ids, bins=tiles_m, range=(0, tiles_m)\n",
    ")[0]\n",
    "return (extra_visits + 1).astype(jnp.int32)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cb173",
   "metadata": {},
   "source": [
    "### Step 2e: M-tile IDs\n",
    "\n",
    "```\n",
    "tile_visits = [1,1,2,1,1,1,1,1]  →  m_tile_ids = [0,1,2,2,3,4,5,6,7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_m_tile_ids(tile_visits, tiles_m, max_len):\n",
    "    \"\"\"Flat array mapping grid index → m-tile id.\n",
    "\n",
    "    Args:\n",
    "        tile_visits: (tiles_m,) int32 from compute_tile_visits\n",
    "        tiles_m: M // bm\n",
    "        max_len: output array length (padded)\n",
    "    Returns:\n",
    "        (max_len,) int32\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f502491",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "assert compute_m_tile_ids(jnp.array([1,1,1,1,1,1,1,1]), 8, 11)[:8].tolist() == [0,1,2,3,4,5,6,7]\n",
    "assert compute_m_tile_ids(jnp.array([1,1,2,1,1,1,1,1]), 8, 10)[:9].tolist() == [0,1,2,2,3,4,5,6,7]\n",
    "print(\"Step 2e — compute_m_tile_ids: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96a80b",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "return jnp.repeat(\n",
    "    jnp.arange(tiles_m, dtype=jnp.int32),\n",
    "    tile_visits,\n",
    "    total_repeat_length=max_len,\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f46e7",
   "metadata": {},
   "source": [
    "### Step 2f: Combined `make_group_metadata`\n",
    "\n",
    "**Goal**: Chain the 5 steps above into the complete function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50623acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_group_metadata_yours(group_sizes, m, bm):\n",
    "    \"\"\"Vectorized group metadata — chains steps 2a–2e.\n",
    "\n",
    "    Args:\n",
    "        group_sizes: jnp.array of shape (num_groups,), dtype int32\n",
    "        m: total number of rows\n",
    "        bm: tile size for m dimension\n",
    "\n",
    "    Returns:\n",
    "        (group_offsets, group_ids, m_tile_ids), num_tiles\n",
    "    \"\"\"\n",
    "    num_groups = group_sizes.shape[0]\n",
    "    tiles_m = m // bm\n",
    "    max_len = tiles_m + num_groups - 1\n",
    "\n",
    "    # YOUR CODE HERE — chain steps 2a–2e, then compute num_tiles\n",
    "    # Replace this raise with your implementation:\n",
    "    raise NotImplementedError(\"Chain compute_group_offsets → ... → compute_m_tile_ids\")\n",
    "\n",
    "    return (group_offsets, group_ids, m_tile_ids), num_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda82dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration tests — compare against reference\n",
    "def check_metadata(name, group_sizes, m, bm):\n",
    "    ref, ref_nt = make_group_metadata_reference(group_sizes, m, bm)\n",
    "    yours, your_nt = make_group_metadata_yours(group_sizes, m, bm)\n",
    "    ok = (ref_nt == your_nt\n",
    "          and bool(jnp.array_equal(ref[0], yours[0]))\n",
    "          and bool(jnp.array_equal(ref[1][:ref_nt], yours[1][:your_nt]))\n",
    "          and bool(jnp.array_equal(ref[2][:ref_nt], yours[2][:your_nt])))\n",
    "    status = \"PASSED ✓\" if ok else \"FAILED ✗\"\n",
    "    print(f\"  {name}: {status}  (num_tiles: ref={ref_nt}, yours={your_nt})\")\n",
    "    if not ok:\n",
    "        print(f\"    group_ids ref:   {ref[1][:ref_nt].tolist()}\")\n",
    "        print(f\"    group_ids yours: {yours[1][:your_nt].tolist()}\")\n",
    "        print(f\"    m_tile_ids ref:   {ref[2][:ref_nt].tolist()}\")\n",
    "        print(f\"    m_tile_ids yours: {yours[2][:your_nt].tolist()}\")\n",
    "\n",
    "print(\"=== Integration tests ===\")\n",
    "check_metadata(\"Aligned groups\",\n",
    "               jnp.array([256, 256, 256, 256], dtype=jnp.int32), 1024, 128)\n",
    "check_metadata(\"Unaligned groups\",\n",
    "               jnp.array([300, 212, 512], dtype=jnp.int32), 1024, 128)\n",
    "check_metadata(\"Zero-size group (aligned)\",\n",
    "               jnp.array([512, 0, 512], dtype=jnp.int32), 1024, 128)\n",
    "check_metadata(\"Zero-size group (non-aligned)\",\n",
    "               jnp.array([300, 0, 724], dtype=jnp.int32), 1024, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece46d59",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group_offsets = compute_group_offsets(group_sizes)\n",
    "group_tiles = compute_group_tiles(group_sizes, group_offsets, bm)\n",
    "group_ids = compute_group_ids(group_tiles, num_groups, max_len)\n",
    "tile_visits = compute_tile_visits(group_sizes, group_offsets, tiles_m, bm)\n",
    "m_tile_ids = compute_m_tile_ids(tile_visits, tiles_m, max_len)\n",
    "num_tiles = int(group_tiles.sum())\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e80c17",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 3: Configure Your Own Scalar-Prefetch `pallas_call`\n",
    "\n",
    "**Goal**: Given a working kernel, write the **entire** `pl.pallas_call`\n",
    "invocation from scratch, including `PrefetchScalarGridSpec` with\n",
    "`num_scalar_prefetch=3`, `in_specs` with runtime index maps, `out_specs`,\n",
    "and `grid`.\n",
    "\n",
    "### Theory\n",
    "\n",
    "This is the most challenging configuration exercise. You need to\n",
    "understand:\n",
    "- **Which args are scalar-prefetched**: metadata arrays that index maps\n",
    "  need at runtime. They appear as leading args in the call and leading\n",
    "  refs in the kernel.\n",
    "- **How index maps receive prefetch refs**: each index map gets the grid\n",
    "  indices first, then all prefetch refs. E.g.,\n",
    "  `lambda i, go, gi, mt: (mt[i], 0)` — `i` is the grid index, `go/gi/mt`\n",
    "  are the three scalar-prefetched refs.\n",
    "- **How grid size comes from num_tiles**: the grid iterates over\n",
    "  `num_tiles` (from `make_group_metadata`), not over `tiles_m`.\n",
    "- **Call argument order**: scalar-prefetched args come first, then\n",
    "  regular inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b522b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 1024, 64\n",
    "bm = 128\n",
    "G = 3\n",
    "\n",
    "group_sizes = jnp.array([300, 212, 512], dtype=jnp.int32)\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata_reference(group_sizes, M, bm)\n",
    "\n",
    "# The kernel is provided (solved):\n",
    "def masked_copy_kernel_solved(group_offsets_ref, group_ids_ref, m_tile_ids_ref,\n",
    "                               x_ref, o_ref):\n",
    "    \"\"\"Copy rows from x to output, masked by group boundaries.\"\"\"\n",
    "    grid_id = pl.program_id(0)\n",
    "    group_id = group_ids_ref[grid_id]\n",
    "    m_tile = m_tile_ids_ref[grid_id]\n",
    "    group_start = group_offsets_ref[group_id]\n",
    "    group_end = group_offsets_ref[group_id + 1]\n",
    "    tile_start = m_tile * bm\n",
    "\n",
    "    row_ids = tile_start + jax.lax.broadcasted_iota(jnp.int32, (bm, N), 0)\n",
    "    mask = (row_ids >= group_start) & (row_ids < group_end)\n",
    "    o_ref[...] = jnp.where(mask, x_ref[...], o_ref[...])\n",
    "\n",
    "# Reference spec\n",
    "def masked_copy_spec(x, group_offsets, group_ids, m_tile_ids):\n",
    "    out = jnp.zeros_like(x)\n",
    "    for grid_id in range(num_tiles):\n",
    "        g = int(group_ids[grid_id])\n",
    "        tile_id = int(m_tile_ids[grid_id])\n",
    "        g_start = int(group_offsets[g])\n",
    "        g_end = int(group_offsets[g + 1])\n",
    "        t_start = tile_id * bm\n",
    "        t_end = t_start + bm\n",
    "        for row in range(t_start, t_end):\n",
    "            if g_start <= row < g_end:\n",
    "                out = out.at[row].set(x[row])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8431d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.key(26), (M, N))\n",
    "expected = masked_copy_spec(x, group_offsets, group_ids, m_tile_ids)\n",
    "\n",
    "# YOUR TASK: Write the complete pl.pallas_call invocation.\n",
    "# Replace `None` with your working code.\n",
    "#\n",
    "# You need:\n",
    "# - PrefetchScalarGridSpec with num_scalar_prefetch=3\n",
    "# - in_specs: BlockSpec that uses m_tile_ids to route tiles\n",
    "# - out_specs: BlockSpec that uses m_tile_ids to route tiles\n",
    "# - grid=(num_tiles,)\n",
    "# - Call args: (group_offsets, group_ids, m_tile_ids, x) — scalar prefetch first!\n",
    "actual = None  # Replace with pl.pallas_call(...)(...) invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb080df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if actual is not None and jnp.allclose(actual, expected, atol=1e-5):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    print(\"FAILED ✗  (fill in the cell above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d53654",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Structure</summary>\n",
    "\n",
    "```python\n",
    "actual = pl.pallas_call(\n",
    "    masked_copy_kernel_solved,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=3,\n",
    "        in_specs=[pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0))],\n",
    "        out_specs=pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0)),\n",
    "        grid=(num_tiles,),\n",
    "    ),\n",
    "    out_shape=...,\n",
    "    interpret=True,\n",
    ")(...)  # scalar-prefetched args first, then regular inputs\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "actual = pl.pallas_call(\n",
    "    masked_copy_kernel_solved,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=3,\n",
    "        in_specs=[pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0))],\n",
    "        out_specs=pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0)),\n",
    "        grid=(num_tiles,),\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_offsets, group_ids, m_tile_ids, x)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de838ef1",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 4: Masked Store with Group Boundaries\n",
    "\n",
    "**Goal**: Write a kernel that copies input rows to output, but **masks**\n",
    "writes based on group boundaries. Only rows belonging to the current\n",
    "group are written; other rows retain their previous value (zero).\n",
    "\n",
    "### Theory\n",
    "\n",
    "When a tile straddles a group boundary, some rows belong to group `g`\n",
    "and others to group `g+1`. The kernel must only store the rows that\n",
    "belong to the **current group** being processed.\n",
    "\n",
    "The mask is built from:\n",
    "- `group_offsets[group_id]` → start row of current group\n",
    "- `group_offsets[group_id + 1]` → end row of current group\n",
    "- `m_tile_ids[grid_id] * bm` → first row of current tile\n",
    "\n",
    "```python\n",
    "row_indices = tile_start + jnp.arange(bm)\n",
    "mask = (row_indices >= group_start) & (row_indices < group_end)\n",
    "```\n",
    "\n",
    "For a 2D mask (bm, N), use `jax.lax.broadcasted_iota(dtype, shape, dim)`\n",
    "— it creates an array where values along `dim` are `0, 1, 2, ...` and\n",
    "all other dimensions are broadcast. Think of it as a multi-dimensional\n",
    "`jnp.arange`:\n",
    "```python\n",
    "broadcasted_iota(int32, (4, 3), 0) → [[0,0,0], [1,1,1], [2,2,2], [3,3,3]]\n",
    "broadcasted_iota(int32, (4, 3), 1) → [[0,1,2], [0,1,2], [0,1,2], [0,1,2]]\n",
    "```\n",
    "\n",
    "This is exactly the `get_store_mask` pattern used in ragged_dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1024\n",
    "N = 64\n",
    "bm = 128\n",
    "G = 3\n",
    "\n",
    "group_sizes = jnp.array([300, 212, 512], dtype=jnp.int32)\n",
    "\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata_reference(group_sizes, M, bm)\n",
    "\n",
    "# --- Reference ---\n",
    "def masked_copy_spec(x, group_offsets, group_ids, m_tile_ids):\n",
    "    \"\"\"Copy x to output, but only rows within their assigned group.\"\"\"\n",
    "    out = jnp.zeros_like(x)\n",
    "    for grid_id in range(num_tiles):\n",
    "        g = int(group_ids[grid_id])\n",
    "        tile_id = int(m_tile_ids[grid_id])\n",
    "        g_start = int(group_offsets[g])\n",
    "        g_end = int(group_offsets[g + 1])\n",
    "        t_start = tile_id * bm\n",
    "        t_end = t_start + bm\n",
    "        for row in range(t_start, t_end):\n",
    "            if g_start <= row < g_end:\n",
    "                out = out.at[row].set(x[row])\n",
    "    return out\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def masked_copy_kernel(group_offsets_ref, group_ids_ref, m_tile_ids_ref,\n",
    "                       x_ref, o_ref):\n",
    "    # group_offsets_ref, group_ids_ref, m_tile_ids_ref: metadata in SMEM\n",
    "    # x_ref: (bm, N) — tile of input\n",
    "    # o_ref: (bm, N) — tile of output\n",
    "    grid_id = pl.program_id(0)\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Look up which group and tile this grid iteration processes\n",
    "    # 2. Get the group's row boundaries\n",
    "    # 3. Build a 2D boolean mask for rows inside this group\n",
    "    # 4. Masked store: only write rows belonging to this group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.key(27), (M, N))\n",
    "expected = masked_copy_spec(x, group_offsets, group_ids, m_tile_ids)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    masked_copy_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=3,\n",
    "        in_specs=[pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0))],\n",
    "        out_specs=pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0)),\n",
    "        grid=(num_tiles,),\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_offsets, group_ids, m_tile_ids, x)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-5):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    nan_count = int(jnp.isnan(actual).sum())\n",
    "    if nan_count > 0:\n",
    "        nan_rows = jnp.where(jnp.isnan(actual).any(axis=1))[0]\n",
    "        print(f\"FAILED ✗  {nan_count} NaN values in output (rows: {nan_rows.tolist()[:8]}...)\")\n",
    "        print(f\"  Common cause: indexing group_offsets_ref with grid_id instead of group_id\")\n",
    "    else:\n",
    "        diff = jnp.abs(actual - expected)\n",
    "        worst_row = int(jnp.argmax(diff.max(axis=1)))\n",
    "        max_err = float(diff[worst_row].max())\n",
    "        print(f\"FAILED ✗  max error: {max_err:.6f} at row {worst_row}\")\n",
    "        g_boundaries = group_offsets.tolist()\n",
    "        print(f\"  Group boundaries at rows: {g_boundaries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641a845",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Key pattern</summary>\n",
    "\n",
    "```python\n",
    "group_id = group_ids_ref[grid_id]\n",
    "m_tile = m_tile_ids_ref[grid_id]\n",
    "group_start = group_offsets_ref[group_id]\n",
    "group_end = group_offsets_ref[group_id + 1]\n",
    "tile_start = m_tile * bm\n",
    "\n",
    "# Build a (bm, N) mask where row_index in [group_start, group_end)\n",
    "# Tip: jax.lax.broadcasted_iota(dtype, shape, dimension)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group_id = group_ids_ref[grid_id]\n",
    "m_tile = m_tile_ids_ref[grid_id]\n",
    "group_start = group_offsets_ref[group_id]\n",
    "group_end = group_offsets_ref[group_id + 1]\n",
    "tile_start = m_tile * bm\n",
    "\n",
    "row_ids = tile_start + jax.lax.broadcasted_iota(jnp.int32, (bm, N), 0)\n",
    "mask = (row_ids >= group_start) & (row_ids < group_end)\n",
    "\n",
    "o_ref[...] = jnp.where(mask, x_ref[...], o_ref[...])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1df21",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 5: Softmax Kernel\n",
    "\n",
    "**Goal**: Write a softmax kernel **and** configure the `pallas_call`\n",
    "yourself. Each kernel invocation processes one row-block of the full\n",
    "matrix.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Softmax is a non-matmul kernel that combines **reduction** (max, sum)\n",
    "with **elementwise** ops (exp, divide). For each row:\n",
    "\n",
    "```\n",
    "softmax(x) = exp(x - max(x)) / sum(exp(x - max(x)))\n",
    "```\n",
    "\n",
    "Since each row fits entirely within one tile (no column-tiling needed),\n",
    "the kernel is simpler than matmul — no `@pl.when` guards, no scratch\n",
    "accumulator, just straight computation. The grid only tiles along rows.\n",
    "\n",
    "**This puzzle is also a \"configure your own pallas_call\" exercise** —\n",
    "you need to write both the kernel body AND the `grid`, `in_specs`,\n",
    "`out_specs`.\n",
    "\n",
    "In production (FlashAttention), the column dimension is also tiled using\n",
    "an **online softmax** algorithm that maintains running max and sum across\n",
    "column tiles. The core pattern of max → subtract → exp → normalize is\n",
    "the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6866a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS, COLS = 256, 128\n",
    "bm = 64\n",
    "\n",
    "# --- Reference ---\n",
    "def softmax_spec(x):\n",
    "    \"\"\"x: (ROWS, COLS) → row-wise softmax\"\"\"\n",
    "    return jax.nn.softmax(x, axis=1)\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def softmax_kernel(x_ref, o_ref):\n",
    "    # x_ref: (bm, COLS) — one row block (full width)\n",
    "    # o_ref: (bm, COLS) — output\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Compute row max for numerical stability\n",
    "    # 2. Subtract max, exponentiate\n",
    "    # 3. Divide by row sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674abd5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.key(28), (ROWS, COLS))\n",
    "\n",
    "# YOUR TASK: Write the kernel above AND define the config below.\n",
    "softmax_grid = ...       # TODO: how many row blocks?\n",
    "softmax_in_specs = ...   # TODO: list with one BlockSpec\n",
    "softmax_out_specs = ...  # TODO: BlockSpec for output\n",
    "\n",
    "expected = softmax_spec(x)\n",
    "actual = pl.pallas_call(\n",
    "    softmax_kernel,\n",
    "    grid=softmax_grid,\n",
    "    in_specs=softmax_in_specs,\n",
    "    out_specs=softmax_out_specs,\n",
    "    out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    interpret=True,\n",
    ")(x)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99a51d",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — Kernel</summary>\n",
    "\n",
    "```python\n",
    "x = x_ref[...]\n",
    "row_max = x.max(axis=1, keepdims=True)\n",
    "exp_x = jnp.exp(x - row_max)\n",
    "row_sum = exp_x.sum(axis=1, keepdims=True)\n",
    "o_ref[...] = exp_x / row_sum\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Config</summary>\n",
    "\n",
    "```python\n",
    "softmax_grid = (ROWS // bm,)  # 256 // 64 = 4 row blocks\n",
    "softmax_in_specs = [pl.BlockSpec((bm, COLS), lambda i: (i, 0))]\n",
    "softmax_out_specs = pl.BlockSpec((bm, COLS), lambda i: (i, 0))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def softmax_kernel(x_ref, o_ref):\n",
    "    x = x_ref[...]\n",
    "    row_max = x.max(axis=1, keepdims=True)\n",
    "    exp_x = jnp.exp(x - row_max)\n",
    "    row_sum = exp_x.sum(axis=1, keepdims=True)\n",
    "    o_ref[...] = exp_x / row_sum\n",
    "\n",
    "softmax_grid = (ROWS // bm,)\n",
    "softmax_in_specs = [pl.BlockSpec((bm, COLS), lambda i: (i, 0))]\n",
    "softmax_out_specs = pl.BlockSpec((bm, COLS), lambda i: (i, 0))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d2551",
   "metadata": {},
   "source": [
    "---\n",
    "# Part IV: Ragged Dot (Puzzles 6–9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2111996",
   "metadata": {},
   "source": [
    "### Provided utilities\n",
    "\n",
    "These are the building blocks from basics.py and Part III. They're provided\n",
    "here so you can focus on the kernel logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2230e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_group_metadata(group_sizes, m, bm, *, visit_empty_groups=False):\n",
    "    \"\"\"Compute tile-to-group mapping for ragged_dot.\n",
    "\n",
    "    Returns:\n",
    "        (group_offsets, group_ids, m_tile_ids), num_tiles\n",
    "    \"\"\"\n",
    "    num_groups = group_sizes.shape[0]\n",
    "    tiles_m = m // bm\n",
    "\n",
    "    group_ends = jnp.cumsum(group_sizes)\n",
    "    group_offsets = jnp.concatenate([jnp.zeros(1, dtype=jnp.int32), group_ends])\n",
    "\n",
    "    group_starts = jnp.concatenate([jnp.zeros(1, dtype=jnp.int32), group_ends[:-1]])\n",
    "    rounded_ends = ((group_ends + bm - 1) // bm * bm).astype(jnp.int32)\n",
    "    rounded_starts = (group_starts // bm * bm).astype(jnp.int32)\n",
    "    rounded_sizes = rounded_ends - rounded_starts\n",
    "    rounded_sizes = jnp.where(group_sizes == 0, 0, rounded_sizes)\n",
    "    group_tiles = rounded_sizes // bm\n",
    "\n",
    "    if visit_empty_groups:\n",
    "        group_tiles = jnp.where(group_sizes == 0, 1, group_tiles)\n",
    "\n",
    "    group_ids = jnp.repeat(\n",
    "        jnp.arange(num_groups, dtype=jnp.int32),\n",
    "        group_tiles,\n",
    "        total_repeat_length=tiles_m + num_groups - 1,\n",
    "    )\n",
    "\n",
    "    partial_mask = ((group_offsets[:-1] % bm) == 0) | (group_sizes == 0)\n",
    "    if visit_empty_groups:\n",
    "        partial_mask = jnp.where(group_sizes == 0, 0, partial_mask)\n",
    "    partial_tile_ids = jnp.where(partial_mask, tiles_m + 1, group_offsets[:-1] // bm)\n",
    "    tile_visits = (\n",
    "        jnp.histogram(partial_tile_ids, bins=tiles_m, range=(0, tiles_m))[0] + 1\n",
    "    )\n",
    "    m_tile_ids = jnp.repeat(\n",
    "        jnp.arange(tiles_m, dtype=jnp.int32),\n",
    "        tile_visits.astype(jnp.int32),\n",
    "        total_repeat_length=tiles_m + num_groups - 1,\n",
    "    )\n",
    "\n",
    "    num_tiles = int(group_tiles.sum())\n",
    "    return (group_offsets, group_ids, m_tile_ids), num_tiles\n",
    "\n",
    "\n",
    "def get_store_mask(grid_id, group_offsets, group_ids, m_tile_ids, bm, bn):\n",
    "    \"\"\"Build a (bm, bn) boolean mask for rows belonging to the current group.\"\"\"\n",
    "    group_id = group_ids[grid_id]\n",
    "    group_start = group_offsets[group_id]\n",
    "    group_end = group_offsets[group_id + 1]\n",
    "    m_id = m_tile_ids[grid_id] * bm\n",
    "    iota = jax.lax.broadcasted_iota(jnp.int32, (bm, bn), 0) + m_id\n",
    "    return (iota >= group_start) & (iota < group_end)\n",
    "\n",
    "\n",
    "# --- Shared index maps for ragged_dot ---\n",
    "def lhs_imap(n_i, grid_id, k_i, group_meta_ref, group_offset_ref):\n",
    "    _, _, m_tile_ids = group_meta_ref\n",
    "    return (m_tile_ids[grid_id], k_i)\n",
    "\n",
    "def rhs_imap(n_i, grid_id, k_i, group_meta_ref, group_offset_ref):\n",
    "    _, group_ids, _ = group_meta_ref\n",
    "    return (group_ids[grid_id], k_i, n_i)\n",
    "\n",
    "def out_imap(n_i, grid_id, k_i, group_meta_ref, group_offset_ref):\n",
    "    _, _, m_tile_ids = group_meta_ref\n",
    "    return (m_tile_ids[grid_id], n_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1565b",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 6: Simple Grouped Matmul — Equal Groups, Tile-Aligned\n",
    "\n",
    "**Goal**: Implement grouped matmul for the simplest case: all groups have\n",
    "equal size and group sizes are divisible by the tile size.\n",
    "\n",
    "### Theory\n",
    "\n",
    "This is the \"easy mode\" ragged_dot. With equal, tile-aligned groups:\n",
    "- No partial tiles (every tile belongs to exactly one group)\n",
    "- `group_ids` is a simple repeat: `[0,0,1,1,2,2,3,3]`\n",
    "- `m_tile_ids` = `[0,1,2,3,4,5,6,7]` (just sequential)\n",
    "- No masking needed on stores\n",
    "\n",
    "**Grid**: `(tiles_n, num_tiles, tiles_k)`\n",
    "- `tiles_n`: N dimension (parallel — independent output columns)\n",
    "- `num_tiles`: M tiles across all groups (may revisit same output)\n",
    "- `tiles_k`: K reduction dimension (accumulates)\n",
    "\n",
    "**Kernel structure** (same as tokamax `gmm`):\n",
    "1. Get `grid_id = program_id(1)`, `k_i = program_id(2)`\n",
    "2. Zero accumulator when `k_i == 0`\n",
    "3. Accumulate `lhs_tile @ rhs_tile`\n",
    "4. Store on last K tile\n",
    "\n",
    "**Index maps** (provided — study them!):\n",
    "- `lhs_imap`: `(n_i, grid_id, k_i) → (m_tile_ids[grid_id], k_i)`\n",
    "- `rhs_imap`: `(n_i, grid_id, k_i) → (group_ids[grid_id], k_i, n_i)`\n",
    "- `out_imap`: `(n_i, grid_id, k_i) → (m_tile_ids[grid_id], n_i)`\n",
    "\n",
    "The `group_ids` lookup in `rhs_imap` is what routes each tile to\n",
    "the correct group's weight matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 4\n",
    "M, K, N = 512, 256, 128\n",
    "bm, bk, bn = 128, 128, 128\n",
    "\n",
    "group_sizes = jnp.array([M // G] * G, dtype=jnp.int32)\n",
    "tiles_k = K // bk\n",
    "tiles_n = N // bn\n",
    "\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata(group_sizes, M, bm)\n",
    "\n",
    "# --- Reference ---\n",
    "def simple_gmm_spec(lhs, rhs, group_sizes):\n",
    "    \"\"\"lhs: (M, K), rhs: (G, K, N), group_sizes: (G,) → (M, N)\"\"\"\n",
    "    offsets = jnp.concatenate([jnp.array([0]), jnp.cumsum(group_sizes)])\n",
    "    out = jnp.zeros((lhs.shape[0], rhs.shape[2]), dtype=jnp.float32)\n",
    "    for g in range(len(group_sizes)):\n",
    "        s, e = int(offsets[g]), int(offsets[g + 1])\n",
    "        out = out.at[s:e].set(lhs[s:e] @ rhs[g])\n",
    "    return out\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def simple_gmm_kernel(group_metadata_ref, group_offset_ref,\n",
    "                      lhs_ref, rhs_ref, o_ref, acc_ref):\n",
    "    # group_metadata_ref: (group_offsets, group_ids, m_tile_ids) in SMEM\n",
    "    # group_offset_ref: unused here (for sharding)\n",
    "    # lhs_ref: (bm, bk) — tile of lhs\n",
    "    # rhs_ref: (bk, bn) — tile of rhs (group dim squeezed by None)\n",
    "    # o_ref: (bm, bn) — output tile\n",
    "    # acc_ref: (bm, bn) — scratch accumulator\n",
    "    grid_id = pl.program_id(1)\n",
    "    k_i = pl.program_id(2)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Zero accumulator on first K tile\n",
    "    # 2. Accumulate tile matmul\n",
    "    # 3. Store result on last K tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = jax.random.normal(jax.random.key(30), (M, K))\n",
    "rhs = jax.random.normal(jax.random.key(31), (G, K, N))\n",
    "expected = simple_gmm_spec(lhs, rhs, group_sizes)\n",
    "\n",
    "group_metadata = (group_offsets, group_ids, m_tile_ids)\n",
    "group_offset = jnp.array([0], dtype=jnp.int32)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    simple_gmm_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=2,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((bm, bk), lhs_imap),\n",
    "            pl.BlockSpec((None, bk, bn), rhs_imap),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((bm, bn), out_imap),\n",
    "        grid=(tiles_n, num_tiles, tiles_k),\n",
    "        scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_metadata, group_offset, lhs, rhs)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-2, rtol=1e-2):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    max_err = float(jnp.max(jnp.abs(actual - expected)))\n",
    "    print(f\"FAILED ✗  max error: {max_err:.6f}\")\n",
    "    print(f\"  Expected[:2,:4]:\\n{expected[:2,:4]}\")\n",
    "    print(f\"  Actual[:2,:4]:\\n{actual[:2,:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdb5cc",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Approach</summary>\n",
    "\n",
    "The kernel body is identical to basics.py Puzzle 8: zero / accumulate / store with `@pl.when`. The index maps (already provided) handle all the group-to-tile routing via `group_ids` and `m_tile_ids`. With `None` in the rhs BlockSpec, the group dimension is squeezed — `rhs_ref` is just `(bk, bn)`.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "\n",
    "acc_ref[...] += lhs_ref[...] @ rhs_ref[...]\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    o_ref[...] = acc_ref[...]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d62633",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 7: Full Ragged Dot — Unequal Groups\n",
    "\n",
    "**Goal**: Handle **variable group sizes** where tiles can straddle group\n",
    "boundaries. This is the real ragged_dot.\n",
    "\n",
    "### Theory\n",
    "\n",
    "The only difference from Puzzle 6: when groups are unequal, a tile may\n",
    "be visited **multiple times** (once per group it straddles). On each visit,\n",
    "the kernel must **mask** the store so only rows belonging to the current\n",
    "group are written.\n",
    "\n",
    "`make_group_metadata` handles all the complexity — the `group_ids` and\n",
    "`m_tile_ids` arrays already encode the repeated visits. The kernel just\n",
    "needs to add the mask at store time:\n",
    "\n",
    "```python\n",
    "mask = get_store_mask(grid_id, group_offsets, group_ids, m_tile_ids, bm, bn)\n",
    "o_ref[...] = jnp.where(mask, acc[...], o_ref[...])\n",
    "```\n",
    "\n",
    "```\n",
    "Tile at row 256, bm=128:\n",
    "┌────────────────────────┐\n",
    "│ rows 256-299: group 0  │ ← Visit 1: mask=True for rows 256-299\n",
    "│ rows 300-383: group 1  │ ← Visit 2: mask=True for rows 300-383\n",
    "└────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5eda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "M, K, N = 1024, 256, 128\n",
    "bm, bk, bn = 128, 128, 128\n",
    "\n",
    "group_sizes = jnp.array([300, 212, 512], dtype=jnp.int32)\n",
    "tiles_k = K // bk\n",
    "tiles_n = N // bn\n",
    "\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata(group_sizes, M, bm)\n",
    "\n",
    "print(f\"M={M}, G={G}, group_sizes={group_sizes.tolist()}\")\n",
    "print(f\"num_tiles={num_tiles} (vs {M//bm} base tiles)\")\n",
    "print(f\"group_ids[:num_tiles]={group_ids[:num_tiles].tolist()}\")\n",
    "print(f\"m_tile_ids[:num_tiles]={m_tile_ids[:num_tiles].tolist()}\")\n",
    "\n",
    "# --- Reference ---\n",
    "def ragged_dot_spec(lhs, rhs, group_sizes):\n",
    "    \"\"\"Same as jax.lax.ragged_dot but explicit for clarity.\"\"\"\n",
    "    offsets = jnp.concatenate([jnp.array([0]), jnp.cumsum(group_sizes)])\n",
    "    out = jnp.zeros((lhs.shape[0], rhs.shape[2]), dtype=jnp.float32)\n",
    "    for g in range(len(group_sizes)):\n",
    "        s, e = int(offsets[g]), int(offsets[g + 1])\n",
    "        if s < e:\n",
    "            out = out.at[s:e].set(lhs[s:e] @ rhs[g])\n",
    "    return out\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def ragged_dot_kernel(group_metadata_ref, group_offset_ref,\n",
    "                      lhs_ref, rhs_ref, o_ref, acc_ref):\n",
    "    group_offsets, group_ids, m_tile_ids = group_metadata_ref\n",
    "    grid_id = pl.program_id(1)\n",
    "    k_i = pl.program_id(2)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Same as Puzzle 6, but on the last K tile, apply a masked store\n",
    "    # so only rows belonging to the current group are written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b20f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = jax.random.normal(jax.random.key(40), (M, K))\n",
    "rhs = jax.random.normal(jax.random.key(41), (G, K, N))\n",
    "expected = ragged_dot_spec(lhs, rhs, group_sizes)\n",
    "\n",
    "group_metadata = (group_offsets, group_ids, m_tile_ids)\n",
    "group_offset = jnp.array([0], dtype=jnp.int32)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    ragged_dot_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=2,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((bm, bk), lhs_imap),\n",
    "            pl.BlockSpec((None, bk, bn), rhs_imap),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((bm, bn), out_imap),\n",
    "        grid=(tiles_n, num_tiles, tiles_k),\n",
    "        scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_metadata, group_offset, lhs, rhs)\n",
    "\n",
    "total_rows = int(group_sizes.sum())\n",
    "if jnp.allclose(actual[:total_rows], expected[:total_rows], atol=1e-2, rtol=1e-2):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "    print(f\"  Verified {total_rows} active rows\")\n",
    "else:\n",
    "    max_err = float(jnp.max(jnp.abs(actual[:total_rows] - expected[:total_rows])))\n",
    "    print(f\"FAILED ✗  max error: {max_err:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e760d1",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — The masked store block</summary>\n",
    "\n",
    "```python\n",
    "# Steps 1-2 are the same as Puzzle 6 (zero + accumulate)\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    mask = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                          m_tile_ids, bm, bn)\n",
    "    acc = acc_ref[...]\n",
    "    o_ref[...] = jnp.where(mask, acc, o_ref[...].astype(acc.dtype))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "\n",
    "acc_ref[...] += lhs_ref[...] @ rhs_ref[...]\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    mask = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                          m_tile_ids, bm, bn)\n",
    "    acc = acc_ref[...]\n",
    "    o_ref[...] = jnp.where(mask, acc, o_ref[...].astype(acc.dtype))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4df0b4",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 8: Transpose Grouped Matmul (tgmm)\n",
    "\n",
    "**Goal**: Implement the **backward-pass** kernel: `tgmm` computes the\n",
    "gradient w.r.t. the RHS weight matrices.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In the backward pass of ragged_dot, we need:\n",
    "- `dlhs = dout @ rhs[g].T` (gradient w.r.t. lhs — another gmm)\n",
    "- `drhs[g] = lhs[g_rows].T @ dout[g_rows]` (gradient w.r.t. rhs — this is tgmm)\n",
    "\n",
    "**tgmm** computes `lhs.T @ rhs` accumulated per group:\n",
    "- `lhs`: `(M, K)` (original lhs, or the transposed `(K, M)` passed as `(M, K)`)\n",
    "- `rhs`: `(M, N)` (dout)\n",
    "- `out`: `(G, K, N)` — one output per group\n",
    "\n",
    "**Key difference from gmm**: In gmm, multiple K tiles contribute to the\n",
    "same output tile (accumulate over K). In tgmm, multiple **M tiles** from\n",
    "the same group contribute to the same output tile (accumulate over group\n",
    "rows). This requires a different accumulation pattern:\n",
    "\n",
    "- **Prologue** (entering new group): zero the accumulator\n",
    "- **Body**: accumulate `lhs_tile.T @ rhs_tile`, masked by group boundaries\n",
    "- **Epilogue** (leaving group): store accumulator to output\n",
    "\n",
    "Group transitions detected by comparing consecutive group_ids.\n",
    "\n",
    "```\n",
    "Grid iteration:  0   1   2   3   4   5   6   7   8   9\n",
    "group_ids:      [0,  0,  0,  1,  1,  2,  2,  2,  2,  2]\n",
    "                 P       E  P    E  P               E\n",
    "                 P = prologue (zero), E = epilogue (store)\n",
    "```\n",
    "\n",
    "**New concepts in this puzzle:**\n",
    "\n",
    "- **`visit_empty_groups=True`**: If a group has zero rows, we still\n",
    "  need one grid iteration for it — so the kernel can zero and store\n",
    "  an empty accumulator. Without this, the output for that group\n",
    "  would contain garbage.\n",
    "\n",
    "- **`pl.num_programs(axis)`**: Returns the total number of grid\n",
    "  iterations along an axis (like `gridDim` in CUDA). Used here to\n",
    "  detect the very last iteration for the final epilogue.\n",
    "\n",
    "- **Grid axis order is `(tiles_n, tiles_k, num_tiles)`** — note\n",
    "  that `num_tiles` is now on axis 2 (not axis 1 like in gmm).\n",
    "  This is because the M-tile iteration is the \"reduction\" axis\n",
    "  in tgmm (we accumulate across M tiles), so it must be the\n",
    "  innermost `\"arbitrary\"` dimension for correct pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "M, K, N = 1024, 128, 128\n",
    "bm, bk, bn = 128, 128, 128\n",
    "\n",
    "group_sizes = jnp.array([300, 340, 384], dtype=jnp.int32)\n",
    "tiles_k = K // bk\n",
    "tiles_n = N // bn\n",
    "\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata(group_sizes, M, bm, visit_empty_groups=True)\n",
    "\n",
    "print(f\"group_sizes={group_sizes.tolist()}, num_tiles={num_tiles}\")\n",
    "print(f\"group_ids={group_ids[:num_tiles].tolist()}\")\n",
    "\n",
    "# --- Reference ---\n",
    "def tgmm_spec(lhs_t, rhs, group_sizes):\n",
    "    \"\"\"lhs_t: (K, M), rhs: (M, N) → (G, K, N)\n",
    "    Computes lhs_t[:, g_start:g_end] @ rhs[g_start:g_end, :] per group.\n",
    "    \"\"\"\n",
    "    offsets = jnp.concatenate([jnp.array([0]), jnp.cumsum(group_sizes)])\n",
    "    G = len(group_sizes)\n",
    "    K, N = lhs_t.shape[0], rhs.shape[1]\n",
    "    out = jnp.zeros((G, K, N), dtype=jnp.float32)\n",
    "    for g in range(G):\n",
    "        s, e = int(offsets[g]), int(offsets[g + 1])\n",
    "        if s < e:\n",
    "            out = out.at[g].set(lhs_t[:, s:e] @ rhs[s:e, :])\n",
    "    return out\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def tgmm_kernel(group_metadata_ref, group_offset_ref,\n",
    "                lhs_ref, rhs_ref, o_ref, acc_ref):\n",
    "    # lhs_ref: (bm, bk) — tile of lhs (M, K)\n",
    "    # rhs_ref: (bm, bn) — tile of rhs\n",
    "    # o_ref: (bk, bn) — output tile for one group (None dim squeezed)\n",
    "    # acc_ref: (bk, bn) — scratch accumulator\n",
    "    group_offsets, group_ids, m_tile_ids = group_metadata_ref\n",
    "    grid_id = pl.program_id(2)  # tgmm grid: (tiles_n, tiles_k, num_tiles)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Detect group transitions: when does a new group start? end?\n",
    "    # 2. Zero accumulator at the start of each group\n",
    "    # 3. Accumulate masked lhs.T @ rhs\n",
    "    # 4. Store accumulator at the end of each group\n",
    "\n",
    "\n",
    "# --- Index maps for tgmm ---\n",
    "def tgmm_lhs_imap(n_i, k_i, grid_id, group_meta_ref, group_offset_ref):\n",
    "    _, _, m_tile_ids = group_meta_ref\n",
    "    return (m_tile_ids[grid_id], k_i)\n",
    "\n",
    "def tgmm_rhs_imap(n_i, k_i, grid_id, group_meta_ref, group_offset_ref):\n",
    "    _, _, m_tile_ids = group_meta_ref\n",
    "    return (m_tile_ids[grid_id], n_i)\n",
    "\n",
    "def tgmm_out_imap(n_i, k_i, grid_id, group_meta_ref, group_offset_ref):\n",
    "    _, group_ids, _ = group_meta_ref\n",
    "    return (group_ids[grid_id], k_i, n_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs_t = jax.random.normal(jax.random.key(50), (K, M))\n",
    "rhs = jax.random.normal(jax.random.key(51), (M, N))\n",
    "expected = tgmm_spec(lhs_t, rhs, group_sizes)\n",
    "\n",
    "# tgmm works on (M, K) internally — transpose lhs\n",
    "lhs = lhs_t.T  # (M, K)\n",
    "\n",
    "group_metadata = (group_offsets, group_ids, m_tile_ids)\n",
    "group_offset = jnp.array([0], dtype=jnp.int32)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    tgmm_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=2,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((bm, bk), tgmm_lhs_imap),\n",
    "            pl.BlockSpec((bm, bn), tgmm_rhs_imap),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((None, bk, bn), tgmm_out_imap),\n",
    "        grid=(tiles_n, tiles_k, num_tiles),\n",
    "        scratch_shapes=[pltpu.VMEM((bk, bn), jnp.float32)],\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((G, K, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_metadata, group_offset, lhs, rhs)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-1, rtol=1e-2):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    max_err = float(jnp.max(jnp.abs(actual - expected)))\n",
    "    print(f\"FAILED ✗  max error: {max_err:.6f}\")\n",
    "    print(f\"  Expected[0,:2,:4]:\\n{expected[0,:2,:4]}\")\n",
    "    print(f\"  Actual[0,:2,:4]:\\n{actual[0,:2,:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f929479",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — Prologue/epilogue detection</summary>\n",
    "\n",
    "```python\n",
    "group = group_ids[grid_id]\n",
    "prev_group = group_ids[jnp.where(grid_id > 0, grid_id - 1, 0)]\n",
    "is_prologue = (grid_id == 0) | (group != prev_group)\n",
    "\n",
    "is_end = grid_id == (pl.num_programs(2) - 1)\n",
    "next_group = group_ids[jnp.where(is_end, grid_id, grid_id + 1)]\n",
    "is_epilogue = is_end | (group != next_group)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Masked accumulation</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(is_prologue)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bk, bn), dtype=jnp.float32)\n",
    "\n",
    "mask_lhs = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                           m_tile_ids, bm, bk)\n",
    "mask_rhs = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                           m_tile_ids, bm, bn)\n",
    "lhs_masked = jnp.where(mask_lhs, lhs_ref[...], 0)\n",
    "rhs_masked = jnp.where(mask_rhs, rhs_ref[...], 0)\n",
    "acc_ref[...] += lhs_masked.T @ rhs_masked\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group = group_ids[grid_id]\n",
    "prev_group = group_ids[jnp.where(grid_id > 0, grid_id - 1, 0)]\n",
    "is_prologue = (grid_id == 0) | (group != prev_group)\n",
    "\n",
    "is_end = grid_id == (pl.num_programs(2) - 1)\n",
    "next_group = group_ids[jnp.where(is_end, grid_id, grid_id + 1)]\n",
    "is_epilogue = is_end | (group != next_group)\n",
    "\n",
    "group_size = group_offsets[group + 1] - group_offsets[group]\n",
    "nonzero_gs = group_size > 0\n",
    "\n",
    "@pl.when(is_prologue)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bk, bn), dtype=jnp.float32)\n",
    "\n",
    "@pl.when(nonzero_gs)\n",
    "def _compute():\n",
    "    mask_lhs = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                               m_tile_ids, bm, bk)\n",
    "    mask_rhs = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                               m_tile_ids, bm, bn)\n",
    "    lhs_masked = jnp.where(mask_lhs, lhs_ref[...], 0)\n",
    "    rhs_masked = jnp.where(mask_rhs, rhs_ref[...], 0)\n",
    "    acc_ref[...] += lhs_masked.T @ rhs_masked\n",
    "\n",
    "@pl.when(is_epilogue)\n",
    "def _store():\n",
    "    o_ref[...] = acc_ref[...]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c811e",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 9: Understanding `emit_pipeline` — Annotated Walkthrough\n",
    "\n",
    "This is a **reading exercise**, not a coding puzzle. We walk through the\n",
    "tokamax `custom_buffered_pallas_call` to understand how production kernels\n",
    "use software pipelining for async DMA on TPU.\n",
    "\n",
    "### Why pipelining?\n",
    "\n",
    "On TPU, data lives in **HBM** (32 GB, high bandwidth but high latency).\n",
    "Computation happens in **VMEM** (small, fast SRAM). Without pipelining:\n",
    "\n",
    "```\n",
    "Time:  [DMA load] [compute] [DMA load] [compute] ...\n",
    "       ^^^idle^^^            ^^^idle^^^\n",
    "```\n",
    "\n",
    "With double-buffered pipelining:\n",
    "\n",
    "```\n",
    "DMA:      [load 0] [load 1] [load 2] [load 3] ...\n",
    "Compute:           [comp 0] [comp 1] [comp 2] ...\n",
    "```\n",
    "\n",
    "The DMA engine and compute engine run in parallel, hiding memory latency.\n",
    "\n",
    "### The `emit_pipeline` wrapper\n",
    "\n",
    "`pltpu.emit_pipeline` transforms a simple kernel into a pipelined one:\n",
    "\n",
    "```python\n",
    "pltpu.emit_pipeline(\n",
    "    kernel_fn,          # Your original kernel\n",
    "    grid=grid,          # Iteration space\n",
    "    in_specs=in_specs,  # How to tile inputs\n",
    "    out_specs=out_specs, # How to tile outputs\n",
    "    dimension_semantics=(\"parallel\", \"arbitrary\", \"arbitrary\"),\n",
    ")\n",
    "```\n",
    "\n",
    "**`dimension_semantics`** tells the compiler about loop dependencies:\n",
    "- `\"parallel\"`: iterations are independent → can be reordered freely\n",
    "- `\"arbitrary\"`: iterations may have dependencies → must execute in order\n",
    "\n",
    "For ragged_dot: `(tiles_n, num_tiles, tiles_k)`:\n",
    "- `tiles_n` is `\"parallel\"` — different output columns are independent\n",
    "- `num_tiles` is `\"arbitrary\"` — tiles may share output locations\n",
    "- `tiles_k` is `\"arbitrary\"` — accumulation across K must be ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotated version of the tokamax custom_buffered_pallas_call\n",
    "# (Read and understand — no code to write)\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "def annotated_custom_buffered_pallas_call(kernel, out_shape, grid_spec,\n",
    "                                          compiler_params,\n",
    "                                          input_buffer_count=None, **kw):\n",
    "    \"\"\"Wraps a kernel with emit_pipeline for async DMA pipelining.\n",
    "\n",
    "    The outer pallas_call sees all data in HBM. Inside, emit_pipeline\n",
    "    creates a software-pipelined loop that overlaps DMA with compute.\n",
    "    \"\"\"\n",
    "    num_scalar_prefetch = grid_spec.num_scalar_prefetch\n",
    "\n",
    "    def pipeline(*args_refs):\n",
    "        # === Phase 1: Unpack grid and SMEM refs ===\n",
    "        smem_refs = args_refs[1 : num_scalar_prefetch + 1]\n",
    "\n",
    "        # === Phase 2: Bind SMEM refs to index maps ===\n",
    "        def _augment_blockspec(bs):\n",
    "            index_map_ = lambda *idxs: bs.index_map(*idxs, *smem_refs)\n",
    "            return pl.BlockSpec(bs.block_shape, index_map_)\n",
    "\n",
    "        in_specs = jax.tree.map(_augment_blockspec, grid_spec.in_specs)\n",
    "        out_specs = jax.tree.map(_augment_blockspec, grid_spec.out_specs)\n",
    "\n",
    "        # === Phase 3: Separate input/output/scratch refs ===\n",
    "        input_output_refs = args_refs[num_scalar_prefetch + 1:]\n",
    "\n",
    "        # === Phase 4: Emit the pipeline! ===\n",
    "        pltpu.emit_pipeline(\n",
    "            lambda *args: kernel(*smem_refs, *args),\n",
    "            grid=grid_spec.grid,\n",
    "            in_specs=in_specs,\n",
    "            out_specs=out_specs,\n",
    "            dimension_semantics=compiler_params.dimension_semantics,\n",
    "        )(*input_output_refs)\n",
    "\n",
    "    # The OUTER pallas_call has NO grid — single invocation.\n",
    "    return pl.pallas_call(\n",
    "        pipeline,\n",
    "        out_shape,\n",
    "        compiler_params=dataclasses.replace(compiler_params, dimension_semantics=()),\n",
    "        in_specs=(\n",
    "            jax.tree.map(lambda _: pl.BlockSpec(memory_space=pltpu.SMEM),\n",
    "                        tuple(range(num_scalar_prefetch + 1))),\n",
    "            jax.tree.map(lambda _: pl.BlockSpec(memory_space=pl.ANY),\n",
    "                        tuple(grid_spec.in_specs)),\n",
    "        ),\n",
    "        out_specs=jax.tree.map(lambda _: pl.BlockSpec(memory_space=pl.ANY),\n",
    "                               grid_spec.out_specs),\n",
    "        **kw,\n",
    "    )\n",
    "\n",
    "print(\"emit_pipeline annotated walkthrough loaded.\")\n",
    "print(\"Study the code above — on real TPU, this is what makes the kernel fast!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fa1fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Comprehension questions\n",
    "\n",
    "Answer these by reading the annotated code above:\n",
    "\n",
    "1. **Why does the outer `pallas_call` have no grid?** What happens\n",
    "   inside `emit_pipeline` that replaces the grid?\n",
    "\n",
    "2. **What does `_augment_blockspec` do?** Why can't we pass the\n",
    "   original `grid_spec.in_specs` directly to `emit_pipeline`?\n",
    "\n",
    "3. **Why is `tiles_n` labeled `\"parallel\"` but `num_tiles` and\n",
    "   `tiles_k` are `\"arbitrary\"`?** What would go wrong if we marked\n",
    "   `tiles_k` as `\"parallel\"`?\n",
    "\n",
    "4. **Double buffering requires 2× the VMEM.** Why is this trade-off\n",
    "   worth it on TPU?\n",
    "\n",
    "<details><summary>Answers</summary>\n",
    "\n",
    "1. The outer `pallas_call` runs once. Inside, `emit_pipeline` creates\n",
    "   its own software-pipelined loop over the grid, overlapping DMA with\n",
    "   compute. The grid iteration is \"inlined\" into the pipeline.\n",
    "\n",
    "2. `_augment_blockspec` rebinds the index maps to include the SMEM\n",
    "   refs. The original index maps expect `(grid_idx, *smem_refs)`,\n",
    "   but `emit_pipeline` only passes grid indices. The wrapper curries\n",
    "   in the SMEM refs.\n",
    "\n",
    "3. `\"parallel\"` means iterations are independent and can be reordered\n",
    "   or executed concurrently. `tiles_k` has accumulation dependencies —\n",
    "   K tile 2 adds to the same accumulator as K tile 1. Marking it\n",
    "   `\"parallel\"` would allow reordering, producing wrong results.\n",
    "   `num_tiles` has masked-store dependencies (boundary tiles are\n",
    "   visited by multiple groups sequentially).\n",
    "\n",
    "4. TPU HBM latency is high (~100s of cycles). Without pipelining,\n",
    "   the MXU sits idle during every DMA load. With double buffering,\n",
    "   the MXU computes on buffer A while DMA fills buffer B. The 2×\n",
    "   VMEM cost is small compared to the throughput gain (often 2-3×\n",
    "   higher MFU).\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
