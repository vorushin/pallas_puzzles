{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5991b266",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vorushin/pallas_puzzles/blob/master/basics.ipynb?flush_caches=true\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Pallas Puzzles: Basics\n",
    "\n",
    "**Pallas** is JAX's kernel language for writing custom operations that run on\n",
    "TPU (and GPU). Think of it as \"NumPy inside a tile\" — you write a function\n",
    "that operates on small blocks of data, and `pallas_call` maps that function\n",
    "across a grid of tiles covering the full arrays.\n",
    "\n",
    "This notebook contains **progressive puzzles** that build your Pallas \n",
    "intuition from scratch. Every puzzle runs on **CPU** via \n",
    "`interpret=True` — no TPU needed. Fill in the kernel skeletons and\n",
    "run the tests.\n",
    "\n",
    "**Prerequisites**: solid JAX/NumPy. No prior Pallas required.\n",
    "\n",
    "**Key Pallas docs**: https://docs.jax.dev/en/latest/pallas/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b7bc1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fa576",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jax jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f29048",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "print(f\"JAX {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bda3d",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: Foundations (Puzzles 1–7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfbd59",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 1: Hello Pallas — Constant Add\n",
    "\n",
    "**Goal**: Write a kernel that adds 10 to every element.\n",
    "\n",
    "### Theory\n",
    "\n",
    "A Pallas kernel is a Python function that receives **Ref** objects — typed\n",
    "pointers to blocks of memory. You read from a Ref with `ref[...]` (loads the\n",
    "entire block) and write with `ref[...] = value`. The `[...]` (Ellipsis)\n",
    "means \"all elements\" — it's the standard way to read or write an entire Ref\n",
    "in Pallas. You can also use slicing like `ref[0:4]`, but full `ref[...]`\n",
    "reads/writes are by far the most common pattern.\n",
    "\n",
    "`pallas_call` invokes your kernel once for each point in a **grid**. With an\n",
    "empty grid `()`, the kernel runs exactly once and sees the full arrays.\n",
    "\n",
    "```\n",
    "  x_ref  →  [ read ]\n",
    "                ↓\n",
    "           x + 10.0\n",
    "                ↓\n",
    "  o_ref  ←  [ write ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "\n",
    "# --- Reference (spec) ---\n",
    "def add10_spec(x):\n",
    "    \"\"\"x: (N,) → x + 10\"\"\"\n",
    "    return x + 10.0\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def add10_kernel(x_ref, o_ref):\n",
    "    # x_ref: Ref to input block (shape (N,))\n",
    "    # o_ref: Ref to output block (shape (N,))\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# --- Tests ---\n",
    "x = jax.random.uniform(jax.random.key(0), (N,))\n",
    "\n",
    "expected = add10_spec(x)\n",
    "actual = pl.pallas_call(\n",
    "    add10_kernel,\n",
    "    grid=(),  # single invocation — kernel sees the full arrays\n",
    "    out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),  # full output shape (not tile shape)\n",
    "    interpret=True,  # run on CPU (no TPU needed)\n",
    ")(x)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01b15a",
   "metadata": {},
   "source": [
    "<details><summary>Hint</summary>\n",
    "\n",
    "Read the entire input with `x_ref[...]`, add 10, write to `o_ref[...] = ...`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b459c",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 2: Tiled Vector Add\n",
    "\n",
    "**Goal**: Add two vectors using a 1D grid with block tiling.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In Puzzle 1 we used `grid=()` — an empty grid — so the kernel ran once\n",
    "and saw the entire array. That's fine for tiny inputs, but real TPU\n",
    "kernels work on arrays with millions of elements. We need to split\n",
    "them into **blocks** (also called tiles) and process one block per\n",
    "kernel invocation.\n",
    "\n",
    "That's what the **grid** is for. It's a tuple that says \"how many times\n",
    "to invoke the kernel, and along which dimensions\":\n",
    "\n",
    "```\n",
    "grid=()      → 1 invocation  (Puzzle 1)\n",
    "grid=(4,)    → 4 invocations, numbered i=0,1,2,3\n",
    "grid=(2, 3)  → 6 invocations, numbered (i,j) for i=0,1 and j=0,1,2\n",
    "```\n",
    "\n",
    "With `grid=(4,)` and a 256-element vector, we get 4 invocations that\n",
    "each process a 64-element block:\n",
    "\n",
    "```\n",
    "Array (256 elements):\n",
    "[████████ ████████ ████████ ████████]\n",
    " block 0   block 1   block 2   block 3\n",
    " i=0       i=1       i=2       i=3\n",
    "```\n",
    "\n",
    "But the grid alone only says *how many* invocations — it doesn't say\n",
    "which slice of the array each invocation sees. That's the job of\n",
    "**BlockSpec**, which pairs a block shape with an **index map**:\n",
    "\n",
    "```python\n",
    "BlockSpec(block_shape, index_map)\n",
    "```\n",
    "\n",
    "The index map is a function from grid indices → block position. For\n",
    "the simplest case, `lambda i: (i,)` means \"invocation `i` gets block `i`\":\n",
    "\n",
    "```\n",
    "grid=(4,)  +  BlockSpec((64,), lambda i: (i,))\n",
    "\n",
    "i=0 → index_map(0) = (0,) → array[0:64]\n",
    "i=1 → index_map(1) = (1,) → array[64:128]\n",
    "i=2 → index_map(2) = (2,) → array[128:192]\n",
    "i=3 → index_map(3) = (3,) → array[192:256]\n",
    "```\n",
    "\n",
    "Inside the kernel, `pl.program_id(axis)` returns the current grid index.\n",
    "But with `BlockSpec`, the Refs already point to the right block — so\n",
    "you often don't need `program_id` at all for element-wise ops!\n",
    "The kernel body stays identical whether you have 4 blocks or 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256   # vector length\n",
    "bm = 64   # tile (block) size — each kernel invocation processes bm elements\n",
    "\n",
    "# --- Reference ---\n",
    "def vadd_spec(x, y):\n",
    "    \"\"\"x, y: (N,) → x + y\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def vadd_kernel(x_ref, y_ref, o_ref):\n",
    "    # Each invocation sees a (bm,) slice thanks to BlockSpec\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# --- Tests ---\n",
    "x = jax.random.uniform(jax.random.key(1), (N,))\n",
    "y = jax.random.uniform(jax.random.key(2), (N,))\n",
    "\n",
    "expected = vadd_spec(x, y)\n",
    "actual = pl.pallas_call(\n",
    "    vadd_kernel,\n",
    "    grid=(N // bm,),              # 256 // 64 = 4 invocations\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((bm,), lambda i: (i,)),  # x: invocation i → block i\n",
    "        pl.BlockSpec((bm,), lambda i: (i,)),  # y: invocation i → block i\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm,), lambda i: (i,)),  # out: invocation i → block i\n",
    "    out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    interpret=True,\n",
    ")(x, y)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797a618",
   "metadata": {},
   "source": [
    "<details><summary>Hint</summary>\n",
    "\n",
    "The BlockSpecs handle all the slicing. Your kernel just needs:\n",
    "`o_ref[...] = x_ref[...] + y_ref[...]`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223a493",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 3: Reversed Block Add — Index Map Manipulation\n",
    "\n",
    "**Goal**: Add `x` to a **block-reversed** version of `y` by changing\n",
    "only the index map. The kernel body is identical to Puzzle 2!\n",
    "\n",
    "### Theory\n",
    "\n",
    "The index map in a `BlockSpec` controls **which block** each grid\n",
    "invocation sees. So far every index map was `lambda i: (i,)` — grid\n",
    "invocation `i` sees block `i` (sequential order). But the map can\n",
    "be any function: `lambda i: (3 - i,)` would read blocks in reverse.\n",
    "\n",
    "```\n",
    "y = [  y₀  |  y₁  |  y₂  |  y₃  ]      4 blocks, bm=64\n",
    "\n",
    "Normal index map     λi: (i,)         → y₀  y₁  y₂  y₃\n",
    "Reversed index map   λi: (3-i,)       → y₃  y₂  y₁  y₀\n",
    "\n",
    "x:          [  x₀  ][  x₁  ][  x₂  ][  x₃  ]\n",
    "y reversed: [  y₃  ][  y₂  ][  y₁  ][  y₀  ]\n",
    "result:     [x₀+y₃ ][x₁+y₂ ][x₂+y₁ ][x₃+y₀]\n",
    "```\n",
    "\n",
    "This is the key insight behind all advanced Pallas kernels: the index\n",
    "map decides what data the kernel sees, while the kernel body stays\n",
    "simple and generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e593c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256   # vector length (same as Puzzle 2)\n",
    "bm = 64   # tile size\n",
    "num_blocks = N // bm   # 4 blocks total\n",
    "\n",
    "# --- Reference ---\n",
    "def vadd_rev_spec(x, y):\n",
    "    \"\"\"x, y: (N,) → x + block_reverse(y)\"\"\"\n",
    "    y_rev = y.reshape(num_blocks, bm)[::-1].reshape(N)\n",
    "    return x + y_rev\n",
    "\n",
    "# Kernel is provided (same body as Puzzle 2):\n",
    "def vadd_rev_kernel(x_ref, y_ref, o_ref):\n",
    "    o_ref[...] = x_ref[...] + y_ref[...]\n",
    "\n",
    "# --- Tests ---\n",
    "x = jax.random.uniform(jax.random.key(100), (N,))\n",
    "y = jax.random.uniform(jax.random.key(101), (N,))\n",
    "\n",
    "# YOUR TASK: Fix the y BlockSpec so it reads blocks in reversed order.\n",
    "# Only the y index map needs to change — x and out are correct.\n",
    "expected = vadd_rev_spec(x, y)\n",
    "actual = pl.pallas_call(\n",
    "    vadd_rev_kernel,\n",
    "    grid=(num_blocks,),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((bm,), lambda i: (i,)),              # x: block i (correct)\n",
    "        pl.BlockSpec((bm,), lambda i: (i,)),              # y: block i — FIX THIS\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm,), lambda i: (i,)),\n",
    "    out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    interpret=True,\n",
    ")(x, y)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e3bd8",
   "metadata": {},
   "source": [
    "<details><summary>Hint</summary>\n",
    "\n",
    "The y index map should map grid index `i` to the reversed block position.\n",
    "With 4 blocks, `i=0 → block 3`, `i=1 → block 2`, etc.:\n",
    "```python\n",
    "pl.BlockSpec((bm,), lambda i: (num_blocks - 1 - i,))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c56c0",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 4: 2D Element-wise with 2D Grid\n",
    "\n",
    "**Goal**: Multiply every element of a 2D matrix by 2, using a 2D grid of\n",
    "blocks.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Grids can be multi-dimensional. A `grid=(4, 4)` creates 16 invocations,\n",
    "each indexed by `(i, j)`. Use `pl.program_id(0)` for `i` and\n",
    "`pl.program_id(1)` for `j`.\n",
    "\n",
    "BlockSpecs for 2D: `BlockSpec((bm, bn), lambda i, j: (i, j))`\n",
    "means \"tile `(i,j)` is the block at rows `[i*bm:(i+1)*bm]`,\n",
    "cols `[j*bn:(j+1)*bn]`\".\n",
    "\n",
    "**Key insight**: The kernel body is identical to Puzzle 2 — just\n",
    "`o_ref[...] = f(x_ref[...])`. The BlockSpec handles all the 2D\n",
    "indexing. This is the power of Pallas's tiling abstraction: the\n",
    "kernel doesn't care whether the grid is 1D, 2D, or 3D.\n",
    "\n",
    "![2D grid of blocks](https://raw.githubusercontent.com/vorushin/pallas_puzzles/master/images/basics-puzzle4.drawio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273001ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 128, 128\n",
    "bm, bn = 32, 32\n",
    "\n",
    "# --- Reference ---\n",
    "def mul2d_spec(x):\n",
    "    \"\"\"x: (M, N) → x * 2\"\"\"\n",
    "    return x * 2.0\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def mul2d_kernel(x_ref, o_ref):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# --- Tests ---\n",
    "x = jax.random.uniform(jax.random.key(3), (M, N))\n",
    "\n",
    "expected = mul2d_spec(x)\n",
    "actual = pl.pallas_call(\n",
    "    mul2d_kernel,\n",
    "    grid=(M // bm, N // bn),  # 2D grid — 4×4 = 16 invocations\n",
    "    in_specs=[pl.BlockSpec((bm, bn), lambda i, j: (i, j))],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda i, j: (i, j)),\n",
    "    out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    interpret=True,\n",
    ")(x)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae868d37",
   "metadata": {},
   "source": [
    "<details><summary>Hint</summary>\n",
    "\n",
    "Same as Puzzle 2 — `o_ref[...] = x_ref[...] * 2.0`. The 2D BlockSpec\n",
    "handles the tiling.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53211e78",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 5: Outer Product (Broadcasting Inside Kernels)\n",
    "\n",
    "**Goal**: Compute the outer product `a[:, None] * b[None, :]` for two\n",
    "vectors, producing a 2D matrix.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Inputs and output can have **different shapes**. Here:\n",
    "- `a`: shape `(M,)` → BlockSpec tiles along dim 0\n",
    "- `b`: shape `(N,)` → BlockSpec tiles along dim 0 (it's 1D)\n",
    "- `out`: shape `(M, N)` → BlockSpec tiles along both dims\n",
    "\n",
    "The index maps must line up correctly:\n",
    "- For `a`: grid `(i, j)` → tile `(i,)` (only depends on row)\n",
    "- For `b`: grid `(i, j)` → tile `(j,)` (only depends on col)\n",
    "- For `out`: grid `(i, j)` → tile `(i, j)`\n",
    "\n",
    "![Outer product tiling](https://raw.githubusercontent.com/vorushin/pallas_puzzles/master/images/basics-puzzle5.drawio.svg)\n",
    "\n",
    "Each tile (i,j): `a_ref` shape `(bm,)`, `b_ref` shape `(bn,)`\n",
    "→ broadcast to `(bm, bn)` via `[:, None] * [None, :]`\n",
    "\n",
    "Inside the kernel, `a_ref` has shape `(bm,)` and `b_ref` has shape `(bn,)`.\n",
    "You need to broadcast them: `a_ref[...][:, None] * b_ref[...][None, :]`\n",
    "produces shape `(bm, bn)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866009cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 128, 64\n",
    "bm, bn = 32, 32\n",
    "\n",
    "# --- Reference ---\n",
    "def outer_spec(a, b):\n",
    "    \"\"\"a: (M,), b: (N,) → (M, N)\"\"\"\n",
    "    return a[:, None] * b[None, :]\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def outer_kernel(a_ref, b_ref, o_ref):\n",
    "    # a_ref: (bm,) — a slice of vector a\n",
    "    # b_ref: (bn,) — a slice of vector b\n",
    "    # o_ref: (bm, bn) — output tile\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# --- Tests ---\n",
    "a = jax.random.uniform(jax.random.key(4), (M,))\n",
    "b = jax.random.uniform(jax.random.key(5), (N,))\n",
    "\n",
    "expected = outer_spec(a, b)\n",
    "actual = pl.pallas_call(\n",
    "    outer_kernel,\n",
    "    grid=(M // bm, N // bn),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((bm,), lambda i, j: (i,)),  # a: select by row i\n",
    "        pl.BlockSpec((bn,), lambda i, j: (j,)),  # b: select by col j\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda i, j: (i, j)),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(a, b)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2e786",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Approach</summary>\n",
    "\n",
    "You need to broadcast `a_ref[...]` (shape `(bm,)`) and `b_ref[...]` (shape `(bn,)`) to produce shape `(bm, bn)`. Use NumPy-style broadcasting: add a new axis with `[:, None]` and `[None, :]`.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "o_ref[...] = a_ref[...][:, None] * b_ref[...][None, :]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25298ee3",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 6: Configure Your Own `pallas_call` — Vector Add\n",
    "\n",
    "**Goal**: Given a working kernel, fill in the `grid`, `in_specs`, and\n",
    "`out_specs` arguments yourself.\n",
    "\n",
    "### Theory\n",
    "\n",
    "So far we've given you the `pallas_call` setup and you only wrote the\n",
    "kernel body. Now it's your turn to configure the call. You need:\n",
    "\n",
    "1. **`grid`**: a tuple specifying how many tiles in each dimension.\n",
    "   For a 1D vector of length `N` with tile size `bm`: `grid = (N // bm,)`.\n",
    "\n",
    "2. **`in_specs`**: a list of `BlockSpec`, one per input. Each says what\n",
    "   shape the kernel sees and how grid indices map to tile positions.\n",
    "\n",
    "3. **`out_specs`**: a single `BlockSpec` for the output.\n",
    "\n",
    "The kernel below is the solved version from Puzzle 2. Your task is to\n",
    "wire up the tiling so it processes `N`-element vectors in blocks of\n",
    "`bm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58833b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "bm = 64\n",
    "\n",
    "def vadd_spec6(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Kernel is provided (solved):\n",
    "def vadd_kernel_solved(x_ref, y_ref, o_ref):\n",
    "    o_ref[...] = x_ref[...] + y_ref[...]\n",
    "\n",
    "# --- Tests ---\n",
    "x = jax.random.uniform(jax.random.key(10), (N,))\n",
    "y = jax.random.uniform(jax.random.key(11), (N,))\n",
    "\n",
    "# YOUR TASK: Define grid, in_specs, out_specs to tile the computation\n",
    "# into bm-sized blocks. The kernel processes one block per invocation.\n",
    "vadd_grid = ...       # TODO: how many tiles? (should be a tuple)\n",
    "vadd_in_specs = ...   # TODO: list of BlockSpec, one per input\n",
    "vadd_out_specs = ...  # TODO: BlockSpec for output\n",
    "\n",
    "expected = vadd_spec6(x, y)\n",
    "actual = pl.pallas_call(\n",
    "    vadd_kernel_solved,\n",
    "    grid=vadd_grid,\n",
    "    in_specs=vadd_in_specs,\n",
    "    out_specs=vadd_out_specs,\n",
    "    out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    interpret=True,\n",
    ")(x, y)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b9ce9",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — What to fill in</summary>\n",
    "\n",
    "```python\n",
    "vadd_grid = (N // bm,)  # 256 // 64 = 4 tiles\n",
    "vadd_in_specs = [\n",
    "    pl.BlockSpec((bm,), lambda i: (i,)),  # one per input\n",
    "    pl.BlockSpec((bm,), lambda i: (i,)),\n",
    "]\n",
    "vadd_out_specs = pl.BlockSpec((bm,), lambda i: (i,))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "vadd_grid = (N // bm,)\n",
    "vadd_in_specs = [\n",
    "    pl.BlockSpec((bm,), lambda i: (i,)),\n",
    "    pl.BlockSpec((bm,), lambda i: (i,)),\n",
    "]\n",
    "vadd_out_specs = pl.BlockSpec((bm,), lambda i: (i,))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef1eb9",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 7: Reduction — Row Sum with `@pl.when`\n",
    "\n",
    "**Goal**: Sum each row of a matrix. The K dimension is tiled, so the\n",
    "kernel must **accumulate** partial sums across multiple invocations.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Matmul and many other operations have a **reduction dimension** (K) that\n",
    "gets summed over. In Pallas, we tile K and iterate:\n",
    "\n",
    "1. Each grid point `(i, k)` processes row-block `i`, K-block `k`\n",
    "2. On the first K-block (`k == 0`): **zero** the output\n",
    "3. On every K-block: **accumulate** the partial sum\n",
    "\n",
    "```\n",
    "x: (ROWS, COLS)\n",
    "    ┌──────┬──────┬──────┬──────┐\n",
    "r=0 │ k=0  │ k=1  │ k=2  │ k=3  │  → sum → out[0:bm]\n",
    "    ├──────┼──────┼──────┼──────┤\n",
    "r=1 │ k=0  │ k=1  │ k=2  │ k=3  │  → sum → out[bm:2*bm]\n",
    "    └──────┴──────┴──────┴──────┘\n",
    "```\n",
    "\n",
    "**`@pl.when(condition)`** is Pallas's conditional execution primitive.\n",
    "It compiles to **predicated execution** on TPU — no branch divergence\n",
    "penalty. Use it to guard operations that should only run on certain\n",
    "grid iterations:\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)           # only runs when k_i is 0\n",
    "def _():\n",
    "    acc[...] = jnp.zeros(...)\n",
    "```\n",
    "\n",
    "This is the key pattern for all reduction kernels: conditionally zero\n",
    "the accumulator on the first tile, accumulate on every tile, and\n",
    "(for matmul) conditionally store on the last tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS, COLS = 16, 256\n",
    "bm, bk = 16, 64\n",
    "tiles_k = COLS // bk\n",
    "\n",
    "# --- Reference ---\n",
    "def rowsum_spec(x):\n",
    "    \"\"\"x: (ROWS, COLS) → (ROWS,)\"\"\"\n",
    "    return x.sum(axis=1)\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def rowsum_kernel(x_ref, o_ref):\n",
    "    # x_ref: (bm, bk) — one tile of x\n",
    "    # o_ref: (bm,) — accumulator for this row block\n",
    "    # Grid: (ROWS // bm, COLS // bk) — iterates (row_block, k_block)\n",
    "    k_i = pl.program_id(1)\n",
    "    # YOUR CODE HERE\n",
    "    # 1. On first k tile (k_i == 0), initialize the output\n",
    "    # 2. Add this tile's contribution to the running sum\n",
    "\n",
    "# --- Tests ---\n",
    "x = jax.random.uniform(jax.random.key(6), (ROWS, COLS))\n",
    "expected = rowsum_spec(x)\n",
    "actual = pl.pallas_call(\n",
    "    rowsum_kernel,\n",
    "    grid=(ROWS // bm, tiles_k),  # iterate (row_blocks, k_blocks)\n",
    "    in_specs=[pl.BlockSpec((bm, bk), lambda i, k: (i, k))],\n",
    "    out_specs=pl.BlockSpec((bm,), lambda i, k: (i,)),  # no k — same tile across K iterations\n",
    "    out_shape=jax.ShapeDtypeStruct((ROWS,), jnp.float32),\n",
    "    interpret=True,\n",
    ")(x)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f9c9e",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — Approach</summary>\n",
    "\n",
    "Use `@pl.when(k_i == 0)` to conditionally zero the output on the first K tile. On every tile, accumulate the partial row sum with `o_ref[...] += x_ref[...].sum(axis=1)`.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Pattern skeleton</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    o_ref[...] = jnp.zeros((bm,), dtype=jnp.float32)\n",
    "\n",
    "o_ref[...] += ...  # partial row sum of x_ref\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    o_ref[...] = jnp.zeros((bm,), dtype=jnp.float32)\n",
    "\n",
    "o_ref[...] += x_ref[...].sum(axis=1)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47c660",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II: Matmul Patterns (Puzzles 8–11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5993499",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 8: Tiled Matmul with Scratch Accumulator\n",
    "\n",
    "**Goal**: Implement tiled matrix multiplication `C = A @ B` using a scratch\n",
    "buffer for accumulation across K tiles.\n",
    "\n",
    "### Theory\n",
    "\n",
    "This is the bread-and-butter of Pallas. Tiled matmul has a **3D grid**:\n",
    "`(tiles_m, tiles_n, tiles_k)`. For each `(m, n)` output tile, we iterate\n",
    "over K tiles (K for \"Kontracting\" dimension) and accumulate\n",
    "`A_tile @ B_tile`.\n",
    "\n",
    "![Tiled matmul block decomposition](https://raw.githubusercontent.com/vorushin/pallas_puzzles/master/images/basics-puzzle8.drawio.svg)\n",
    "\n",
    "We use **scratch memory** (`scratch_shapes`) for the accumulator.\n",
    "Scratch is allocated in **VMEM** — TPU's fast on-chip SRAM (like shared\n",
    "memory on GPU). Why not just accumulate directly in `o_ref`? Two reasons:\n",
    "1. **Performance**: `o_ref` points to HBM. Reading and writing it on\n",
    "   every K iteration means round-trips to slow off-chip memory.\n",
    "   Scratch stays in fast VMEM throughout all K iterations.\n",
    "2. **Correctness**: The output BlockSpec maps `(m, n, k) → (m, n)` —\n",
    "   multiple K iterations target the same output tile. Without a local\n",
    "   accumulator, K iteration 1 would overwrite K iteration 0's result.\n",
    "\n",
    "Specify scratch with `pltpu.VMEM(shape, dtype)`.\n",
    "\n",
    "**`out_shape`** tells `pallas_call` the shape and dtype of the output\n",
    "array to allocate. It's a `jax.ShapeDtypeStruct` — just metadata, no\n",
    "actual data:\n",
    "```python\n",
    "out_shape = jax.ShapeDtypeStruct((M, N), jnp.float32)\n",
    "```\n",
    "(In earlier puzzles, `out_shape` matched the input shape. Here the\n",
    "output shape `(M, N)` differs from either input, so it must be explicit.)\n",
    "\n",
    "Inside a kernel, use `a @ b` (or equivalently `jax.lax.dot(a, b)`) for\n",
    "the matrix multiply. Both map to the TPU's MXU (Matrix Multiplier Unit).\n",
    "\n",
    "The production-ready pattern uses `@pl.when` guards:\n",
    "```python\n",
    "@pl.when(k_i == 0)           # ZERO on first K tile\n",
    "def _(): acc[...] = zeros\n",
    "\n",
    "acc[...] += a @ b             # ACCUMULATE on every tile\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1) # STORE on last K tile\n",
    "def _(): out[...] = acc[...]\n",
    "```\n",
    "\n",
    "On TPU hardware, `@pl.when` compiles to predicated execution — no branch\n",
    "divergence penalty. This zero/accumulate/store pattern is used in every\n",
    "production Pallas kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b97a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, K, N = 128, 256, 128\n",
    "bm, bk, bn = 64, 128, 64\n",
    "tiles_m = M // bm\n",
    "tiles_n = N // bn\n",
    "tiles_k = K // bk\n",
    "\n",
    "# --- Reference ---\n",
    "def matmul_spec(a, b):\n",
    "    \"\"\"a: (M, K), b: (K, N) → (M, N)\"\"\"\n",
    "    return a @ b\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def matmul_kernel(a_ref, b_ref, o_ref, acc_ref):\n",
    "    # a_ref: (bm, bk) — tile of A\n",
    "    # b_ref: (bk, bn) — tile of B\n",
    "    # o_ref: (bm, bn) — output tile\n",
    "    # acc_ref: (bm, bn) — scratch accumulator (VMEM on TPU)\n",
    "    k_i = pl.program_id(2)\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Zero acc_ref when k_i == 0\n",
    "    # 2. Accumulate: acc_ref[...] += a_ref[...] @ b_ref[...]\n",
    "    # 3. Store acc_ref → o_ref when k_i == tiles_k - 1\n",
    "\n",
    "# --- Tests ---\n",
    "a = jax.random.normal(jax.random.key(7), (M, K))\n",
    "b = jax.random.normal(jax.random.key(8), (K, N))\n",
    "\n",
    "expected = matmul_spec(a, b)\n",
    "actual = pl.pallas_call(\n",
    "    matmul_kernel,\n",
    "    grid=(tiles_m, tiles_n, tiles_k),  # 3D grid: M tiles × N tiles × K tiles\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((bm, bk), lambda m, n, k: (m, k)),\n",
    "        pl.BlockSpec((bk, bn), lambda m, n, k: (k, n)),\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda m, n, k: (m, n)),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],  # accumulator in fast on-chip VMEM\n",
    "    interpret=True,\n",
    ")(a, b)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abefc73c",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Pattern skeleton</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "\n",
    "acc_ref[...] += ...  # A_tile @ B_tile\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    o_ref[...] = acc_ref[...]\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "\n",
    "acc_ref[...] += a_ref[...] @ b_ref[...]\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    o_ref[...] = acc_ref[...]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ffe7a",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 9: Configure Your Own Matmul `pallas_call`\n",
    "\n",
    "**Goal**: Given a working matmul kernel, fill in **all** the `pallas_call`\n",
    "arguments: `grid`, `in_specs`, `out_specs`, `out_shape`, and\n",
    "`scratch_shapes`.\n",
    "\n",
    "### Theory\n",
    "\n",
    "This is the next step in learning to configure `pallas_call` yourself.\n",
    "Unlike Puzzle 6 (1D vector add), matmul has a **3D grid** and requires\n",
    "scratch memory. You need to understand how `BlockSpec` index maps route\n",
    "tiles in a 3D grid:\n",
    "\n",
    "- `A` tile `(m, k)` is at `A[m*bm:(m+1)*bm, k*bk:(k+1)*bk]`\n",
    "  → index map: `lambda m, n, k: (m, k)`\n",
    "- `B` tile `(k, n)` is at `B[k*bk:(k+1)*bk, n*bn:(n+1)*bn]`\n",
    "  → index map: `lambda m, n, k: (k, n)`\n",
    "- `C` tile `(m, n)` is at `C[m*bm:(m+1)*bm, n*bn:(n+1)*bn]`\n",
    "  → index map: `lambda m, n, k: (m, n)` (no K dependency!)\n",
    "\n",
    "Don't forget `out_shape` (the full output shape, not the tile shape)\n",
    "and `scratch_shapes` (the VMEM accumulator from Puzzle 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, K, N = 128, 256, 128\n",
    "bm, bk, bn = 64, 128, 64\n",
    "tiles_m = M // bm\n",
    "tiles_n = N // bn\n",
    "tiles_k = K // bk\n",
    "\n",
    "def matmul_spec9(a, b):\n",
    "    return a @ b\n",
    "\n",
    "# Kernel is provided (solved — same pattern as Puzzle 8):\n",
    "def matmul_kernel_solved(a_ref, b_ref, o_ref, acc_ref):\n",
    "    k_i = pl.program_id(2)\n",
    "    @pl.when(k_i == 0)\n",
    "    def _zero():\n",
    "        acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "    acc_ref[...] += a_ref[...] @ b_ref[...]\n",
    "    @pl.when(k_i == tiles_k - 1)\n",
    "    def _store():\n",
    "        o_ref[...] = acc_ref[...]\n",
    "\n",
    "# --- Tests ---\n",
    "a = jax.random.normal(jax.random.key(20), (M, K))\n",
    "b = jax.random.normal(jax.random.key(21), (K, N))\n",
    "\n",
    "expected = matmul_spec9(a, b)\n",
    "\n",
    "# YOUR TASK: Replace ALL arguments with correct values.\n",
    "actual = pl.pallas_call(\n",
    "    matmul_kernel_solved,\n",
    "    grid=(),                   # FIX THIS — 3D grid (tiles_m, tiles_n, tiles_k)\n",
    "    in_specs=None,             # FIX THIS — BlockSpec for A and B\n",
    "    out_specs=None,            # FIX THIS — BlockSpec for C\n",
    "    out_shape=None,            # FIX THIS — output ShapeDtypeStruct\n",
    "    scratch_shapes=(),         # FIX THIS — VMEM scratch for accumulator\n",
    "    interpret=True,\n",
    ")(a, b)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a285d",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — What to fill in</summary>\n",
    "\n",
    "You need five things:\n",
    "- `grid = (tiles_m, tiles_n, tiles_k)`\n",
    "- `in_specs` with two BlockSpecs: A maps `(m,n,k)→(m,k)`, B maps `(m,n,k)→(k,n)`\n",
    "- `out_specs` maps `(m,n,k)→(m,n)`\n",
    "- `out_shape = jax.ShapeDtypeStruct((M, N), jnp.float32)`\n",
    "- `scratch_shapes = [pltpu.VMEM((bm, bn), jnp.float32)]`\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "actual = pl.pallas_call(\n",
    "    matmul_kernel_solved,\n",
    "    grid=(tiles_m, tiles_n, tiles_k),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((bm, bk), lambda m, n, k: (m, k)),\n",
    "        pl.BlockSpec((bk, bn), lambda m, n, k: (k, n)),\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda m, n, k: (m, n)),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "    interpret=True,\n",
    ")(a, b)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e7646",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 10: Batched Matmul\n",
    "\n",
    "**Goal**: Compute `out[b] = lhs[b] @ rhs[b]` for `B` independent batches.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Batched matmul is everywhere in ML — multi-head attention, batched\n",
    "inference, parallel projections. Both inputs have a leading batch\n",
    "dimension: `lhs` is `(B, M, K)` and `rhs` is `(B, K, N)`.\n",
    "\n",
    "The grid adds a **batch dimension**: `grid = (B,)`.\n",
    "Each iteration's BlockSpec selects one batch element at a time.\n",
    "\n",
    "![Batched matmul diagram](https://raw.githubusercontent.com/vorushin/pallas_puzzles/master/images/basics-puzzle10.drawio.svg)\n",
    "\n",
    "**`None` vs integer in block_shape**: Using `None` means \"load the entire\n",
    "axis and **squeeze** that dimension\". The ref will NOT have that dim.\n",
    "Using an integer (e.g. `1`) means \"load 1 element\" — the ref keeps that\n",
    "dim with size 1.\n",
    "\n",
    "For a batch dim, `None` is convenient — the kernel sees simple 2D\n",
    "shapes like `(M, K)` instead of `(1, M, K)`:\n",
    "```\n",
    "BlockSpec((None, M, K), lambda b: (b, 0, 0))\n",
    "           ^^^^\n",
    "           squeezed — ref shape is (M, K), not (1, M, K)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, M, K, N = 4, 64, 128, 64\n",
    "\n",
    "# --- Reference ---\n",
    "def batched_matmul_spec(lhs, rhs):\n",
    "    \"\"\"lhs: (B, M, K), rhs: (B, K, N) → (B, M, N)\"\"\"\n",
    "    return jnp.einsum('bmk,bkn->bmn', lhs, rhs)\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def batched_matmul_kernel(lhs_ref, rhs_ref, o_ref):\n",
    "    # With None in block_shape, the batch dim is squeezed:\n",
    "    # lhs_ref: (M, K) — one batch's lhs (batch dim squeezed)\n",
    "    # rhs_ref: (K, N) — one batch's rhs (batch dim squeezed)\n",
    "    # o_ref: (M, N) — one batch's output (batch dim squeezed)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# --- Tests ---\n",
    "lhs = jax.random.normal(jax.random.key(12), (B, M, K))\n",
    "rhs = jax.random.normal(jax.random.key(13), (B, K, N))\n",
    "\n",
    "expected = batched_matmul_spec(lhs, rhs)\n",
    "actual = pl.pallas_call(\n",
    "    batched_matmul_kernel,\n",
    "    grid=(B,),  # one invocation per batch element\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((None, M, K), lambda b: (b, 0, 0)),  # None squeezes batch dim → ref is (M, K)\n",
    "        pl.BlockSpec((None, K, N), lambda b: (b, 0, 0)),  # None squeezes batch dim → ref is (K, N)\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((None, M, N), lambda b: (b, 0, 0)),\n",
    "    # Full output is (B, M, N) even though each kernel invocation writes (M, N)\n",
    "    out_shape=jax.ShapeDtypeStruct((B, M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(lhs, rhs)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64ed15",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Approach</summary>\n",
    "\n",
    "With `None` in BlockSpec, the batch dimension is **squeezed** — the refs have shape `(M, K)` and `(K, N)` directly (no leading dim). So the kernel just needs a single matmul.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "o_ref[...] = lhs_ref[...] @ rhs_ref[...]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093bf8e4",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 11: Fused Matmul + ReLU\n",
    "\n",
    "**Goal**: Compute `ReLU(A @ B)` in a single fused kernel — matmul and\n",
    "activation in one pass, no intermediate materialization.\n",
    "\n",
    "### Theory\n",
    "\n",
    "On TPU, fusing operations into the kernel avoids an extra HBM round-trip.\n",
    "Without fusion: matmul writes `C` to HBM, then a separate kernel reads\n",
    "`C` back and applies ReLU. With fusion: ReLU is applied inside the kernel\n",
    "before the final store, saving one full read+write of the output matrix.\n",
    "\n",
    "The pattern is the same zero/accumulate/store from Puzzle 8, but the\n",
    "**store** step applies the activation before writing:\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    o_ref[...] = jnp.maximum(acc_ref[...], 0)  # fused ReLU!\n",
    "```\n",
    "\n",
    "This fusion pattern generalizes to any elementwise activation (GELU,\n",
    "SiLU, etc.) and is used in production MoE kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f930dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, K, N = 128, 256, 128\n",
    "bm, bk, bn = 64, 128, 64\n",
    "tiles_m = M // bm\n",
    "tiles_n = N // bn\n",
    "tiles_k = K // bk\n",
    "\n",
    "# --- Reference ---\n",
    "def fused_relu_spec(a, b):\n",
    "    \"\"\"a: (M, K), b: (K, N) → ReLU(a @ b)\"\"\"\n",
    "    return jnp.maximum(a @ b, 0)\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def fused_relu_kernel(a_ref, b_ref, o_ref, acc_ref):\n",
    "    k_i = pl.program_id(2)\n",
    "    # YOUR CODE HERE\n",
    "    # Same zero/accumulate/store as Puzzle 8, but apply ReLU before storing\n",
    "\n",
    "# --- Tests ---\n",
    "a = jax.random.normal(jax.random.key(22), (M, K))\n",
    "b = jax.random.normal(jax.random.key(23), (K, N))\n",
    "\n",
    "expected = fused_relu_spec(a, b)\n",
    "actual = pl.pallas_call(\n",
    "    fused_relu_kernel,\n",
    "    grid=(tiles_m, tiles_n, tiles_k),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((bm, bk), lambda m, n, k: (m, k)),\n",
    "        pl.BlockSpec((bk, bn), lambda m, n, k: (k, n)),\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bm, bn), lambda m, n, k: (m, n)),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "    interpret=True,\n",
    ")(a, b)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "else:\n",
    "    diff = jnp.abs(actual - expected)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff)):.6f}\")\n",
    "    print(f\"  Expected:\\n{expected[:4]}\")\n",
    "    print(f\"  Got:\\n{actual[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328574a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<details><summary>Hint 1 of 2 — Approach</summary>\n",
    "\n",
    "Copy the Puzzle 8 solution, but change the store step to apply `jnp.maximum(..., 0)` before writing to `o_ref`.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "\n",
    "acc_ref[...] += a_ref[...] @ b_ref[...]\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    o_ref[...] = jnp.maximum(acc_ref[...], 0)\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
