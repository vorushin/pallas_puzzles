{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a245d247",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vorushin/pallas_puzzles/blob/master/splash_attention.ipynb?flush_caches=true\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Pallas Puzzles: Splash Attention\n",
    "\n",
    "**8 progressive puzzles** building from basic dot-product attention to\n",
    "**splash attention** — JAX's production kernel for efficient\n",
    "block-sparse attention on TPU. Along the way you'll implement online\n",
    "softmax, flash attention, causal masking, and block-sparse dispatch.\n",
    "\n",
    "Every puzzle runs on **CPU** via `interpret=True` — no TPU needed.\n",
    "\n",
    "**Prerequisites**: Complete **basics.py** first (Pallas foundations and\n",
    "tiled matmul patterns). Puzzles 7–8 optionally reference scalar prefetch\n",
    "from **ragged_dot.py**.\n",
    "\n",
    "**Key references**:\n",
    "- [Flash Attention paper](https://arxiv.org/abs/2205.14135) (Dao et al., 2022)\n",
    "- [Online normalizer calculation](https://arxiv.org/abs/1805.02867) (Milakov & Gimelshein, 2018)\n",
    "- [JAX Pallas docs](https://docs.jax.dev/en/latest/pallas/index.html)\n",
    "- [JAX splash attention source](https://github.com/jax-ml/jax/tree/main/jax/experimental/pallas/ops/tpu/splash_attention)\n",
    "\n",
    "| Part | Puzzles | Focus |\n",
    "|------|---------|-------|\n",
    "| I — Flash Attention | 1–5 | Attention, online softmax, tiled flash attention |\n",
    "| II — Splash Attention | 6–8 | Causal masks, block-sparse dispatch, full splash |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bc6c6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "!pip install -q jax jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import functools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "print(f\"JAX {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ffca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title check() helper\n",
    "def check(kernel_fn, spec_fn, inputs, *, grid=(), in_specs=None, out_specs=None,\n",
    "          out_shape=None, scratch_shapes=(), atol=1e-3, rtol=1e-3, **kwargs):\n",
    "    \"\"\"Run a Pallas kernel in interpret mode and compare against a reference spec.\n",
    "\n",
    "    Args:\n",
    "        kernel_fn: The Pallas kernel to test.\n",
    "        spec_fn: Reference function computing the expected output in pure JAX.\n",
    "        inputs: Tuple of input arrays.\n",
    "        grid: Pallas grid tuple.\n",
    "        in_specs: List of BlockSpec for inputs (None = no blocking).\n",
    "        out_specs: BlockSpec for output (None = no blocking).\n",
    "        out_shape: jax.ShapeDtypeStruct for the output.\n",
    "        scratch_shapes: Scratch memory specs (empty by default).\n",
    "        atol, rtol: Tolerance for comparison.\n",
    "        **kwargs: Extra args to pl.pallas_call.\n",
    "    \"\"\"\n",
    "    expected = spec_fn(*inputs)\n",
    "    if out_shape is None:\n",
    "        out_shape = jax.ShapeDtypeStruct(expected.shape, expected.dtype)\n",
    "\n",
    "    # Handle default specs\n",
    "    if in_specs is None:\n",
    "        in_specs = [pl.BlockSpec(memory_space=pl.ANY)] * len(inputs)\n",
    "    if out_specs is None:\n",
    "        out_specs = pl.BlockSpec(memory_space=pl.ANY)\n",
    "\n",
    "    actual = pl.pallas_call(\n",
    "        kernel_fn,\n",
    "        grid=grid,\n",
    "        in_specs=in_specs,\n",
    "        out_specs=out_specs,\n",
    "        out_shape=out_shape,\n",
    "        scratch_shapes=scratch_shapes,\n",
    "        interpret=True,\n",
    "        **kwargs,\n",
    "    )(*inputs)\n",
    "\n",
    "    if jnp.allclose(actual, expected, atol=atol, rtol=rtol):\n",
    "        print(f\"PASSED ✓  (shape={actual.shape}, dtype={actual.dtype})\")\n",
    "    else:\n",
    "        diff = jnp.abs(actual - expected)\n",
    "        max_err = float(jnp.max(diff))\n",
    "        worst_idx = jnp.unravel_index(jnp.argmax(diff), diff.shape)\n",
    "        print(f\"FAILED ✗  max error: {max_err:.6f} at index {tuple(int(i) for i in worst_idx)}\")\n",
    "        n = min(4, expected.shape[0])\n",
    "        print(f\"  Expected (first {n}):\\n{expected[:n]}\")\n",
    "        print(f\"  Got      (first {n}):\\n{actual[:n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff07f4",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: Flash Attention (Puzzles 1–5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eae631",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 1: Dot-Product Attention\n",
    "\n",
    "**Goal**: Implement the standard attention equation in pure JAX (no Pallas\n",
    "yet). This is the **reference spec** that all later kernels must match.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Attention maps a query against a set of key-value pairs:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{Q K^T}{\\sqrt{d}}\\right) V$$\n",
    "\n",
    "where $Q, K, V \\in \\mathbb{R}^{T \\times d}$, $T$ is the sequence length\n",
    "and $d$ is the head dimension.\n",
    "\n",
    "The score matrix $S = Q K^T / \\sqrt{d}$ has shape $(T, T)$ — that's the\n",
    "**O(T²) memory bottleneck** we'll learn to eliminate. For a 4K-token\n",
    "sequence with 64-dim heads, $S$ alone is 64 MB in float32. At 128K tokens\n",
    "(common in modern LLMs), it would be **64 GB**. Clearly we can't\n",
    "materialize this matrix.\n",
    "\n",
    "```\n",
    "Q (T×d)     K^T (d×T)       S (T×T)          P (T×T)          O (T×d)\n",
    "┌──────┐   ┌──────────┐   ┌───────────┐    ┌───────────┐    ┌──────┐\n",
    "│      │   │          │   │           │    │ softmax   │    │      │\n",
    "│      │ @ │          │ = │  S / √d   │ →  │  rows     │ @  V  =  │  O   │\n",
    "│      │   │          │   │           │    │           │    │      │\n",
    "└──────┘   └──────────┘   └───────────┘    └───────────┘    └──────┘\n",
    " T × d       d × T          T × T            T × T           T × d\n",
    "                           ← O(T²) memory! →\n",
    "```\n",
    "\n",
    "Let's start by implementing this naive version, then spend the rest of the\n",
    "notebook learning to **never materialize S**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Diagram: Dot-Product Attention\n",
    "from IPython.display import SVG, display\n",
    "display(SVG(data='''<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 720 340\" font-family=\"monospace\" font-size=\"13\">\n",
    "  <rect width=\"720\" height=\"340\" fill=\"white\"/>\n",
    "\n",
    "  <!-- Q matrix -->\n",
    "  <rect x=\"30\" y=\"60\" width=\"60\" height=\"120\" rx=\"4\" fill=\"#dbeafe\" stroke=\"#3b82f6\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"60\" y=\"130\" text-anchor=\"middle\" fill=\"#1e40af\" font-size=\"12\">Q</text>\n",
    "  <text x=\"60\" y=\"200\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"10\">T × d</text>\n",
    "\n",
    "  <!-- × sign -->\n",
    "  <text x=\"110\" y=\"125\" text-anchor=\"middle\" fill=\"#374151\" font-size=\"18\">@</text>\n",
    "\n",
    "  <!-- K^T matrix -->\n",
    "  <rect x=\"130\" y=\"80\" width=\"120\" height=\"60\" rx=\"4\" fill=\"#dbeafe\" stroke=\"#3b82f6\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"190\" y=\"117\" text-anchor=\"middle\" fill=\"#1e40af\" font-size=\"12\">K^T</text>\n",
    "  <text x=\"190\" y=\"160\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"10\">d × T</text>\n",
    "\n",
    "  <!-- = sign -->\n",
    "  <text x=\"270\" y=\"125\" text-anchor=\"middle\" fill=\"#374151\" font-size=\"18\">=</text>\n",
    "\n",
    "  <!-- S matrix (big red square - THE BOTTLENECK) -->\n",
    "  <rect x=\"290\" y=\"40\" width=\"140\" height=\"140\" rx=\"4\" fill=\"#fef2f2\" stroke=\"#ef4444\" stroke-width=\"2.5\"/>\n",
    "  <text x=\"360\" y=\"115\" text-anchor=\"middle\" fill=\"#b91c1c\" font-weight=\"bold\" font-size=\"14\">S / √d</text>\n",
    "  <text x=\"360\" y=\"135\" text-anchor=\"middle\" fill=\"#dc2626\" font-size=\"11\">T × T</text>\n",
    "  <text x=\"360\" y=\"205\" text-anchor=\"middle\" fill=\"#dc2626\" font-weight=\"bold\" font-size=\"13\">⚠ O(T²) memory!</text>\n",
    "\n",
    "  <!-- Arrow down to P -->\n",
    "  <line x1=\"360\" y1=\"185\" x2=\"360\" y2=\"225\" stroke=\"#6b7280\" stroke-width=\"1\" marker-end=\"url(#arrow)\"/>\n",
    "  <text x=\"385\" y=\"215\" fill=\"#6b7280\" font-size=\"10\">softmax</text>\n",
    "\n",
    "  <!-- Arrow to P @ V -->\n",
    "  <text x=\"460\" y=\"270\" text-anchor=\"middle\" fill=\"#374151\" font-size=\"14\">→</text>\n",
    "\n",
    "  <!-- P matrix -->\n",
    "  <rect x=\"290\" y=\"230\" width=\"140\" height=\"50\" rx=\"4\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"360\" y=\"260\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"12\">P (T×T)</text>\n",
    "\n",
    "  <!-- × V -->\n",
    "  <text x=\"445\" y=\"260\" text-anchor=\"middle\" fill=\"#374151\" font-size=\"14\">@</text>\n",
    "  <rect x=\"465\" y=\"230\" width=\"50\" height=\"50\" rx=\"4\" fill=\"#dbeafe\" stroke=\"#3b82f6\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"490\" y=\"260\" text-anchor=\"middle\" fill=\"#1e40af\" font-size=\"12\">V</text>\n",
    "\n",
    "  <!-- = O -->\n",
    "  <text x=\"530\" y=\"260\" text-anchor=\"middle\" fill=\"#374151\" font-size=\"14\">=</text>\n",
    "  <rect x=\"545\" y=\"230\" width=\"60\" height=\"50\" rx=\"4\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"575\" y=\"260\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"12\">O</text>\n",
    "  <text x=\"575\" y=\"300\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"10\">T × d</text>\n",
    "\n",
    "  <!-- Title -->\n",
    "  <text x=\"360\" y=\"25\" text-anchor=\"middle\" fill=\"#111827\" font-weight=\"bold\" font-size=\"15\">Dot-Product Attention</text>\n",
    "\n",
    "  <!-- Arrow marker -->\n",
    "  <defs>\n",
    "    <marker id=\"arrow\" viewBox=\"0 0 10 10\" refX=\"9\" refY=\"5\" markerWidth=\"6\" markerHeight=\"6\" orient=\"auto\">\n",
    "      <path d=\"M 0 0 L 10 5 L 0 10 z\" fill=\"#6b7280\"/>\n",
    "    </marker>\n",
    "  </defs>\n",
    "</svg>'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = 128    # sequence length\n",
    "d1 = 64     # head dimension (per head)\n",
    "\n",
    "Q1 = jax.random.normal(jax.random.key(0), (T1, d1))\n",
    "K1 = jax.random.normal(jax.random.key(1), (T1, d1))\n",
    "V1 = jax.random.normal(jax.random.key(2), (T1, d1))\n",
    "\n",
    "\n",
    "# --- Reference ---\n",
    "def attention_spec(Q, K, V):\n",
    "    \"\"\"Standard dot-product attention: softmax(Q @ K.T / sqrt(d)) @ V\"\"\"\n",
    "    d = Q.shape[-1]\n",
    "    S = Q @ K.T / jnp.sqrt(d).astype(Q.dtype)\n",
    "    P = jax.nn.softmax(S, axis=-1)\n",
    "    return P @ V\n",
    "\n",
    "\n",
    "# --- Your implementation ---\n",
    "def my_attention(Q, K, V):\n",
    "    \"\"\"Implement: softmax(Q @ K.T / sqrt(d)) @ V\n",
    "\n",
    "    Steps:\n",
    "      1. Compute scores S = Q @ K^T, scaled by 1/sqrt(d)\n",
    "      2. Apply softmax along the last axis (over keys)\n",
    "      3. Multiply the attention weights P by V\n",
    "    \"\"\"\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1222c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected1 = attention_spec(Q1, K1, V1)\n",
    "actual1 = my_attention(Q1, K1, V1)\n",
    "\n",
    "if actual1 is not None and jnp.allclose(actual1, expected1, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual1.shape}, dtype={actual1.dtype})\")\n",
    "else:\n",
    "    print(\"FAILED ✗\")\n",
    "    if actual1 is not None:\n",
    "        print(f\"  Max error: {float(jnp.max(jnp.abs(actual1 - expected1))):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc29063",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Step by step</summary>\n",
    "\n",
    "```python\n",
    "d = Q.shape[-1]\n",
    "S = Q @ K.T / jnp.sqrt(d).astype(Q.dtype)   # (T, T) scores\n",
    "P = jax.nn.softmax(S, axis=-1)                # (T, T) weights\n",
    "return P @ V                                   # (T, d) output\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def my_attention(Q, K, V):\n",
    "    d = Q.shape[-1]\n",
    "    S = Q @ K.T / jnp.sqrt(d).astype(Q.dtype)\n",
    "    P = jax.nn.softmax(S, axis=-1)\n",
    "    return P @ V\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98356962",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 2: Tiled Softmax — The Denominator Problem\n",
    "\n",
    "**Goal**: Compute `softmax(x)` for a long vector using a Pallas kernel\n",
    "that processes tiles of the input, **accumulating the global max and\n",
    "sum(exp)** across tiles.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Numerically stable softmax has three steps:\n",
    "\n",
    "1. **Global max**: $m = \\max(x)$ — for numerical stability\n",
    "2. **Sum of exponentials**: $\\ell = \\sum_i \\exp(x_i - m)$\n",
    "3. **Normalize**: $\\text{softmax}(x)_i = \\exp(x_i - m) / \\ell$\n",
    "\n",
    "When `x` is too large for on-chip memory, we tile it. But here's the\n",
    "catch: step 1 needs to see *all* tiles before step 2 can start, and step\n",
    "2 needs to finish before step 3. That means **three separate passes**\n",
    "over the data from HBM:\n",
    "\n",
    "```\n",
    "Pass 1: HBM → SRAM → HBM    Find global max m\n",
    "         ──────────────→\n",
    "\n",
    "Pass 2: HBM → SRAM → HBM    Compute ℓ = Σ exp(xᵢ - m)\n",
    "         ──────────────→\n",
    "\n",
    "Pass 3: HBM → SRAM → HBM    Write exp(xᵢ - m) / ℓ\n",
    "         ──────────────→\n",
    "```\n",
    "\n",
    "Each pass reads the entire input from slow HBM. For attention, x is a\n",
    "*row* of the score matrix — and we do this for every row. That's a lot\n",
    "of HBM traffic. Can we do better? (Spoiler: yes — Puzzle 3.)\n",
    "\n",
    "For now, let's implement the honest 3-pass version. We'll use a Pallas\n",
    "kernel for the **reduction** part (passes 1 and 2 together — compute\n",
    "max and sum_exp in one pass since they can be combined), then apply the\n",
    "normalization.\n",
    "\n",
    "This kernel tiles over the K dimension using the zero/accumulate pattern\n",
    "from basics.py Puzzle 7:\n",
    "- `@pl.when(k == 0)`: initialize max and sum_exp\n",
    "- Every tile: update running max, accumulate sum of exponentials\n",
    "\n",
    "The kernel outputs **two scalars**: the global max `m` and the\n",
    "log-sum-exp `l = log(sum(exp(x - m)))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N2 = 512          # vector length\n",
    "bn2 = 128         # tile size\n",
    "tiles_k2 = N2 // bn2\n",
    "\n",
    "# --- Reference ---\n",
    "def softmax_spec(x):\n",
    "    \"\"\"x: (N2,) → (N2,)\"\"\"\n",
    "    return jax.nn.softmax(x)\n",
    "\n",
    "\n",
    "# --- Kernel: compute (max, sum_exp) via tiled reduction ---\n",
    "def softmax_stats_kernel(x_ref, m_ref, l_ref):\n",
    "    \"\"\"Tile over x to compute global max m and sum_exp l.\n",
    "\n",
    "    x_ref: (bn2,) — one tile of x\n",
    "    m_ref: ()     — running global max (scalar output)\n",
    "    l_ref: ()     — running sum of exp(x - m) (scalar output)\n",
    "\n",
    "    Grid: (tiles_k2,) — iterates over tiles of x\n",
    "    \"\"\"\n",
    "    k = pl.program_id(0)\n",
    "    pass  # YOUR CODE HERE\n",
    "    # 1. On first tile (k == 0): set m = max(tile), l = sum(exp(tile - m))\n",
    "    # 2. On later tiles: update m = max(m, max(tile)),\n",
    "    #    correct l for the new max, add new exponentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f77f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = jax.random.normal(jax.random.key(10), (N2,))\n",
    "\n",
    "# Run the stats kernel\n",
    "m2_shape = jax.ShapeDtypeStruct((), jnp.float32)\n",
    "l2_shape = jax.ShapeDtypeStruct((), jnp.float32)\n",
    "\n",
    "m2, l2 = pl.pallas_call(\n",
    "    softmax_stats_kernel,\n",
    "    grid=(tiles_k2,),\n",
    "    in_specs=[pl.BlockSpec((bn2,), lambda k: (k,))],\n",
    "    out_specs=(\n",
    "        pl.BlockSpec(memory_space=pl.ANY),  # m: scalar, no blocking\n",
    "        pl.BlockSpec(memory_space=pl.ANY),  # l: scalar, no blocking\n",
    "    ),\n",
    "    out_shape=(m2_shape, l2_shape),\n",
    "    interpret=True,\n",
    ")(x2)\n",
    "\n",
    "# Now use m and l to compute softmax (this part is given)\n",
    "softmax2 = jnp.exp(x2 - m2) / l2\n",
    "\n",
    "expected2 = softmax_spec(x2)\n",
    "if jnp.allclose(softmax2, expected2, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (m={float(m2):.3f}, l={float(l2):.3f})\")\n",
    "else:\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(jnp.abs(softmax2 - expected2))):.6f}\")\n",
    "    print(f\"  m={float(m2):.3f} (expected {float(jnp.max(x2)):.3f})\")\n",
    "    print(f\"  l={float(l2):.3f} (expected {float(jnp.sum(jnp.exp(x2 - jnp.max(x2)))):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ce0ab",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — First tile</summary>\n",
    "\n",
    "On the first tile, just compute the local statistics:\n",
    "```python\n",
    "@pl.when(k == 0)\n",
    "def _():\n",
    "    tile = x_ref[...]\n",
    "    m_ref[...] = jnp.max(tile)\n",
    "    l_ref[...] = jnp.sum(jnp.exp(tile - jnp.max(tile)))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Later tiles (the tricky part)</summary>\n",
    "\n",
    "When a new tile has a larger max, you need to **correct** the running sum:\n",
    "```python\n",
    "@pl.when(k > 0)\n",
    "def _():\n",
    "    tile = x_ref[...]\n",
    "    m_old = m_ref[...]\n",
    "    m_new = jnp.maximum(m_old, jnp.max(tile))\n",
    "    # Old exponentials were computed with m_old — rescale them\n",
    "    correction = jnp.exp(m_old - m_new)\n",
    "    l_ref[...] = l_ref[...] * correction + jnp.sum(jnp.exp(tile - m_new))\n",
    "    m_ref[...] = m_new\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def softmax_stats_kernel(x_ref, m_ref, l_ref):\n",
    "    k = pl.program_id(0)\n",
    "\n",
    "    @pl.when(k == 0)\n",
    "    def _():\n",
    "        tile = x_ref[...]\n",
    "        m_ref[...] = jnp.max(tile)\n",
    "        l_ref[...] = jnp.sum(jnp.exp(tile - jnp.max(tile)))\n",
    "\n",
    "    @pl.when(k > 0)\n",
    "    def _():\n",
    "        tile = x_ref[...]\n",
    "        m_old = m_ref[...]\n",
    "        m_new = jnp.maximum(m_old, jnp.max(tile))\n",
    "        correction = jnp.exp(m_old - m_new)\n",
    "        l_ref[...] = l_ref[...] * correction + jnp.sum(jnp.exp(tile - m_new))\n",
    "        m_ref[...] = m_new\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649d987",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 3: Online Softmax — One Pass to Rule Them All\n",
    "\n",
    "**Goal**: Compute softmax in a **single pass** by maintaining running\n",
    "statistics that get corrected on-the-fly as new tiles arrive.\n",
    "\n",
    "### Theory\n",
    "\n",
    "The 3-pass softmax from Puzzle 2 works, but it reads the data from HBM\n",
    "multiple times. **Online softmax** (Milakov & Gimelshein, 2018) is THE\n",
    "breakthrough that makes flash attention possible — it computes softmax\n",
    "in a **single pass** by maintaining running statistics `(m, ℓ)` that\n",
    "self-correct:\n",
    "\n",
    "```\n",
    "Initialize: m = -∞,  ℓ = 0\n",
    "\n",
    "For each tile xᵢ:\n",
    "    m_new = max(m, max(xᵢ))           ← update running max\n",
    "    correction = exp(m - m_new)        ← rescale factor for old stats\n",
    "    ℓ = ℓ · correction                 ← correct old sum\n",
    "      + Σⱼ exp(xᵢⱼ - m_new)           ← add new exponentials\n",
    "    m = m_new\n",
    "```\n",
    "\n",
    "The **correction factor** `exp(m_old - m_new)` is the magic. When a new\n",
    "tile has a bigger max, all previous exponentials need to be rescaled.\n",
    "Instead of going back and recomputing them, we just multiply the running\n",
    "sum by this factor. It works because:\n",
    "\n",
    "```\n",
    "exp(x - m_old) · exp(m_old - m_new) = exp(x - m_new)\n",
    "```\n",
    "\n",
    "After processing all tiles, `m` is the global max and `ℓ` is\n",
    "`sum(exp(x - m))` — exactly what softmax needs.\n",
    "\n",
    "Doesn't this look a lot like what you wrote in Puzzle 2? It is! But\n",
    "there's a crucial difference: in Puzzle 2 we used separate `@pl.when`\n",
    "branches for `k == 0` and `k > 0`. Here we initialize `m = -∞` and\n",
    "`ℓ = 0` and let the same update rule handle all tiles uniformly —\n",
    "including the first one. This unified formulation is what we'll use in\n",
    "flash attention.\n",
    "\n",
    "```\n",
    "Tile 0          Tile 1          Tile 2          Tile 3\n",
    "┌──────┐       ┌──────┐       ┌──────┐       ┌──────┐\n",
    "│ x[0] │  ───→ │ x[1] │  ───→ │ x[2] │  ───→ │ x[3] │\n",
    "└──────┘       └──────┘       └──────┘       └──────┘\n",
    "   │               │               │               │\n",
    "m=-∞, ℓ=0     update m,ℓ     update m,ℓ     update m,ℓ\n",
    "   │               │               │               │\n",
    "m=3.2, ℓ=47   m=3.5, ℓ=89   m=3.5, ℓ=134  m=3.7, ℓ=201\n",
    "                 ↑ correction!              ↑ correction!\n",
    "```\n",
    "\n",
    "**Your task**: Implement a Pallas kernel that computes `m` and `ℓ` in a\n",
    "single pass, then use them to compute the final softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cfbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Diagram: Online Softmax\n",
    "from IPython.display import SVG, display\n",
    "display(SVG(data='''<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 760 300\" font-family=\"monospace\" font-size=\"12\">\n",
    "  <rect width=\"760\" height=\"300\" fill=\"white\"/>\n",
    "\n",
    "  <!-- Title -->\n",
    "  <text x=\"380\" y=\"25\" text-anchor=\"middle\" fill=\"#111827\" font-weight=\"bold\" font-size=\"15\">Online Softmax — Single Pass</text>\n",
    "\n",
    "  <!-- Initial state -->\n",
    "  <text x=\"15\" y=\"90\" fill=\"#6b7280\" font-size=\"11\">m = -∞</text>\n",
    "  <text x=\"15\" y=\"108\" fill=\"#6b7280\" font-size=\"11\">ℓ = 0</text>\n",
    "\n",
    "  <!-- Tile 0 -->\n",
    "  <rect x=\"100\" y=\"60\" width=\"120\" height=\"60\" rx=\"6\" fill=\"#dbeafe\" stroke=\"#3b82f6\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"160\" y=\"85\" text-anchor=\"middle\" fill=\"#1e40af\" font-weight=\"bold\">Tile 0</text>\n",
    "  <text x=\"160\" y=\"105\" text-anchor=\"middle\" fill=\"#3b82f6\" font-size=\"10\">x[0:128]</text>\n",
    "  <text x=\"160\" y=\"145\" text-anchor=\"middle\" fill=\"#1e3a5f\" font-size=\"11\">m=2.1</text>\n",
    "  <text x=\"160\" y=\"163\" text-anchor=\"middle\" fill=\"#1e3a5f\" font-size=\"11\">ℓ=47.3</text>\n",
    "\n",
    "  <!-- Arrow 0→1 -->\n",
    "  <line x1=\"225\" y1=\"90\" x2=\"265\" y2=\"90\" stroke=\"#9ca3af\" stroke-width=\"1.5\" marker-end=\"url(#arr)\"/>\n",
    "\n",
    "  <!-- Tile 1 -->\n",
    "  <rect x=\"270\" y=\"60\" width=\"120\" height=\"60\" rx=\"6\" fill=\"#dbeafe\" stroke=\"#3b82f6\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"330\" y=\"85\" text-anchor=\"middle\" fill=\"#1e40af\" font-weight=\"bold\">Tile 1</text>\n",
    "  <text x=\"330\" y=\"105\" text-anchor=\"middle\" fill=\"#3b82f6\" font-size=\"10\">x[128:256]</text>\n",
    "  <text x=\"330\" y=\"145\" text-anchor=\"middle\" fill=\"#1e3a5f\" font-size=\"11\">m=3.4</text>\n",
    "  <text x=\"330\" y=\"163\" text-anchor=\"middle\" fill=\"#1e3a5f\" font-size=\"11\">ℓ=89.1</text>\n",
    "\n",
    "  <!-- Correction star 1 -->\n",
    "  <text x=\"330\" y=\"188\" text-anchor=\"middle\" fill=\"#f59e0b\" font-size=\"18\">★</text>\n",
    "  <text x=\"330\" y=\"205\" text-anchor=\"middle\" fill=\"#d97706\" font-size=\"10\" font-weight=\"bold\">correction!</text>\n",
    "  <text x=\"330\" y=\"218\" text-anchor=\"middle\" fill=\"#92400e\" font-size=\"9\">ℓ *= exp(2.1 - 3.4)</text>\n",
    "\n",
    "  <!-- Arrow 1→2 -->\n",
    "  <line x1=\"395\" y1=\"90\" x2=\"435\" y2=\"90\" stroke=\"#9ca3af\" stroke-width=\"1.5\" marker-end=\"url(#arr)\"/>\n",
    "\n",
    "  <!-- Tile 2 -->\n",
    "  <rect x=\"440\" y=\"60\" width=\"120\" height=\"60\" rx=\"6\" fill=\"#dbeafe\" stroke=\"#3b82f6\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"500\" y=\"85\" text-anchor=\"middle\" fill=\"#1e40af\" font-weight=\"bold\">Tile 2</text>\n",
    "  <text x=\"500\" y=\"105\" text-anchor=\"middle\" fill=\"#3b82f6\" font-size=\"10\">x[256:384]</text>\n",
    "  <text x=\"500\" y=\"145\" text-anchor=\"middle\" fill=\"#1e3a5f\" font-size=\"11\">m=3.4</text>\n",
    "  <text x=\"500\" y=\"163\" text-anchor=\"middle\" fill=\"#1e3a5f\" font-size=\"11\">ℓ=134.7</text>\n",
    "  <text x=\"500\" y=\"183\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"9\">(no correction)</text>\n",
    "\n",
    "  <!-- Arrow 2→3 -->\n",
    "  <line x1=\"565\" y1=\"90\" x2=\"605\" y2=\"90\" stroke=\"#9ca3af\" stroke-width=\"1.5\" marker-end=\"url(#arr)\"/>\n",
    "\n",
    "  <!-- Tile 3 -->\n",
    "  <rect x=\"610\" y=\"60\" width=\"120\" height=\"60\" rx=\"6\" fill=\"#dbeafe\" stroke=\"#3b82f6\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"670\" y=\"85\" text-anchor=\"middle\" fill=\"#1e40af\" font-weight=\"bold\">Tile 3</text>\n",
    "  <text x=\"670\" y=\"105\" text-anchor=\"middle\" fill=\"#3b82f6\" font-size=\"10\">x[384:512]</text>\n",
    "  <text x=\"670\" y=\"145\" text-anchor=\"middle\" fill=\"#1e3a5f\" font-size=\"11\">m=3.7</text>\n",
    "  <text x=\"670\" y=\"163\" text-anchor=\"middle\" fill=\"#1e3a5f\" font-size=\"11\">ℓ=201.5</text>\n",
    "\n",
    "  <!-- Correction star 3 -->\n",
    "  <text x=\"670\" y=\"188\" text-anchor=\"middle\" fill=\"#f59e0b\" font-size=\"18\">★</text>\n",
    "  <text x=\"670\" y=\"205\" text-anchor=\"middle\" fill=\"#d97706\" font-size=\"10\" font-weight=\"bold\">correction!</text>\n",
    "  <text x=\"670\" y=\"218\" text-anchor=\"middle\" fill=\"#92400e\" font-size=\"9\">ℓ *= exp(3.4 - 3.7)</text>\n",
    "\n",
    "  <!-- Final result -->\n",
    "  <rect x=\"200\" y=\"250\" width=\"360\" height=\"35\" rx=\"6\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"380\" y=\"273\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"12\">Final: m = global max, ℓ = Σ exp(xᵢ - m)  ✓ one pass!</text>\n",
    "\n",
    "  <defs>\n",
    "    <marker id=\"arr\" viewBox=\"0 0 10 10\" refX=\"9\" refY=\"5\" markerWidth=\"6\" markerHeight=\"6\" orient=\"auto\">\n",
    "      <path d=\"M 0 0 L 10 5 L 0 10 z\" fill=\"#9ca3af\"/>\n",
    "    </marker>\n",
    "  </defs>\n",
    "</svg>'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b87c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "N3 = 512\n",
    "bn3 = 128\n",
    "tiles_k3 = N3 // bn3\n",
    "\n",
    "# --- Reference ---\n",
    "def softmax_spec3(x):\n",
    "    \"\"\"x: (N3,) → (N3,)\"\"\"\n",
    "    return jax.nn.softmax(x)\n",
    "\n",
    "\n",
    "# --- Kernel: online softmax stats ---\n",
    "def online_softmax_kernel(x_ref, m_ref, l_ref):\n",
    "    \"\"\"Single-pass softmax stats: m = global max, l = sum(exp(x - m)).\n",
    "\n",
    "    x_ref: (bn3,) — one tile of x\n",
    "    m_ref: ()     — running global max (scalar)\n",
    "    l_ref: ()     — running sum of exponentials (scalar)\n",
    "\n",
    "    Grid: (tiles_k3,) — iterates over tiles of x\n",
    "\n",
    "    Unlike Puzzle 2, we use a unified update rule for ALL tiles:\n",
    "    - Initialize m = -inf, l = 0 on first tile\n",
    "    - Same correction logic handles both first and subsequent tiles\n",
    "    \"\"\"\n",
    "    k = pl.program_id(0)\n",
    "    pass  # YOUR CODE HERE\n",
    "    # 1. On first tile: initialize m_ref = -inf, l_ref = 0\n",
    "    # 2. On ALL tiles (including first): read tile, compute new max,\n",
    "    #    apply correction to l, add new exponentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af68983",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = jax.random.normal(jax.random.key(20), (N3,))\n",
    "\n",
    "m3, l3 = pl.pallas_call(\n",
    "    online_softmax_kernel,\n",
    "    grid=(tiles_k3,),\n",
    "    in_specs=[pl.BlockSpec((bn3,), lambda k: (k,))],\n",
    "    out_specs=(\n",
    "        pl.BlockSpec(memory_space=pl.ANY),\n",
    "        pl.BlockSpec(memory_space=pl.ANY),\n",
    "    ),\n",
    "    out_shape=(\n",
    "        jax.ShapeDtypeStruct((), jnp.float32),\n",
    "        jax.ShapeDtypeStruct((), jnp.float32),\n",
    "    ),\n",
    "    interpret=True,\n",
    ")(x3)\n",
    "\n",
    "softmax3 = jnp.exp(x3 - m3) / l3\n",
    "\n",
    "expected3 = softmax_spec3(x3)\n",
    "if jnp.allclose(softmax3, expected3, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (m={float(m3):.3f}, l={float(l3):.3f})\")\n",
    "else:\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(jnp.abs(softmax3 - expected3))):.6f}\")\n",
    "    print(f\"  m={float(m3):.3f} (expected {float(jnp.max(x3)):.3f})\")\n",
    "    print(f\"  l={float(l3):.3f} (expected {float(jnp.sum(jnp.exp(x3 - jnp.max(x3)))):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e3e05",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — The unified update rule</summary>\n",
    "\n",
    "Initialize on the first tile, then run the same update code on ALL tiles:\n",
    "```python\n",
    "@pl.when(k == 0)\n",
    "def _init():\n",
    "    m_ref[...] = jnp.float32(-jnp.inf)\n",
    "    l_ref[...] = jnp.float32(0.0)\n",
    "\n",
    "# This runs on EVERY tile (including first!)\n",
    "tile = x_ref[...]\n",
    "m_new = jnp.maximum(m_ref[...], jnp.max(tile))\n",
    "...\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Correction factor</summary>\n",
    "\n",
    "```python\n",
    "correction = jnp.exp(m_ref[...] - m_new)\n",
    "l_ref[...] = l_ref[...] * correction + jnp.sum(jnp.exp(tile - m_new))\n",
    "m_ref[...] = m_new\n",
    "```\n",
    "When `k == 0`: `m_ref = -inf`, so `correction = exp(-inf - m_new) = 0`,\n",
    "and `l_ref = 0 * 0 + sum(exp(tile - m_new))`. It just works!\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def online_softmax_kernel(x_ref, m_ref, l_ref):\n",
    "    k = pl.program_id(0)\n",
    "\n",
    "    @pl.when(k == 0)\n",
    "    def _init():\n",
    "        m_ref[...] = jnp.float32(-jnp.inf)\n",
    "        l_ref[...] = jnp.float32(0.0)\n",
    "\n",
    "    tile = x_ref[...]\n",
    "    m_new = jnp.maximum(m_ref[...], jnp.max(tile))\n",
    "    correction = jnp.exp(m_ref[...] - m_new)\n",
    "    l_ref[...] = l_ref[...] * correction + jnp.sum(jnp.exp(tile - m_new))\n",
    "    m_ref[...] = m_new\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbe849",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 4: Tiled Attention — One Q Block\n",
    "\n",
    "**Goal**: Compute the attention output for a **single block of Q rows**,\n",
    "iterating over all K,V blocks with online softmax to avoid materializing\n",
    "the full score matrix.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Now we combine online softmax with tiled matmul. This is the **core loop**\n",
    "of flash attention — processing one Q block against all KV blocks.\n",
    "\n",
    "For a Q block of shape `(bq, d)`, we iterate over KV blocks:\n",
    "\n",
    "```\n",
    "  Q block       K blocks (iterate →)      Output\n",
    "  ┌──────┐     ┌──────┬──────┬──────┬──────┐     ┌──────┐\n",
    "  │      │     │      │      │      │      │     │      │\n",
    "  │ bq×d │  @  │ bk×d │ bk×d │ bk×d │ bk×d │  →  │ bq×d │\n",
    "  │      │     │      │      │      │      │     │      │\n",
    "  └──────┘     └──────┴──────┴──────┴──────┘     └──────┘\n",
    "                kv=0    kv=1   kv=2   kv=3\n",
    "```\n",
    "\n",
    "For each KV block, the kernel:\n",
    "1. Computes scores: `s = Q_block @ K_block.T / sqrt(d)` → `(bq, bk)`\n",
    "2. Updates online softmax stats `(m, ℓ)` per row\n",
    "3. **Corrects** the running output accumulator for the new max\n",
    "4. Adds new contribution: `acc += P_block @ V_block`\n",
    "5. After last KV block: normalizes by `1/ℓ`\n",
    "\n",
    "The key insight is step 3: when the max changes, we must **rescale**\n",
    "the entire accumulator. Without this correction, outputs from earlier\n",
    "KV blocks would have the wrong scale:\n",
    "\n",
    "```python\n",
    "correction = exp(m_old - m_new)     # (bq,) per-row correction\n",
    "acc = acc * correction[:, None]     # rescale all d columns\n",
    "acc += P_block @ V_block            # add new contribution\n",
    "```\n",
    "\n",
    "After the last KV block, we normalize: `output = acc / ℓ[:, None]`.\n",
    "(This is because we've been accumulating unnormalized `exp(s - m) @ V`,\n",
    "and need to divide by the total `ℓ = sum(exp(s - m))` at the end.)\n",
    "\n",
    "**Scratch memory** holds three things:\n",
    "- `acc`: `(bq, d)` — running output accumulator\n",
    "- `m`: `(bq,)` — running max per row\n",
    "- `l`: `(bq,)` — running sum of exponentials per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30902b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "T4 = 128      # sequence length\n",
    "d4 = 64       # head dimension\n",
    "bq4 = 32      # Q block size\n",
    "bk4 = 32      # KV block size\n",
    "tiles_kv4 = T4 // bk4\n",
    "\n",
    "Q4 = jax.random.normal(jax.random.key(30), (T4, d4))\n",
    "K4 = jax.random.normal(jax.random.key(31), (T4, d4))\n",
    "V4 = jax.random.normal(jax.random.key(32), (T4, d4))\n",
    "\n",
    "# --- Reference: attention for just the first Q block ---\n",
    "def attention_one_block_spec(Q, K, V):\n",
    "    \"\"\"Attention output for first bq4 rows only.\"\"\"\n",
    "    q_block = Q[:bq4]                          # (bq4, d4)\n",
    "    S = q_block @ K.T / jnp.sqrt(d4).astype(Q.dtype)  # (bq4, T4)\n",
    "    P = jax.nn.softmax(S, axis=-1)             # (bq4, T4)\n",
    "    return P @ V                               # (bq4, d4)\n",
    "\n",
    "\n",
    "# --- Kernel: tiled attention for one Q block ---\n",
    "def tiled_attention_one_block_kernel(\n",
    "    q_ref,      # (bq4, d4) — the Q block (same for all KV iterations)\n",
    "    k_ref,      # (bk4, d4) — one KV block\n",
    "    v_ref,      # (bk4, d4) — one KV block\n",
    "    o_ref,      # (bq4, d4) — output\n",
    "    acc_ref,    # (bq4, d4) — scratch: running output accumulator\n",
    "    m_ref,      # (bq4,)    — scratch: running row max\n",
    "    l_ref,      # (bq4,)    — scratch: running row sum_exp\n",
    "):\n",
    "    \"\"\"Process one KV block for a single Q block using online softmax.\n",
    "\n",
    "    Grid: (tiles_kv4,) — iterates over KV blocks\n",
    "    \"\"\"\n",
    "    kv = pl.program_id(0)\n",
    "    pass  # YOUR CODE HERE\n",
    "    # 1. On first KV block: init acc=0, m=-inf, l=0\n",
    "    # 2. Compute scores: s = q @ k.T / sqrt(d4)       → (bq4, bk4)\n",
    "    # 3. Compute row-wise max of scores: m_tile        → (bq4,)\n",
    "    # 4. Update running max: m_new = max(m, m_tile)    → (bq4,)\n",
    "    # 5. Correction factor: corr = exp(m - m_new)      → (bq4,)\n",
    "    # 6. Rescale accumulator: acc *= corr[:, None]\n",
    "    # 7. Compute P_block = exp(s - m_new[:, None])     → (bq4, bk4)\n",
    "    # 8. Update l: l = l * corr + P_block.sum(axis=-1)\n",
    "    # 9. Accumulate: acc += P_block @ v\n",
    "    # 10. Update m = m_new\n",
    "    # 11. On LAST KV block: o = acc / l[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected4 = attention_one_block_spec(Q4, K4, V4)\n",
    "\n",
    "actual4 = pl.pallas_call(\n",
    "    tiled_attention_one_block_kernel,\n",
    "    grid=(tiles_kv4,),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((bq4, d4), lambda kv: (0, 0)),       # Q: always first block\n",
    "        pl.BlockSpec((bk4, d4), lambda kv: (kv, 0)),      # K: iterate over blocks\n",
    "        pl.BlockSpec((bk4, d4), lambda kv: (kv, 0)),      # V: iterate over blocks\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bq4, d4), lambda kv: (0, 0)),\n",
    "    out_shape=jax.ShapeDtypeStruct((bq4, d4), jnp.float32),\n",
    "    scratch_shapes=[\n",
    "        pltpu.VMEM((bq4, d4), jnp.float32),    # acc\n",
    "        pltpu.VMEM((bq4,), jnp.float32),        # m\n",
    "        pltpu.VMEM((bq4,), jnp.float32),        # l\n",
    "    ],\n",
    "    interpret=True,\n",
    ")(Q4, K4, V4)\n",
    "\n",
    "if jnp.allclose(actual4, expected4, atol=1e-2, rtol=1e-2):\n",
    "    print(f\"PASSED ✓  (shape={actual4.shape})\")\n",
    "else:\n",
    "    diff4 = jnp.abs(actual4 - expected4)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff4)):.6f}\")\n",
    "    print(f\"  Expected (first 2 rows):\\n{expected4[:2, :8]}\")\n",
    "    print(f\"  Got      (first 2 rows):\\n{actual4[:2, :8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1419d5b",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — Initialization and scores</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(kv == 0)\n",
    "def _init():\n",
    "    acc_ref[...] = jnp.zeros((bq4, d4), dtype=jnp.float32)\n",
    "    m_ref[...] = jnp.full((bq4,), -jnp.inf, dtype=jnp.float32)\n",
    "    l_ref[...] = jnp.zeros((bq4,), dtype=jnp.float32)\n",
    "\n",
    "q = q_ref[...]\n",
    "k = k_ref[...]\n",
    "v = v_ref[...]\n",
    "s = q @ k.T / jnp.sqrt(d4).astype(jnp.float32)  # (bq4, bk4)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Online softmax update with output correction</summary>\n",
    "\n",
    "```python\n",
    "m_tile = jnp.max(s, axis=-1)                    # (bq4,)\n",
    "m_new = jnp.maximum(m_ref[...], m_tile)          # (bq4,)\n",
    "corr = jnp.exp(m_ref[...] - m_new)               # (bq4,)\n",
    "\n",
    "acc_ref[...] = acc_ref[...] * corr[:, None]       # rescale old output\n",
    "p = jnp.exp(s - m_new[:, None])                   # (bq4, bk4)\n",
    "l_ref[...] = l_ref[...] * corr + p.sum(axis=-1)  # update sum_exp\n",
    "acc_ref[...] = acc_ref[...] + p @ v               # accumulate P @ V\n",
    "m_ref[...] = m_new\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def tiled_attention_one_block_kernel(q_ref, k_ref, v_ref, o_ref,\n",
    "                                      acc_ref, m_ref, l_ref):\n",
    "    kv = pl.program_id(0)\n",
    "\n",
    "    @pl.when(kv == 0)\n",
    "    def _init():\n",
    "        acc_ref[...] = jnp.zeros((bq4, d4), dtype=jnp.float32)\n",
    "        m_ref[...] = jnp.full((bq4,), -jnp.inf, dtype=jnp.float32)\n",
    "        l_ref[...] = jnp.zeros((bq4,), dtype=jnp.float32)\n",
    "\n",
    "    q = q_ref[...]\n",
    "    k = k_ref[...]\n",
    "    v = v_ref[...]\n",
    "    s = q @ k.T / jnp.sqrt(d4).astype(jnp.float32)\n",
    "\n",
    "    m_tile = jnp.max(s, axis=-1)\n",
    "    m_new = jnp.maximum(m_ref[...], m_tile)\n",
    "    corr = jnp.exp(m_ref[...] - m_new)\n",
    "\n",
    "    acc_ref[...] = acc_ref[...] * corr[:, None]\n",
    "    p = jnp.exp(s - m_new[:, None])\n",
    "    l_ref[...] = l_ref[...] * corr + p.sum(axis=-1)\n",
    "    acc_ref[...] = acc_ref[...] + p @ v\n",
    "    m_ref[...] = m_new\n",
    "\n",
    "    @pl.when(kv == tiles_kv4 - 1)\n",
    "    def _norm():\n",
    "        o_ref[...] = acc_ref[...] / l_ref[...][:, None]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef2472",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 5: Flash Attention Forward\n",
    "\n",
    "**Goal**: Extend Puzzle 4 to process **all Q blocks** via a 2D grid.\n",
    "This is full **flash attention** — computing exact attention without\n",
    "materializing the T×T score matrix.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In Puzzle 4, we handled one Q block. Now we grid over all Q blocks.\n",
    "The grid has two dimensions:\n",
    "\n",
    "```\n",
    "grid = (tiles_q, tiles_kv)\n",
    "        ↓         ↓\n",
    "    which Q    iterate over\n",
    "    block      all KV blocks\n",
    "```\n",
    "\n",
    "For each Q block `i`, the kernel sweeps through ALL KV blocks\n",
    "`kv ∈ [0, tiles_kv)`, maintaining per-row online softmax statistics.\n",
    "Each Q block is completely independent — they don't share state.\n",
    "\n",
    "```\n",
    "                   K blocks\n",
    "             kv=0  kv=1  kv=2  kv=3\n",
    "           ┌──────┬──────┬──────┬──────┐\n",
    " Q    i=0  │ s0,0 │ s0,1 │ s0,2 │ s0,3 │ → O[0:bq]     ← grid point (0, *)\n",
    "blocks     ├──────┼──────┼──────┼──────┤\n",
    "      i=1  │ s1,0 │ s1,1 │ s1,2 │ s1,3 │ → O[bq:2*bq]  ← grid point (1, *)\n",
    "           ├──────┼──────┼──────┼──────┤\n",
    "      i=2  │ s2,0 │ s2,1 │ s2,2 │ s2,3 │ → O[2*bq:3*bq] ← grid point (2, *)\n",
    "           ├──────┼──────┼──────┼──────┤\n",
    "      i=3  │ s3,0 │ s3,1 │ s3,2 │ s3,3 │ → O[3*bq:4*bq] ← grid point (3, *)\n",
    "           └──────┴──────┴──────┴──────┘\n",
    "\n",
    "We never materialize the full score matrix!\n",
    "Each block s[i,j] = Q_block_i @ K_block_j.T / √d  is (bq × bk)\n",
    "and lives only in SRAM for the duration of that grid point.\n",
    "```\n",
    "\n",
    "The kernel body is almost identical to Puzzle 4. The only differences:\n",
    "- `i = pl.program_id(0)` selects which Q block\n",
    "- `kv = pl.program_id(1)` iterates over KV blocks\n",
    "- BlockSpecs route Q/O by `i`, and K/V by `kv`\n",
    "\n",
    "**This is flash attention.** Same exact outputs as Puzzle 1, but O(T)\n",
    "memory instead of O(T²). The score matrix never exists in full — each\n",
    "`(bq, bk)` tile is computed, used, and discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Diagram: Flash Attention Grid\n",
    "from IPython.display import SVG, display\n",
    "display(SVG(data='''<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 580 460\" font-family=\"monospace\" font-size=\"12\">\n",
    "  <rect width=\"580\" height=\"460\" fill=\"white\"/>\n",
    "\n",
    "  <!-- Title -->\n",
    "  <text x=\"290\" y=\"25\" text-anchor=\"middle\" fill=\"#111827\" font-weight=\"bold\" font-size=\"15\">Flash Attention — Tiled Grid</text>\n",
    "\n",
    "  <!-- Column labels -->\n",
    "  <text x=\"195\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"10\">KV 0</text>\n",
    "  <text x=\"275\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"10\">KV 1</text>\n",
    "  <text x=\"355\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"10\">KV 2</text>\n",
    "  <text x=\"435\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"10\">KV 3</text>\n",
    "\n",
    "  <!-- Row labels -->\n",
    "  <text x=\"125\" y=\"100\" text-anchor=\"end\" fill=\"#6b7280\" font-size=\"10\">Q block 0</text>\n",
    "  <text x=\"125\" y=\"180\" text-anchor=\"end\" fill=\"#6b7280\" font-size=\"10\">Q block 1</text>\n",
    "  <text x=\"125\" y=\"260\" text-anchor=\"end\" fill=\"#6b7280\" font-size=\"10\">Q block 2</text>\n",
    "  <text x=\"125\" y=\"340\" text-anchor=\"end\" fill=\"#6b7280\" font-size=\"10\">Q block 3</text>\n",
    "\n",
    "  <!-- Grid cells - all blue since flash processes everything -->\n",
    "  <!-- Row 0 -->\n",
    "  <rect x=\"155\" y=\"65\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"235\" y=\"65\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"315\" y=\"65\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"395\" y=\"65\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "\n",
    "  <!-- Row 1 - highlighted as active -->\n",
    "  <rect x=\"155\" y=\"145\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#93c5fd\" stroke=\"#3b82f6\" stroke-width=\"2.5\"/>\n",
    "  <rect x=\"235\" y=\"145\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#93c5fd\" stroke=\"#3b82f6\" stroke-width=\"2.5\"/>\n",
    "  <rect x=\"315\" y=\"145\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#93c5fd\" stroke=\"#3b82f6\" stroke-width=\"2.5\"/>\n",
    "  <rect x=\"395\" y=\"145\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#93c5fd\" stroke=\"#3b82f6\" stroke-width=\"2.5\"/>\n",
    "\n",
    "  <!-- Active row arrow -->\n",
    "  <line x1=\"160\" y1=\"178\" x2=\"458\" y2=\"178\" stroke=\"#2563eb\" stroke-width=\"2\" stroke-dasharray=\"6,3\" marker-end=\"url(#bluearr)\"/>\n",
    "  <text x=\"490\" y=\"170\" fill=\"#2563eb\" font-size=\"10\" font-weight=\"bold\">iterate</text>\n",
    "  <text x=\"490\" y=\"185\" fill=\"#2563eb\" font-size=\"10\" font-weight=\"bold\">all KV →</text>\n",
    "\n",
    "  <!-- Row 2 -->\n",
    "  <rect x=\"155\" y=\"225\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"235\" y=\"225\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"315\" y=\"225\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"395\" y=\"225\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "\n",
    "  <!-- Row 3 -->\n",
    "  <rect x=\"155\" y=\"305\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"235\" y=\"305\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"315\" y=\"305\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "  <rect x=\"395\" y=\"305\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dbeafe\" stroke=\"#93c5fd\" stroke-width=\"1\"/>\n",
    "\n",
    "  <!-- Cell labels - bq×bk -->\n",
    "  <text x=\"190\" y=\"102\" text-anchor=\"middle\" fill=\"#3b82f6\" font-size=\"9\">bq×bk</text>\n",
    "  <text x=\"270\" y=\"102\" text-anchor=\"middle\" fill=\"#3b82f6\" font-size=\"9\">bq×bk</text>\n",
    "\n",
    "  <!-- Bottom label -->\n",
    "  <text x=\"290\" y=\"400\" text-anchor=\"middle\" fill=\"#374151\" font-size=\"11\">Each (bq×bk) score tile computed in SRAM, then discarded</text>\n",
    "  <text x=\"290\" y=\"418\" text-anchor=\"middle\" fill=\"#374151\" font-size=\"11\">Score matrix never fully materialized</text>\n",
    "\n",
    "  <!-- Memory badge -->\n",
    "  <rect x=\"210\" y=\"430\" width=\"160\" height=\"25\" rx=\"12\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"290\" y=\"448\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"12\">O(T) memory ✓</text>\n",
    "\n",
    "  <defs>\n",
    "    <marker id=\"bluearr\" viewBox=\"0 0 10 10\" refX=\"9\" refY=\"5\" markerWidth=\"6\" markerHeight=\"6\" orient=\"auto\">\n",
    "      <path d=\"M 0 0 L 10 5 L 0 10 z\" fill=\"#2563eb\"/>\n",
    "    </marker>\n",
    "  </defs>\n",
    "</svg>'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5 = 128\n",
    "d5 = 64\n",
    "bq5 = 32\n",
    "bk5 = 32\n",
    "tiles_q5 = T5 // bq5\n",
    "tiles_kv5 = T5 // bk5\n",
    "\n",
    "Q5 = jax.random.normal(jax.random.key(40), (T5, d5))\n",
    "K5 = jax.random.normal(jax.random.key(41), (T5, d5))\n",
    "V5 = jax.random.normal(jax.random.key(42), (T5, d5))\n",
    "\n",
    "# --- Reference ---\n",
    "def flash_attention_spec(Q, K, V):\n",
    "    \"\"\"Full attention: softmax(Q @ K.T / sqrt(d)) @ V\"\"\"\n",
    "    d = Q.shape[-1]\n",
    "    S = Q @ K.T / jnp.sqrt(d).astype(Q.dtype)\n",
    "    P = jax.nn.softmax(S, axis=-1)\n",
    "    return P @ V\n",
    "\n",
    "\n",
    "# --- Kernel ---\n",
    "def flash_attention_kernel(\n",
    "    q_ref,      # (bq5, d5)\n",
    "    k_ref,      # (bk5, d5)\n",
    "    v_ref,      # (bk5, d5)\n",
    "    o_ref,      # (bq5, d5) — output\n",
    "    acc_ref,    # (bq5, d5) — scratch: accumulator\n",
    "    m_ref,      # (bq5,)    — scratch: running max\n",
    "    l_ref,      # (bq5,)    — scratch: running sum_exp\n",
    "):\n",
    "    \"\"\"Flash attention kernel.\n",
    "\n",
    "    Grid: (tiles_q5, tiles_kv5)\n",
    "      - program_id(0) = i: which Q block\n",
    "      - program_id(1) = kv: which KV block (reduction dimension)\n",
    "    \"\"\"\n",
    "    i = pl.program_id(0)\n",
    "    kv = pl.program_id(1)\n",
    "    pass  # YOUR CODE HERE\n",
    "    # Same pattern as Puzzle 4, but now:\n",
    "    # - Use kv (not i) for the KV iteration\n",
    "    # - Init on kv == 0, normalize on kv == tiles_kv5 - 1\n",
    "    # - The BlockSpecs handle routing Q by i, K/V by kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e33f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check(flash_attention_kernel, flash_attention_spec, (Q5, K5, V5),\n",
    "      grid=(tiles_q5, tiles_kv5),\n",
    "      in_specs=[\n",
    "          pl.BlockSpec((bq5, d5), lambda i, kv: (i, 0)),    # Q: route by i\n",
    "          pl.BlockSpec((bk5, d5), lambda i, kv: (kv, 0)),   # K: route by kv\n",
    "          pl.BlockSpec((bk5, d5), lambda i, kv: (kv, 0)),   # V: route by kv\n",
    "      ],\n",
    "      out_specs=pl.BlockSpec((bq5, d5), lambda i, kv: (i, 0)),\n",
    "      out_shape=jax.ShapeDtypeStruct((T5, d5), jnp.float32),\n",
    "      scratch_shapes=[\n",
    "          pltpu.VMEM((bq5, d5), jnp.float32),   # acc\n",
    "          pltpu.VMEM((bq5,), jnp.float32),       # m\n",
    "          pltpu.VMEM((bq5,), jnp.float32),       # l\n",
    "      ],\n",
    "      atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a102c3",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — It's Puzzle 4 with different program_id</summary>\n",
    "\n",
    "The kernel body is identical to Puzzle 4. Just:\n",
    "- Use `kv = pl.program_id(1)` instead of `pl.program_id(0)`\n",
    "- Init on `kv == 0`, normalize on `kv == tiles_kv5 - 1`\n",
    "- The BlockSpecs handle routing — you don't need to think about `i` inside the kernel\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def flash_attention_kernel(q_ref, k_ref, v_ref, o_ref,\n",
    "                           acc_ref, m_ref, l_ref):\n",
    "    i = pl.program_id(0)\n",
    "    kv = pl.program_id(1)\n",
    "\n",
    "    @pl.when(kv == 0)\n",
    "    def _init():\n",
    "        acc_ref[...] = jnp.zeros((bq5, d5), dtype=jnp.float32)\n",
    "        m_ref[...] = jnp.full((bq5,), -jnp.inf, dtype=jnp.float32)\n",
    "        l_ref[...] = jnp.zeros((bq5,), dtype=jnp.float32)\n",
    "\n",
    "    q = q_ref[...]\n",
    "    k = k_ref[...]\n",
    "    v = v_ref[...]\n",
    "    s = q @ k.T / jnp.sqrt(d5).astype(jnp.float32)\n",
    "\n",
    "    m_tile = jnp.max(s, axis=-1)\n",
    "    m_new = jnp.maximum(m_ref[...], m_tile)\n",
    "    corr = jnp.exp(m_ref[...] - m_new)\n",
    "\n",
    "    acc_ref[...] = acc_ref[...] * corr[:, None]\n",
    "    p = jnp.exp(s - m_new[:, None])\n",
    "    l_ref[...] = l_ref[...] * corr + p.sum(axis=-1)\n",
    "    acc_ref[...] = acc_ref[...] + p @ v\n",
    "    m_ref[...] = m_new\n",
    "\n",
    "    @pl.when(kv == tiles_kv5 - 1)\n",
    "    def _norm():\n",
    "        o_ref[...] = acc_ref[...] / l_ref[...][:, None]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245d7c6",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II: Splash Attention (Puzzles 6–8)\n",
    "\n",
    "Flash attention processes every KV block for every Q block.\n",
    "But many attention patterns have **structure** — causal masking, sliding\n",
    "windows, block-sparse patterns. **Splash attention** exploits this\n",
    "structure by **skipping entire blocks** that would be fully masked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e2f16",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 6: Causal Masking\n",
    "\n",
    "**Goal**: Add a causal mask to flash attention, **skipping** blocks that\n",
    "are entirely above the diagonal.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In autoregressive models (GPT, LLaMA, etc.), token $i$ can only attend\n",
    "to tokens $j \\le i$. This triangular mask zeros out the upper-right\n",
    "portion of the score matrix.\n",
    "\n",
    "When we tile the score matrix into blocks, each block falls into one of\n",
    "three categories:\n",
    "\n",
    "```\n",
    "             KV blocks\n",
    "          kv=0  kv=1  kv=2  kv=3\n",
    "        ┌──────┬──────┬──────┬──────┐\n",
    "   i=0  │ PART │ SKIP │ SKIP │ SKIP │\n",
    "        ├──────┼──────┼──────┼──────┤\n",
    "   i=1  │ FULL │ PART │ SKIP │ SKIP │\n",
    " Q      ├──────┼──────┼──────┼──────┤\n",
    "blocks  │ FULL │ FULL │ PART │ SKIP │\n",
    "   i=2  ├──────┼──────┼──────┼──────┤\n",
    "   i=3  │ FULL │ FULL │ FULL │ PART │\n",
    "        └──────┴──────┴──────┴──────┘\n",
    "\n",
    " FULL  = all positions visible → compute normally\n",
    " PART  = diagonal block → apply element-wise mask\n",
    " SKIP  = all positions masked → skip entirely (don't load K,V!)\n",
    "```\n",
    "\n",
    "**When to skip**: Block `(i, kv)` is entirely above the diagonal when\n",
    "the **first Q row** in the block (= `i * bq`) is less than the **last\n",
    "K column** in the block (= `(kv + 1) * bk - 1`). Simplifying:\n",
    "`i * bq < kv * bk` (when bq == bk).\n",
    "\n",
    "**When to mask**: For partial blocks (on the diagonal), we create a\n",
    "boolean mask using global row/column indices:\n",
    "```python\n",
    "q_idx = i * bq + jnp.arange(bq)[:, None]     # (bq, 1)\n",
    "kv_idx = kv * bk + jnp.arange(bk)[None, :]   # (1, bk)\n",
    "causal_mask = q_idx >= kv_idx                  # (bq, bk) True where visible\n",
    "```\n",
    "Then apply: `s = jnp.where(causal_mask, s, -jnp.inf)` — masked positions\n",
    "become $-\\infty$ so `exp(−∞) = 0` in softmax.\n",
    "\n",
    "Skipping blocks is a huge win: for causal attention, ~half the blocks\n",
    "are skipped. And the `@pl.when` guard means K and V tiles for skipped\n",
    "blocks are **never loaded from HBM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12226d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Diagram: Causal Block Mask\n",
    "from IPython.display import SVG, display\n",
    "display(SVG(data='''<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 560 480\" font-family=\"monospace\" font-size=\"12\">\n",
    "  <rect width=\"560\" height=\"480\" fill=\"white\"/>\n",
    "\n",
    "  <!-- Title -->\n",
    "  <text x=\"280\" y=\"25\" text-anchor=\"middle\" fill=\"#111827\" font-weight=\"bold\" font-size=\"15\">Causal Block Mask</text>\n",
    "\n",
    "  <!-- Column labels -->\n",
    "  <text x=\"180\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"11\">kv=0</text>\n",
    "  <text x=\"260\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"11\">kv=1</text>\n",
    "  <text x=\"340\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"11\">kv=2</text>\n",
    "  <text x=\"420\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"11\">kv=3</text>\n",
    "\n",
    "  <!-- Row labels -->\n",
    "  <text x=\"115\" y=\"100\" text-anchor=\"end\" fill=\"#6b7280\" font-size=\"11\">i=0</text>\n",
    "  <text x=\"115\" y=\"180\" text-anchor=\"end\" fill=\"#6b7280\" font-size=\"11\">i=1</text>\n",
    "  <text x=\"115\" y=\"260\" text-anchor=\"end\" fill=\"#6b7280\" font-size=\"11\">i=2</text>\n",
    "  <text x=\"115\" y=\"340\" text-anchor=\"end\" fill=\"#6b7280\" font-size=\"11\">i=3</text>\n",
    "\n",
    "  <!-- Row 0: P S S S -->\n",
    "  <rect x=\"140\" y=\"65\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"175\" y=\"103\" text-anchor=\"middle\" fill=\"#854d0e\" font-weight=\"bold\" font-size=\"16\">P</text>\n",
    "  <rect x=\"220\" y=\"65\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"255\" y=\"103\" text-anchor=\"middle\" fill=\"#991b1b\" font-weight=\"bold\" font-size=\"16\">S</text>\n",
    "  <rect x=\"300\" y=\"65\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"335\" y=\"103\" text-anchor=\"middle\" fill=\"#991b1b\" font-weight=\"bold\" font-size=\"16\">S</text>\n",
    "  <rect x=\"380\" y=\"65\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"415\" y=\"103\" text-anchor=\"middle\" fill=\"#991b1b\" font-weight=\"bold\" font-size=\"16\">S</text>\n",
    "\n",
    "  <!-- Row 1: F P S S -->\n",
    "  <rect x=\"140\" y=\"145\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"175\" y=\"183\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"16\">F</text>\n",
    "  <rect x=\"220\" y=\"145\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"255\" y=\"183\" text-anchor=\"middle\" fill=\"#854d0e\" font-weight=\"bold\" font-size=\"16\">P</text>\n",
    "  <rect x=\"300\" y=\"145\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"335\" y=\"183\" text-anchor=\"middle\" fill=\"#991b1b\" font-weight=\"bold\" font-size=\"16\">S</text>\n",
    "  <rect x=\"380\" y=\"145\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"415\" y=\"183\" text-anchor=\"middle\" fill=\"#991b1b\" font-weight=\"bold\" font-size=\"16\">S</text>\n",
    "\n",
    "  <!-- Row 2: F F P S -->\n",
    "  <rect x=\"140\" y=\"225\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"175\" y=\"263\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"16\">F</text>\n",
    "  <rect x=\"220\" y=\"225\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"255\" y=\"263\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"16\">F</text>\n",
    "  <rect x=\"300\" y=\"225\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"335\" y=\"263\" text-anchor=\"middle\" fill=\"#854d0e\" font-weight=\"bold\" font-size=\"16\">P</text>\n",
    "  <rect x=\"380\" y=\"225\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"415\" y=\"263\" text-anchor=\"middle\" fill=\"#991b1b\" font-weight=\"bold\" font-size=\"16\">S</text>\n",
    "\n",
    "  <!-- Row 3: F F F P -->\n",
    "  <rect x=\"140\" y=\"305\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"175\" y=\"343\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"16\">F</text>\n",
    "  <rect x=\"220\" y=\"305\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"255\" y=\"343\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"16\">F</text>\n",
    "  <rect x=\"300\" y=\"305\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"335\" y=\"343\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"16\">F</text>\n",
    "  <rect x=\"380\" y=\"305\" width=\"70\" height=\"65\" rx=\"3\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"415\" y=\"343\" text-anchor=\"middle\" fill=\"#854d0e\" font-weight=\"bold\" font-size=\"16\">P</text>\n",
    "\n",
    "  <!-- Legend -->\n",
    "  <rect x=\"80\" y=\"400\" width=\"20\" height=\"20\" rx=\"3\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1\"/>\n",
    "  <text x=\"108\" y=\"415\" fill=\"#374151\" font-size=\"11\">F = FULL — compute normally</text>\n",
    "\n",
    "  <rect x=\"80\" y=\"428\" width=\"20\" height=\"20\" rx=\"3\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1\"/>\n",
    "  <text x=\"108\" y=\"443\" fill=\"#374151\" font-size=\"11\">P = PARTIAL — apply element mask</text>\n",
    "\n",
    "  <rect x=\"80\" y=\"456\" width=\"20\" height=\"20\" rx=\"3\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"108\" y=\"471\" fill=\"#374151\" font-size=\"11\">S = SKIP — don't load K,V!</text>\n",
    "\n",
    "  <!-- Stats -->\n",
    "  <text x=\"460\" y=\"415\" fill=\"#6b7280\" font-size=\"11\">6 of 16 SKIP</text>\n",
    "  <text x=\"460\" y=\"433\" fill=\"#dc2626\" font-weight=\"bold\" font-size=\"11\">~38% skipped</text>\n",
    "</svg>'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T6 = 128\n",
    "d6 = 64\n",
    "bq6 = 32\n",
    "bk6 = 32\n",
    "tiles_q6 = T6 // bq6\n",
    "tiles_kv6 = T6 // bk6\n",
    "\n",
    "Q6 = jax.random.normal(jax.random.key(50), (T6, d6))\n",
    "K6 = jax.random.normal(jax.random.key(51), (T6, d6))\n",
    "V6 = jax.random.normal(jax.random.key(52), (T6, d6))\n",
    "\n",
    "# --- Reference ---\n",
    "def causal_attention_spec(Q, K, V):\n",
    "    \"\"\"Attention with causal mask.\"\"\"\n",
    "    d = Q.shape[-1]\n",
    "    T = Q.shape[0]\n",
    "    S = Q @ K.T / jnp.sqrt(d).astype(Q.dtype)\n",
    "    mask = jnp.tril(jnp.ones((T, T), dtype=jnp.bool_))\n",
    "    S = jnp.where(mask, S, -jnp.inf)\n",
    "    P = jax.nn.softmax(S, axis=-1)\n",
    "    return P @ V\n",
    "\n",
    "\n",
    "# --- Kernel ---\n",
    "def causal_flash_kernel(\n",
    "    q_ref, k_ref, v_ref, o_ref,\n",
    "    acc_ref, m_ref, l_ref,\n",
    "):\n",
    "    \"\"\"Flash attention with causal masking.\n",
    "\n",
    "    Grid: (tiles_q6, tiles_kv6)\n",
    "\n",
    "    You need to handle three cases:\n",
    "    - SKIP: i * bq6 < kv * bk6 → do nothing\n",
    "    - FULL: (i + 1) * bq6 > (kv + 1) * bk6 → normal flash attention\n",
    "    - PARTIAL: diagonal block → apply causal mask to scores\n",
    "    \"\"\"\n",
    "    i = pl.program_id(0)\n",
    "    kv = pl.program_id(1)\n",
    "    pass  # YOUR CODE HERE\n",
    "    # 1. Init on kv == 0 (same as Puzzle 5)\n",
    "    # 2. Determine if this block should be skipped\n",
    "    # 3. @pl.when(should_compute): compute scores, apply mask if needed,\n",
    "    #    do online softmax update\n",
    "    # 4. Normalize on last kv block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc46542",
   "metadata": {},
   "outputs": [],
   "source": [
    "check(causal_flash_kernel, causal_attention_spec, (Q6, K6, V6),\n",
    "      grid=(tiles_q6, tiles_kv6),\n",
    "      in_specs=[\n",
    "          pl.BlockSpec((bq6, d6), lambda i, kv: (i, 0)),\n",
    "          pl.BlockSpec((bk6, d6), lambda i, kv: (kv, 0)),\n",
    "          pl.BlockSpec((bk6, d6), lambda i, kv: (kv, 0)),\n",
    "      ],\n",
    "      out_specs=pl.BlockSpec((bq6, d6), lambda i, kv: (i, 0)),\n",
    "      out_shape=jax.ShapeDtypeStruct((T6, d6), jnp.float32),\n",
    "      scratch_shapes=[\n",
    "          pltpu.VMEM((bq6, d6), jnp.float32),\n",
    "          pltpu.VMEM((bq6,), jnp.float32),\n",
    "          pltpu.VMEM((bq6,), jnp.float32),\n",
    "      ],\n",
    "      atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1462167",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — Block classification</summary>\n",
    "\n",
    "```python\n",
    "should_compute = (i * bq6 >= kv * bk6)    # not above diagonal\n",
    "is_full = ((i + 1) * bq6 > (kv + 1) * bk6)  # entirely below diagonal\n",
    "```\n",
    "When `should_compute` is False, the block is fully masked — skip it.\n",
    "When `is_full` is True, no per-element mask needed.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Applying the causal mask</summary>\n",
    "\n",
    "For partial (diagonal) blocks, create the mask with global indices:\n",
    "```python\n",
    "q_idx = i * bq6 + jnp.arange(bq6)[:, None]\n",
    "kv_idx = kv * bk6 + jnp.arange(bk6)[None, :]\n",
    "causal_mask = q_idx >= kv_idx\n",
    "s = jnp.where(causal_mask, s, -jnp.inf)\n",
    "```\n",
    "For full blocks, just use `s` as-is.\n",
    "You can unify: `s = jnp.where(causal_mask | is_full, s, -jnp.inf)`\n",
    "or use `lax.cond`.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def causal_flash_kernel(q_ref, k_ref, v_ref, o_ref,\n",
    "                         acc_ref, m_ref, l_ref):\n",
    "    i = pl.program_id(0)\n",
    "    kv = pl.program_id(1)\n",
    "\n",
    "    @pl.when(kv == 0)\n",
    "    def _init():\n",
    "        acc_ref[...] = jnp.zeros((bq6, d6), dtype=jnp.float32)\n",
    "        m_ref[...] = jnp.full((bq6,), -jnp.inf, dtype=jnp.float32)\n",
    "        l_ref[...] = jnp.zeros((bq6,), dtype=jnp.float32)\n",
    "\n",
    "    should_compute = (i * bq6 >= kv * bk6)\n",
    "\n",
    "    @pl.when(should_compute)\n",
    "    def _compute():\n",
    "        q = q_ref[...]\n",
    "        k = k_ref[...]\n",
    "        v = v_ref[...]\n",
    "        s = q @ k.T / jnp.sqrt(d6).astype(jnp.float32)\n",
    "\n",
    "        # Apply causal mask for partial blocks\n",
    "        q_idx = i * bq6 + jnp.arange(bq6)[:, None]\n",
    "        kv_idx = kv * bk6 + jnp.arange(bk6)[None, :]\n",
    "        causal_mask = q_idx >= kv_idx\n",
    "        s = jnp.where(causal_mask, s, -jnp.inf)\n",
    "\n",
    "        m_tile = jnp.max(s, axis=-1)\n",
    "        m_new = jnp.maximum(m_ref[...], m_tile)\n",
    "        corr = jnp.exp(m_ref[...] - m_new)\n",
    "\n",
    "        acc_ref[...] = acc_ref[...] * corr[:, None]\n",
    "        p = jnp.exp(s - m_new[:, None])\n",
    "        l_ref[...] = l_ref[...] * corr + p.sum(axis=-1)\n",
    "        acc_ref[...] = acc_ref[...] + p @ v\n",
    "        m_ref[...] = m_new\n",
    "\n",
    "    @pl.when(kv == tiles_kv6 - 1)\n",
    "    def _norm():\n",
    "        o_ref[...] = acc_ref[...] / l_ref[...][:, None]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45750d96",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 7: Block-Sparse Masks and Prefetch Maps\n",
    "\n",
    "**Goal**: Generalize causal masking to **arbitrary block-sparse patterns**.\n",
    "Use a `block_mask` array to classify blocks and a `data_next` map to\n",
    "iterate only over non-skipped blocks.\n",
    "\n",
    "### Theory\n",
    "\n",
    "Causal masking (Puzzle 6) hardcoded the skip/compute logic into the\n",
    "kernel. But production attention uses many patterns: causal, sliding\n",
    "window, local + global, block-sparse for long documents, etc. We need\n",
    "a **general mechanism**.\n",
    "\n",
    "**Splash attention** represents the mask as a precomputed **block_mask**\n",
    "array. For each Q block `i` and KV block `kv`, `block_mask[i, kv]`\n",
    "says:\n",
    "\n",
    "| Value | Meaning | Action |\n",
    "|-------|---------|--------|\n",
    "| 0 | SKIP | Don't load K,V — block is fully masked |\n",
    "| 1 | PARTIAL | Apply per-element mask (partial visibility) |\n",
    "| 2 | FULL | Compute normally (all positions visible) |\n",
    "\n",
    "In this puzzle, we keep the grid shape `(tiles_q, tiles_kv)` and use\n",
    "`block_mask` to decide what to do at each grid point — just like\n",
    "Puzzle 6, but reading the classification from an array instead of\n",
    "computing it.\n",
    "\n",
    "```\n",
    "   block_mask (precomputed)\n",
    "          kv=0  kv=1  kv=2  kv=3\n",
    "        ┌──────┬──────┬──────┬──────┐\n",
    "   i=0  │  1   │  0   │  0   │  0   │     0 = SKIP\n",
    "        ├──────┼──────┼──────┼──────┤     1 = PARTIAL\n",
    "   i=1  │  2   │  1   │  0   │  0   │     2 = FULL\n",
    "        ├──────┼──────┼──────┼──────┤\n",
    "   i=2  │  2   │  2   │  1   │  0   │\n",
    "        ├──────┼──────┼──────┼──────┤\n",
    "   i=3  │  2   │  2   │  2   │  1   │\n",
    "        └──────┴──────┴──────┴──────┘\n",
    "```\n",
    "\n",
    "For the mask itself on partial blocks, we precompute per-element mask\n",
    "arrays and store them in `partial_mask_blocks`. The kernel looks up which\n",
    "partial mask to use for diagonal blocks.\n",
    "\n",
    "**Why precompute the mask?** On TPU, branches are expensive. By\n",
    "precomputing block_mask on the host, the kernel just reads integers and\n",
    "acts accordingly — no complex logic inside the kernel.\n",
    "\n",
    "For this puzzle, we'll use `block_mask` as a regular input (passed via\n",
    "BlockSpec). In Puzzle 8, we'll upgrade to `PrefetchScalarGridSpec` for\n",
    "production-grade dispatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831fb734",
   "metadata": {},
   "outputs": [],
   "source": [
    "T7 = 128\n",
    "d7 = 64\n",
    "bq7 = 32\n",
    "bk7 = 32\n",
    "tiles_q7 = T7 // bq7\n",
    "tiles_kv7 = T7 // bk7\n",
    "\n",
    "Q7 = jax.random.normal(jax.random.key(60), (T7, d7))\n",
    "K7 = jax.random.normal(jax.random.key(61), (T7, d7))\n",
    "V7 = jax.random.normal(jax.random.key(62), (T7, d7))\n",
    "\n",
    "# --- Build causal block_mask ---\n",
    "# 0 = SKIP, 1 = PARTIAL (diagonal), 2 = FULL (below diagonal)\n",
    "block_mask7 = jnp.zeros((tiles_q7, tiles_kv7), dtype=jnp.int32)\n",
    "for qi in range(tiles_q7):\n",
    "    for kvi in range(tiles_kv7):\n",
    "        if qi * bq7 < kvi * bk7:\n",
    "            block_mask7 = block_mask7.at[qi, kvi].set(0)    # SKIP\n",
    "        elif (qi + 1) * bq7 > (kvi + 1) * bk7:\n",
    "            block_mask7 = block_mask7.at[qi, kvi].set(2)    # FULL\n",
    "        else:\n",
    "            block_mask7 = block_mask7.at[qi, kvi].set(1)    # PARTIAL\n",
    "\n",
    "print(\"block_mask7:\")\n",
    "print(block_mask7)\n",
    "\n",
    "# --- Build partial mask blocks ---\n",
    "# For each diagonal block, precompute the per-element causal mask.\n",
    "# We store them as a list indexed by the diagonal block number.\n",
    "num_partial7 = int(min(tiles_q7, tiles_kv7))\n",
    "partial_masks7 = jnp.zeros((num_partial7, bq7, bk7), dtype=jnp.bool_)\n",
    "partial_idx = 0\n",
    "for qi in range(tiles_q7):\n",
    "    for kvi in range(tiles_kv7):\n",
    "        if int(block_mask7[qi, kvi]) == 1:\n",
    "            q_idx = qi * bq7 + jnp.arange(bq7)[:, None]\n",
    "            kv_idx = kvi * bk7 + jnp.arange(bk7)[None, :]\n",
    "            partial_masks7 = partial_masks7.at[partial_idx].set(q_idx >= kv_idx)\n",
    "            partial_idx += 1\n",
    "\n",
    "print(f\"\\npartial_masks7 shape: {partial_masks7.shape} ({partial_idx} partial blocks)\")\n",
    "\n",
    "\n",
    "# --- Reference ---\n",
    "def block_sparse_attention_spec(Q, K, V, block_mask, partial_masks):\n",
    "    \"\"\"Attention with block-sparse mask (reference implementation).\"\"\"\n",
    "    d = Q.shape[-1]\n",
    "    T = Q.shape[0]\n",
    "    S = Q @ K.T / jnp.sqrt(d).astype(Q.dtype)\n",
    "    # Reconstruct full mask from block_mask + partial_masks\n",
    "    full_mask = jnp.zeros((T, T), dtype=jnp.bool_)\n",
    "    pidx = 0\n",
    "    for qi in range(tiles_q7):\n",
    "        for kvi in range(tiles_kv7):\n",
    "            bm = int(block_mask[qi, kvi])\n",
    "            r0, r1 = qi * bq7, (qi + 1) * bq7\n",
    "            c0, c1 = kvi * bk7, (kvi + 1) * bk7\n",
    "            if bm == 2:\n",
    "                full_mask = full_mask.at[r0:r1, c0:c1].set(True)\n",
    "            elif bm == 1:\n",
    "                full_mask = full_mask.at[r0:r1, c0:c1].set(partial_masks[pidx])\n",
    "                pidx += 1\n",
    "    S = jnp.where(full_mask, S, -jnp.inf)\n",
    "    P = jax.nn.softmax(S, axis=-1)\n",
    "    return P @ V\n",
    "\n",
    "\n",
    "# --- Kernel ---\n",
    "def block_sparse_flash_kernel(\n",
    "    q_ref, k_ref, v_ref,\n",
    "    block_mask_ref,        # (tiles_kv7,) — mask row for this Q block\n",
    "    partial_masks_ref,     # (num_partial7, bq7, bk7) — all partial masks\n",
    "    o_ref,\n",
    "    acc_ref, m_ref, l_ref,\n",
    "):\n",
    "    \"\"\"Flash attention with block-sparse mask from block_mask array.\n",
    "\n",
    "    Grid: (tiles_q7, tiles_kv7)\n",
    "\n",
    "    block_mask_ref: row of block_mask for current Q block\n",
    "      - 0 = skip, 1 = partial, 2 = full\n",
    "    partial_masks_ref: precomputed element masks for partial blocks\n",
    "    \"\"\"\n",
    "    i = pl.program_id(0)\n",
    "    kv = pl.program_id(1)\n",
    "    pass  # YOUR CODE HERE\n",
    "    # 1. Init on kv == 0\n",
    "    # 2. Read block_mask_ref[kv] to get block type\n",
    "    # 3. @pl.when(block_type > 0): compute attention\n",
    "    #    - If block_type == 1: apply partial mask from partial_masks_ref\n",
    "    #    - If block_type == 2: no mask needed (full visibility)\n",
    "    # 4. Normalize on last kv block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected7 = block_sparse_attention_spec(Q7, K7, V7, block_mask7, partial_masks7)\n",
    "\n",
    "# For partial block indexing, we need to know which partial mask index\n",
    "# corresponds to each diagonal block. For causal: block (i,i) is the\n",
    "# i-th partial block.\n",
    "# We pass block_mask row-by-row via BlockSpec.\n",
    "actual7 = pl.pallas_call(\n",
    "    block_sparse_flash_kernel,\n",
    "    grid=(tiles_q7, tiles_kv7),\n",
    "    in_specs=[\n",
    "        pl.BlockSpec((bq7, d7), lambda i, kv: (i, 0)),              # Q\n",
    "        pl.BlockSpec((bk7, d7), lambda i, kv: (kv, 0)),             # K\n",
    "        pl.BlockSpec((bk7, d7), lambda i, kv: (kv, 0)),             # V\n",
    "        pl.BlockSpec((tiles_kv7,), lambda i, kv: (i,)),             # block_mask row\n",
    "        pl.BlockSpec(memory_space=pl.ANY),                           # partial_masks (full)\n",
    "    ],\n",
    "    out_specs=pl.BlockSpec((bq7, d7), lambda i, kv: (i, 0)),\n",
    "    out_shape=jax.ShapeDtypeStruct((T7, d7), jnp.float32),\n",
    "    scratch_shapes=[\n",
    "        pltpu.VMEM((bq7, d7), jnp.float32),\n",
    "        pltpu.VMEM((bq7,), jnp.float32),\n",
    "        pltpu.VMEM((bq7,), jnp.float32),\n",
    "    ],\n",
    "    interpret=True,\n",
    ")(Q7, K7, V7, block_mask7, partial_masks7)\n",
    "\n",
    "if jnp.allclose(actual7, expected7, atol=1e-2, rtol=1e-2):\n",
    "    print(f\"PASSED ✓  (shape={actual7.shape})\")\n",
    "else:\n",
    "    diff7 = jnp.abs(actual7 - expected7)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff7)):.6f}\")\n",
    "    print(f\"  Expected (first 2 rows):\\n{expected7[:2, :8]}\")\n",
    "    print(f\"  Got      (first 2 rows):\\n{actual7[:2, :8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727882d9",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — Reading the block mask</summary>\n",
    "\n",
    "`block_mask_ref` contains the entire mask row for this Q block. Read\n",
    "the entry for the current KV block:\n",
    "```python\n",
    "block_type = block_mask_ref[kv]    # 0, 1, or 2\n",
    "should_compute = block_type > 0\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Applying partial masks</summary>\n",
    "\n",
    "For causal masking, the partial block for Q block `i` is at index `i`\n",
    "in partial_masks_ref:\n",
    "```python\n",
    "mask = partial_masks_ref[i]  # (bq7, bk7) boolean mask\n",
    "s = jnp.where(mask, s, -jnp.inf)\n",
    "```\n",
    "For the full case, skip the mask step (or use `True` mask).\n",
    "Unify with: `is_partial = (block_type == 1)`\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def block_sparse_flash_kernel(q_ref, k_ref, v_ref,\n",
    "                               block_mask_ref, partial_masks_ref,\n",
    "                               o_ref, acc_ref, m_ref, l_ref):\n",
    "    i = pl.program_id(0)\n",
    "    kv = pl.program_id(1)\n",
    "\n",
    "    @pl.when(kv == 0)\n",
    "    def _init():\n",
    "        acc_ref[...] = jnp.zeros((bq7, d7), dtype=jnp.float32)\n",
    "        m_ref[...] = jnp.full((bq7,), -jnp.inf, dtype=jnp.float32)\n",
    "        l_ref[...] = jnp.zeros((bq7,), dtype=jnp.float32)\n",
    "\n",
    "    block_type = block_mask_ref[kv]\n",
    "    should_compute = block_type > 0\n",
    "\n",
    "    @pl.when(should_compute)\n",
    "    def _compute():\n",
    "        q = q_ref[...]\n",
    "        k = k_ref[...]\n",
    "        v = v_ref[...]\n",
    "        s = q @ k.T / jnp.sqrt(d7).astype(jnp.float32)\n",
    "\n",
    "        # Apply partial mask for diagonal blocks\n",
    "        is_partial = (block_type == 1)\n",
    "        mask = partial_masks_ref[i]\n",
    "        # Use mask only when partial; for full blocks, keep all scores\n",
    "        s = jnp.where(is_partial & ~mask, -jnp.inf, s)\n",
    "\n",
    "        m_tile = jnp.max(s, axis=-1)\n",
    "        m_new = jnp.maximum(m_ref[...], m_tile)\n",
    "        corr = jnp.exp(m_ref[...] - m_new)\n",
    "\n",
    "        acc_ref[...] = acc_ref[...] * corr[:, None]\n",
    "        p = jnp.exp(s - m_new[:, None])\n",
    "        l_ref[...] = l_ref[...] * corr + p.sum(axis=-1)\n",
    "        acc_ref[...] = acc_ref[...] + p @ v\n",
    "        m_ref[...] = m_new\n",
    "\n",
    "    @pl.when(kv == tiles_kv7 - 1)\n",
    "    def _norm():\n",
    "        o_ref[...] = acc_ref[...] / l_ref[...][:, None]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad9aa7",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 8: Splash Attention\n",
    "\n",
    "**Goal**: Put it all together — build the full **splash attention** kernel\n",
    "with block-sparse masks and `PrefetchScalarGridSpec` for efficient\n",
    "block dispatch.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In Puzzle 7, we iterated over ALL KV blocks and checked the mask at\n",
    "runtime. But if 80% of blocks are SKIP, we're wasting 80% of grid\n",
    "iterations just to check a flag and do nothing. On TPU, each grid\n",
    "iteration has overhead even if the kernel body is skipped.\n",
    "\n",
    "**Splash attention** solves this with **compacted iteration**: instead of\n",
    "iterating over all `tiles_kv` blocks for each Q block, we iterate only\n",
    "over the **non-skip** blocks. A precomputed `data_next` array tells\n",
    "each grid iteration which KV block to process.\n",
    "\n",
    "```\n",
    "   Regular grid (Puzzle 7):          Compacted grid (Puzzle 8):\n",
    "\n",
    "   kv: 0  1  2  3                    step: 0    1\n",
    "   ┌──┬──┬──┬──┐                     ┌──────┬──────┐\n",
    "   │ P│ S│ S│ S│  → 4 iterations     │ kv=0 │      │  → 2 iterations\n",
    "   ├──┼──┼──┼──┤                     │(PART)│      │    (skip nothing!)\n",
    "   │ F│ P│ S│ S│  → 4 iterations     ├──────┼──────┤\n",
    "   ├──┼──┼──┼──┤                     │ kv=0 │ kv=1 │  → 2 iterations\n",
    "   │ F│ F│ P│ S│  → 4 iterations     │(FULL)│(PART)│\n",
    "   ├──┼──┼──┼──┤                     ├──────┼──────┤\n",
    "   │ F│ F│ F│ P│  → 4 iterations     │ kv=0 │ kv=1 │\n",
    "   └──┴──┴──┴──┘                     │(FULL)│(FULL)│  etc.\n",
    "   16 total iterations               └──────┴──────┘\n",
    "   (10 are SKIP!)                    10 total iterations\n",
    "                                     (0 wasted!)\n",
    "```\n",
    "\n",
    "**How it works**: We precompute:\n",
    "- `data_next[i, step]`: which KV block index to load at step `step`\n",
    "   for Q block `i`\n",
    "- `mask_next[i, step]`: block type (0=skip, 1=partial, 2=full) at this step\n",
    "- `grid_width`: max number of non-skip blocks across all Q blocks\n",
    "\n",
    "The grid becomes `(tiles_q, grid_width)` instead of `(tiles_q, tiles_kv)`.\n",
    "\n",
    "To pass `data_next` and `mask_next` to the kernel, we use\n",
    "**`PrefetchScalarGridSpec`** (from ragged_dot.py) — small metadata arrays\n",
    "are loaded into scalar memory (SMEM) and accessible to both index maps\n",
    "and the kernel body. The index maps use `data_next` to route K,V loads\n",
    "to the correct blocks.\n",
    "\n",
    "This is the production pattern from `jax.experimental.pallas.ops.tpu.splash_attention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf349ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Diagram: Compacted Iteration (Splash)\n",
    "from IPython.display import SVG, display\n",
    "display(SVG(data='''<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 780 420\" font-family=\"monospace\" font-size=\"12\">\n",
    "  <rect width=\"780\" height=\"420\" fill=\"white\"/>\n",
    "\n",
    "  <!-- Title -->\n",
    "  <text x=\"390\" y=\"25\" text-anchor=\"middle\" fill=\"#111827\" font-weight=\"bold\" font-size=\"15\">Splash Attention: Compacted Iteration</text>\n",
    "\n",
    "  <!-- LEFT: Regular grid -->\n",
    "  <text x=\"150\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-weight=\"bold\" font-size=\"12\">Regular Grid (Puzzle 7)</text>\n",
    "\n",
    "  <!-- Left grid - 4×4 causal -->\n",
    "  <!-- Row 0: P S S S -->\n",
    "  <rect x=\"50\" y=\"70\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1\"/>\n",
    "  <text x=\"72\" y=\"98\" text-anchor=\"middle\" fill=\"#854d0e\" font-size=\"10\">P</text>\n",
    "  <rect x=\"100\" y=\"70\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"122\" y=\"98\" text-anchor=\"middle\" fill=\"#b91c1c\" font-size=\"10\">S</text>\n",
    "  <rect x=\"150\" y=\"70\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"172\" y=\"98\" text-anchor=\"middle\" fill=\"#b91c1c\" font-size=\"10\">S</text>\n",
    "  <rect x=\"200\" y=\"70\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"222\" y=\"98\" text-anchor=\"middle\" fill=\"#b91c1c\" font-size=\"10\">S</text>\n",
    "\n",
    "  <!-- Row 1: F P S S -->\n",
    "  <rect x=\"50\" y=\"120\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1\"/>\n",
    "  <text x=\"72\" y=\"148\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"10\">F</text>\n",
    "  <rect x=\"100\" y=\"120\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1\"/>\n",
    "  <text x=\"122\" y=\"148\" text-anchor=\"middle\" fill=\"#854d0e\" font-size=\"10\">P</text>\n",
    "  <rect x=\"150\" y=\"120\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"172\" y=\"148\" text-anchor=\"middle\" fill=\"#b91c1c\" font-size=\"10\">S</text>\n",
    "  <rect x=\"200\" y=\"120\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"222\" y=\"148\" text-anchor=\"middle\" fill=\"#b91c1c\" font-size=\"10\">S</text>\n",
    "\n",
    "  <!-- Row 2: F F P S -->\n",
    "  <rect x=\"50\" y=\"170\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1\"/>\n",
    "  <text x=\"72\" y=\"198\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"10\">F</text>\n",
    "  <rect x=\"100\" y=\"170\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1\"/>\n",
    "  <text x=\"122\" y=\"198\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"10\">F</text>\n",
    "  <rect x=\"150\" y=\"170\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1\"/>\n",
    "  <text x=\"172\" y=\"198\" text-anchor=\"middle\" fill=\"#854d0e\" font-size=\"10\">P</text>\n",
    "  <rect x=\"200\" y=\"170\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fee2e2\" stroke=\"#fca5a5\" stroke-width=\"1\"/>\n",
    "  <text x=\"222\" y=\"198\" text-anchor=\"middle\" fill=\"#b91c1c\" font-size=\"10\">S</text>\n",
    "\n",
    "  <!-- Row 3: F F F P -->\n",
    "  <rect x=\"50\" y=\"220\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1\"/>\n",
    "  <text x=\"72\" y=\"248\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"10\">F</text>\n",
    "  <rect x=\"100\" y=\"220\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1\"/>\n",
    "  <text x=\"122\" y=\"248\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"10\">F</text>\n",
    "  <rect x=\"150\" y=\"220\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1\"/>\n",
    "  <text x=\"172\" y=\"248\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"10\">F</text>\n",
    "  <rect x=\"200\" y=\"220\" width=\"45\" height=\"45\" rx=\"2\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1\"/>\n",
    "  <text x=\"222\" y=\"248\" text-anchor=\"middle\" fill=\"#854d0e\" font-size=\"10\">P</text>\n",
    "\n",
    "  <!-- Left label -->\n",
    "  <text x=\"150\" y=\"290\" text-anchor=\"middle\" fill=\"#b91c1c\" font-size=\"11\">16 iterations</text>\n",
    "  <text x=\"150\" y=\"305\" text-anchor=\"middle\" fill=\"#b91c1c\" font-weight=\"bold\" font-size=\"11\">6 wasted on SKIP</text>\n",
    "\n",
    "  <!-- BIG ARROW -->\n",
    "  <line x1=\"270\" y1=\"165\" x2=\"360\" y2=\"165\" stroke=\"#7c3aed\" stroke-width=\"3\" marker-end=\"url(#bigarr)\"/>\n",
    "  <text x=\"315\" y=\"150\" text-anchor=\"middle\" fill=\"#7c3aed\" font-weight=\"bold\" font-size=\"11\">data_next</text>\n",
    "  <text x=\"315\" y=\"190\" text-anchor=\"middle\" fill=\"#7c3aed\" font-size=\"10\">maps steps</text>\n",
    "  <text x=\"315\" y=\"205\" text-anchor=\"middle\" fill=\"#7c3aed\" font-size=\"10\">→ KV blocks</text>\n",
    "\n",
    "  <!-- RIGHT: Compacted grid -->\n",
    "  <text x=\"560\" y=\"55\" text-anchor=\"middle\" fill=\"#6b7280\" font-weight=\"bold\" font-size=\"12\">Compacted Grid (Splash)</text>\n",
    "\n",
    "  <!-- Row labels -->\n",
    "  <text x=\"395\" y=\"98\" fill=\"#6b7280\" font-size=\"10\">i=0</text>\n",
    "  <text x=\"395\" y=\"148\" fill=\"#6b7280\" font-size=\"10\">i=1</text>\n",
    "  <text x=\"395\" y=\"198\" fill=\"#6b7280\" font-size=\"10\">i=2</text>\n",
    "  <text x=\"395\" y=\"248\" fill=\"#6b7280\" font-size=\"10\">i=3</text>\n",
    "\n",
    "  <!-- Step labels -->\n",
    "  <text x=\"445\" y=\"68\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"9\">step 0</text>\n",
    "  <text x=\"515\" y=\"68\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"9\">step 1</text>\n",
    "  <text x=\"585\" y=\"68\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"9\">step 2</text>\n",
    "  <text x=\"655\" y=\"68\" text-anchor=\"middle\" fill=\"#6b7280\" font-size=\"9\">step 3</text>\n",
    "\n",
    "  <!-- Compacted Row 0: 1 block -->\n",
    "  <rect x=\"420\" y=\"75\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"445\" y=\"98\" text-anchor=\"middle\" fill=\"#854d0e\" font-size=\"9\">kv=0</text>\n",
    "\n",
    "  <!-- Compacted Row 1: 2 blocks -->\n",
    "  <rect x=\"420\" y=\"125\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"445\" y=\"148\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"9\">kv=0</text>\n",
    "  <rect x=\"490\" y=\"125\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"515\" y=\"148\" text-anchor=\"middle\" fill=\"#854d0e\" font-size=\"9\">kv=1</text>\n",
    "\n",
    "  <!-- Compacted Row 2: 3 blocks -->\n",
    "  <rect x=\"420\" y=\"175\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"445\" y=\"198\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"9\">kv=0</text>\n",
    "  <rect x=\"490\" y=\"175\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"515\" y=\"198\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"9\">kv=1</text>\n",
    "  <rect x=\"560\" y=\"175\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"585\" y=\"198\" text-anchor=\"middle\" fill=\"#854d0e\" font-size=\"9\">kv=2</text>\n",
    "\n",
    "  <!-- Compacted Row 3: 4 blocks -->\n",
    "  <rect x=\"420\" y=\"225\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"445\" y=\"248\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"9\">kv=0</text>\n",
    "  <rect x=\"490\" y=\"225\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"515\" y=\"248\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"9\">kv=1</text>\n",
    "  <rect x=\"560\" y=\"225\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#dcfce7\" stroke=\"#22c55e\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"585\" y=\"248\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"9\">kv=2</text>\n",
    "  <rect x=\"630\" y=\"225\" width=\"50\" height=\"40\" rx=\"2\" fill=\"#fef9c3\" stroke=\"#eab308\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"655\" y=\"248\" text-anchor=\"middle\" fill=\"#854d0e\" font-size=\"9\">kv=3</text>\n",
    "\n",
    "  <!-- Right label -->\n",
    "  <text x=\"560\" y=\"290\" text-anchor=\"middle\" fill=\"#166534\" font-size=\"11\">10 iterations</text>\n",
    "  <text x=\"560\" y=\"305\" text-anchor=\"middle\" fill=\"#166534\" font-weight=\"bold\" font-size=\"11\">0 wasted!</text>\n",
    "\n",
    "  <!-- Bottom explanation -->\n",
    "  <rect x=\"140\" y=\"330\" width=\"500\" height=\"75\" rx=\"8\" fill=\"#f5f3ff\" stroke=\"#7c3aed\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"390\" y=\"355\" text-anchor=\"middle\" fill=\"#5b21b6\" font-weight=\"bold\" font-size=\"12\">PrefetchScalarGridSpec</text>\n",
    "  <text x=\"390\" y=\"375\" text-anchor=\"middle\" fill=\"#6d28d9\" font-size=\"11\">data_next[i, step] → which KV block to load at each step</text>\n",
    "  <text x=\"390\" y=\"393\" text-anchor=\"middle\" fill=\"#6d28d9\" font-size=\"11\">Index maps use data_next to route K,V to the right block</text>\n",
    "\n",
    "  <defs>\n",
    "    <marker id=\"bigarr\" viewBox=\"0 0 10 10\" refX=\"9\" refY=\"5\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\">\n",
    "      <path d=\"M 0 0 L 10 5 L 0 10 z\" fill=\"#7c3aed\"/>\n",
    "    </marker>\n",
    "  </defs>\n",
    "</svg>'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99481992",
   "metadata": {},
   "outputs": [],
   "source": [
    "T8 = 128\n",
    "d8 = 64\n",
    "bq8 = 32\n",
    "bk8 = 32\n",
    "tiles_q8 = T8 // bq8\n",
    "tiles_kv8 = T8 // bk8\n",
    "\n",
    "Q8 = jax.random.normal(jax.random.key(70), (T8, d8))\n",
    "K8 = jax.random.normal(jax.random.key(71), (T8, d8))\n",
    "V8 = jax.random.normal(jax.random.key(72), (T8, d8))\n",
    "\n",
    "# --- Build causal block_mask (same as Puzzle 7) ---\n",
    "block_mask8 = jnp.zeros((tiles_q8, tiles_kv8), dtype=jnp.int32)\n",
    "for qi in range(tiles_q8):\n",
    "    for kvi in range(tiles_kv8):\n",
    "        if qi * bq8 < kvi * bk8:\n",
    "            block_mask8 = block_mask8.at[qi, kvi].set(0)\n",
    "        elif (qi + 1) * bq8 > (kvi + 1) * bk8:\n",
    "            block_mask8 = block_mask8.at[qi, kvi].set(2)\n",
    "        else:\n",
    "            block_mask8 = block_mask8.at[qi, kvi].set(1)\n",
    "\n",
    "# --- Build compacted iteration maps ---\n",
    "# For each Q block, list the non-skip KV blocks in order\n",
    "grid_width8 = 0\n",
    "for qi in range(tiles_q8):\n",
    "    count = int(jnp.sum(block_mask8[qi] > 0))\n",
    "    grid_width8 = max(grid_width8, count)\n",
    "\n",
    "# data_next[i, step] = which KV block to process at step `step`\n",
    "# mask_next[i, step] = block type at that step\n",
    "data_next8 = jnp.zeros((tiles_q8, grid_width8), dtype=jnp.int32)\n",
    "mask_next8 = jnp.zeros((tiles_q8, grid_width8), dtype=jnp.int32)\n",
    "\n",
    "for qi in range(tiles_q8):\n",
    "    step = 0\n",
    "    for kvi in range(tiles_kv8):\n",
    "        if int(block_mask8[qi, kvi]) > 0:\n",
    "            data_next8 = data_next8.at[qi, step].set(kvi)\n",
    "            mask_next8 = mask_next8.at[qi, step].set(int(block_mask8[qi, kvi]))\n",
    "            step += 1\n",
    "\n",
    "print(f\"block_mask8:\\n{block_mask8}\")\n",
    "print(f\"\\ndata_next8 (kv indices per step):\\n{data_next8}\")\n",
    "print(f\"\\nmask_next8 (block types per step):\\n{mask_next8}\")\n",
    "print(f\"\\ngrid_width8: {grid_width8} (max non-skip blocks per Q block)\")\n",
    "\n",
    "# --- Build partial masks ---\n",
    "num_partial8 = int(min(tiles_q8, tiles_kv8))\n",
    "partial_masks8 = jnp.zeros((num_partial8, bq8, bk8), dtype=jnp.bool_)\n",
    "pidx8 = 0\n",
    "for qi in range(tiles_q8):\n",
    "    for kvi in range(tiles_kv8):\n",
    "        if int(block_mask8[qi, kvi]) == 1:\n",
    "            q_idx = qi * bq8 + jnp.arange(bq8)[:, None]\n",
    "            kv_idx = kvi * bk8 + jnp.arange(bk8)[None, :]\n",
    "            partial_masks8 = partial_masks8.at[pidx8].set(q_idx >= kv_idx)\n",
    "            pidx8 += 1\n",
    "\n",
    "\n",
    "# --- Reference ---\n",
    "def splash_attention_spec(Q, K, V, data_next, mask_next, partial_masks):\n",
    "    \"\"\"Same as causal attention — splash is an optimization, not a different computation.\"\"\"\n",
    "    d = Q.shape[-1]\n",
    "    T = Q.shape[0]\n",
    "    S = Q @ K.T / jnp.sqrt(d).astype(Q.dtype)\n",
    "    mask = jnp.tril(jnp.ones((T, T), dtype=jnp.bool_))\n",
    "    S = jnp.where(mask, S, -jnp.inf)\n",
    "    P = jax.nn.softmax(S, axis=-1)\n",
    "    return P @ V\n",
    "\n",
    "\n",
    "# --- Index maps for PrefetchScalarGridSpec ---\n",
    "# data_next_ref and mask_next_ref are scalar-prefetched.\n",
    "# Index maps receive them as extra arguments.\n",
    "def q_index_map(i, step, data_next_ref, mask_next_ref):\n",
    "    return (i, 0)\n",
    "\n",
    "def kv_index_map(i, step, data_next_ref, mask_next_ref):\n",
    "    \"\"\"Use data_next to look up which KV block to load.\"\"\"\n",
    "    kv_block = data_next_ref[i, step]\n",
    "    return (kv_block, 0)\n",
    "\n",
    "def o_index_map(i, step, data_next_ref, mask_next_ref):\n",
    "    return (i, 0)\n",
    "\n",
    "\n",
    "# --- Kernel ---\n",
    "def splash_attention_kernel(\n",
    "    data_next_ref,       # (tiles_q8, grid_width8) — scalar prefetch\n",
    "    mask_next_ref,       # (tiles_q8, grid_width8) — scalar prefetch\n",
    "    q_ref,               # (bq8, d8) — Q block\n",
    "    k_ref,               # (bk8, d8) — KV block (routed by data_next)\n",
    "    v_ref,               # (bk8, d8) — KV block (routed by data_next)\n",
    "    partial_masks_ref,   # (num_partial8, bq8, bk8) — all partial masks\n",
    "    o_ref,               # (bq8, d8) — output\n",
    "    acc_ref,             # (bq8, d8) — scratch\n",
    "    m_ref,               # (bq8,) — scratch\n",
    "    l_ref,               # (bq8,) — scratch\n",
    "):\n",
    "    \"\"\"Splash attention kernel with compacted block-sparse iteration.\n",
    "\n",
    "    Grid: (tiles_q8, grid_width8)\n",
    "      - program_id(0) = i: Q block index\n",
    "      - program_id(1) = step: compacted iteration step\n",
    "\n",
    "    Scalar-prefetched: data_next (kv block indices), mask_next (block types)\n",
    "    Index maps use data_next to route K,V loads to the correct block.\n",
    "    \"\"\"\n",
    "    i = pl.program_id(0)\n",
    "    step = pl.program_id(1)\n",
    "    pass  # YOUR CODE HERE\n",
    "    # 1. Init on step == 0\n",
    "    # 2. Read block type from mask_next_ref[i, step]\n",
    "    # 3. @pl.when(block_type > 0): flash attention with optional mask\n",
    "    #    - K,V are already routed to the right block by the index map!\n",
    "    #    - For partial blocks, look up mask from partial_masks_ref[i]\n",
    "    # 4. Normalize on step == grid_width8 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected8 = splash_attention_spec(Q8, K8, V8, data_next8, mask_next8, partial_masks8)\n",
    "\n",
    "actual8 = pl.pallas_call(\n",
    "    splash_attention_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=2,   # data_next and mask_next\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((bq8, d8), q_index_map),              # Q\n",
    "            pl.BlockSpec((bk8, d8), kv_index_map),             # K (routed!)\n",
    "            pl.BlockSpec((bk8, d8), kv_index_map),             # V (routed!)\n",
    "            pl.BlockSpec(memory_space=pl.ANY),                  # partial_masks (full)\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((bq8, d8), o_index_map),\n",
    "        scratch_shapes=[\n",
    "            pltpu.VMEM((bq8, d8), jnp.float32),   # acc\n",
    "            pltpu.VMEM((bq8,), jnp.float32),       # m\n",
    "            pltpu.VMEM((bq8,), jnp.float32),       # l\n",
    "        ],\n",
    "        grid=(tiles_q8, grid_width8),\n",
    "    ),\n",
    "    interpret=True,\n",
    ")(data_next8, mask_next8, Q8, K8, V8, partial_masks8)\n",
    "\n",
    "if jnp.allclose(actual8, expected8, atol=1e-2, rtol=1e-2):\n",
    "    print(f\"PASSED ✓  (shape={actual8.shape})\")\n",
    "    print(f\"\\n🎉 Congratulations! You've built splash attention from scratch!\")\n",
    "    print(f\"   Grid: ({tiles_q8}, {grid_width8}) instead of ({tiles_q8}, {tiles_kv8})\")\n",
    "    print(f\"   Skipped {tiles_q8 * tiles_kv8 - tiles_q8 * grid_width8} \"\n",
    "          f\"of {tiles_q8 * tiles_kv8} total block pairs\")\n",
    "else:\n",
    "    diff8 = jnp.abs(actual8 - expected8)\n",
    "    print(f\"FAILED ✗  max error: {float(jnp.max(diff8)):.6f}\")\n",
    "    print(f\"  Expected (first 2 rows):\\n{expected8[:2, :8]}\")\n",
    "    print(f\"  Got      (first 2 rows):\\n{actual8[:2, :8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a763096",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — The kernel is similar to Puzzle 7</summary>\n",
    "\n",
    "The main differences from Puzzle 7:\n",
    "- K,V are already loaded for the correct block (index maps handle routing)\n",
    "- Read block type from `mask_next_ref[i, step]` instead of `block_mask_ref[kv]`\n",
    "- Init/normalize on `step == 0` and `step == grid_width8 - 1`\n",
    "- No wasted iterations — every step does useful work (unless padded)\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — Watch out for padding</summary>\n",
    "\n",
    "If a Q block has fewer non-skip blocks than `grid_width`, the remaining\n",
    "steps have `mask_next = 0` (skip). The `@pl.when(block_type > 0)` guard\n",
    "handles this automatically.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "def splash_attention_kernel(data_next_ref, mask_next_ref,\n",
    "                             q_ref, k_ref, v_ref, partial_masks_ref,\n",
    "                             o_ref, acc_ref, m_ref, l_ref):\n",
    "    i = pl.program_id(0)\n",
    "    step = pl.program_id(1)\n",
    "\n",
    "    @pl.when(step == 0)\n",
    "    def _init():\n",
    "        acc_ref[...] = jnp.zeros((bq8, d8), dtype=jnp.float32)\n",
    "        m_ref[...] = jnp.full((bq8,), -jnp.inf, dtype=jnp.float32)\n",
    "        l_ref[...] = jnp.zeros((bq8,), dtype=jnp.float32)\n",
    "\n",
    "    block_type = mask_next_ref[i, step]\n",
    "    should_compute = block_type > 0\n",
    "\n",
    "    @pl.when(should_compute)\n",
    "    def _compute():\n",
    "        q = q_ref[...]\n",
    "        k = k_ref[...]\n",
    "        v = v_ref[...]\n",
    "        s = q @ k.T / jnp.sqrt(d8).astype(jnp.float32)\n",
    "\n",
    "        is_partial = (block_type == 1)\n",
    "        mask = partial_masks_ref[i]\n",
    "        s = jnp.where(is_partial & ~mask, -jnp.inf, s)\n",
    "\n",
    "        m_tile = jnp.max(s, axis=-1)\n",
    "        m_new = jnp.maximum(m_ref[...], m_tile)\n",
    "        corr = jnp.exp(m_ref[...] - m_new)\n",
    "\n",
    "        acc_ref[...] = acc_ref[...] * corr[:, None]\n",
    "        p = jnp.exp(s - m_new[:, None])\n",
    "        l_ref[...] = l_ref[...] * corr + p.sum(axis=-1)\n",
    "        acc_ref[...] = acc_ref[...] + p @ v\n",
    "        m_ref[...] = m_new\n",
    "\n",
    "    @pl.when(step == grid_width8 - 1)\n",
    "    def _norm():\n",
    "        o_ref[...] = acc_ref[...] / l_ref[...][:, None]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7462a0",
   "metadata": {},
   "source": [
    "---\n",
    "# TODO: Backward Pass\n",
    "\n",
    "The backward pass for splash attention is what makes it truly\n",
    "production-ready. Here's what those puzzles would cover:\n",
    "\n",
    "### dQ Kernel\n",
    "For each Q block, iterate over KV blocks (same pattern as forward) and\n",
    "compute gradients with respect to Q:\n",
    "- **Recompute** the score matrix S from Q and K — don't store it!\n",
    "  This is the key memory optimization from the Flash Attention paper.\n",
    "- Use dO (the upstream gradient) and the saved statistics (m, l) from\n",
    "  the forward pass to compute dP, then dS, then dQ.\n",
    "- Accumulate dQ contributions across all KV blocks.\n",
    "\n",
    "### dKV Kernel\n",
    "For each KV block, iterate over **Q blocks** (the reverse direction):\n",
    "- Compute S^T contributions and accumulate dK and dV.\n",
    "- This is the \"transpose\" of the forward kernel — instead of iterating\n",
    "  KV blocks for a fixed Q block, we iterate Q blocks for a fixed KV block.\n",
    "\n",
    "### Block-Sparse Backward\n",
    "Skip the same blocks as the forward pass! The block_mask and data_next\n",
    "maps work in both directions:\n",
    "- dQ kernel: same mask as forward (skip KV blocks)\n",
    "- dKV kernel: needs a **transposed** version of data_next (skip Q blocks)\n",
    "\n",
    "The backward pass roughly doubles the kernel code, but the patterns are\n",
    "the same: online accumulation with `@pl.when` guards, scratch memory\n",
    "for accumulators, and block-sparse dispatch via precomputed maps.\n",
    "\n",
    "See `jax.experimental.pallas.ops.tpu.splash_attention.splash_attention_kernel`\n",
    "for the production implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947181b",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎉 Summary\n",
    "\n",
    "You've built splash attention from the ground up:\n",
    "\n",
    "| Puzzle | Concept | Key Insight |\n",
    "|--------|---------|-------------|\n",
    "| 1 | Dot-product attention | The O(T²) score matrix problem |\n",
    "| 2 | Tiled softmax | Computing max and sum_exp across tiles |\n",
    "| 3 | Online softmax | Single-pass with correction factor `exp(m_old - m_new)` |\n",
    "| 4 | Tiled attention (1 Q block) | Combining online softmax with tiled matmul |\n",
    "| 5 | Flash attention | Grid over all Q blocks — O(T) memory |\n",
    "| 6 | Causal masking | Block classification: FULL / PARTIAL / SKIP |\n",
    "| 7 | Block-sparse masks | Precomputed block_mask + partial_mask arrays |\n",
    "| 8 | Splash attention | Compacted iteration via data_next + PrefetchScalarGridSpec |\n",
    "\n",
    "**What makes splash attention \"splash\"?** The compacted iteration from\n",
    "Puzzle 8. Instead of checking a mask at every grid point, we precompute\n",
    "which blocks to visit and only iterate over those. For attention patterns\n",
    "with significant sparsity (causal, sliding window, block-sparse), this\n",
    "eliminates wasted computation entirely.\n",
    "\n",
    "**Next steps**:\n",
    "- Try changing the block_mask in Puzzle 8 to a **sliding window** pattern\n",
    "- Read the [production source](https://github.com/jax-ml/jax/tree/main/jax/experimental/pallas/ops/tpu/splash_attention) — you now understand every concept it uses\n",
    "- Implement the backward pass (see TODO above) for full training support\n",
    "- Add multi-head support: it's just `jax.vmap` over the head dimension!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
