{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2366f500",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vorushin/pallas_puzzles/blob/master/grouped_matmul.ipynb?flush_caches=true\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Pallas Puzzles: Grouped Matmul\n",
    "\n",
    "**Progressive puzzles** building toward a production **grouped matmul**\n",
    "kernel — the core operation behind **Mixture-of-Experts** (MoE) dispatch\n",
    "on TPU. You'll implement scalar prefetch, group metadata, masked stores,\n",
    "and full grouped matmul with variable-size groups.\n",
    "\n",
    "Every puzzle runs on **CPU** via `interpret=True` — no TPU needed.\n",
    "\n",
    "**Prerequisites**: Complete **basics.py** first (Pallas foundations and\n",
    "tiled matmul patterns).\n",
    "\n",
    "**Key Pallas docs**: https://docs.jax.dev/en/latest/pallas/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277addf",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Click > to collapse this section, then click ▶ to get everything ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d290b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jax jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bab7b3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import tpu as pltpu\n",
    "print(f\"JAX {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec9aa4",
   "metadata": {},
   "source": [
    "---\n",
    "# MoE & Grouped Matmul\n",
    "\n",
    "Before writing any Pallas code, let's understand **why** grouped matmul\n",
    "exists.\n",
    "\n",
    "### What is Mixture-of-Experts?\n",
    "\n",
    "A standard transformer FFN applies the same weight matrix to every token.\n",
    "**Mixture-of-Experts** (MoE) replaces this single FFN with `E` parallel\n",
    "**expert** FFNs. A small **router** network selects the top-k experts\n",
    "for each token (typically k=1 or k=2). The result: model capacity scales\n",
    "with the number of experts, but compute stays constant per token — each\n",
    "token only activates k out of E experts.\n",
    "\n",
    "### Router and top-k gating\n",
    "\n",
    "The router is a linear layer producing logits over experts. Top-k\n",
    "selection + softmax gives gating weights. After routing, tokens are\n",
    "grouped by their assigned expert.\n",
    "\n",
    "### The grouped matmul problem\n",
    "\n",
    "After routing, we have G groups of tokens (one per expert) with variable\n",
    "sizes. The naive approach — a Python loop of G separate matmuls — has\n",
    "terrible hardware utilization because each matmul is small and\n",
    "underuses the MXU (TPU's matrix unit).\n",
    "\n",
    "**Grouped matmul** solves this: concatenate all tokens into a single\n",
    "`lhs (M, K)`, stack expert weights into `rhs (G, K, N)`, and run one\n",
    "kernel that handles all G matmuls. The kernel uses metadata to route\n",
    "each tile to the correct expert.\n",
    "\n",
    "### Data shapes for this notebook\n",
    "\n",
    "- `lhs (M, K)` — concatenated token representations\n",
    "- `rhs (G, K, N)` — stacked expert weights\n",
    "- `group_sizes (G,)` — number of tokens per expert\n",
    "- `out (M, N)` — concatenated outputs\n",
    "\n",
    "### The naive loop (our reference spec)\n",
    "\n",
    "This is what the kernel must match — but in a single fused operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682cd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_matmul_spec(lhs, rhs, group_sizes):\n",
    "    \"\"\"Reference: G separate matmuls in a loop.\"\"\"\n",
    "    offsets = jnp.concatenate([jnp.array([0]), jnp.cumsum(group_sizes)])\n",
    "    out = jnp.zeros((lhs.shape[0], rhs.shape[2]), dtype=jnp.float32)\n",
    "    for g in range(len(group_sizes)):\n",
    "        s, e = int(offsets[g]), int(offsets[g + 1])\n",
    "        out = out.at[s:e].set(lhs[s:e] @ rhs[g])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b4447",
   "metadata": {},
   "source": [
    "```\n",
    "TODO diagram: MoE data layout. Left side: tokens colored by expert\n",
    "assignment. Right side: lhs matrix with colored row bands per group,\n",
    "rhs as a stack of G weight matrices. Arrow showing the grouped matmul\n",
    "producing output with same row coloring. Keep compact.\n",
    "```\n",
    "\n",
    "> **Further reading** (not needed for these puzzles): auxiliary loss,\n",
    "> load balancing, capacity factor, token dropping, expert parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2313bf0",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: Scalar Prefetch & Group Metadata (Puzzles 1–2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3eeec8",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 1: Scalar Prefetch — Permuted Batched Matmul\n",
    "\n",
    "**Goal**: Implement a **permuted batched matmul** where the mapping from\n",
    "output group to rhs group is determined at runtime by a permutation array.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In grouped matmul, the tile-to-group mapping is computed at runtime (from\n",
    "`group_sizes`). Standard `BlockSpec` index maps only see grid indices —\n",
    "they can't access runtime arrays.\n",
    "\n",
    "**Scalar prefetch** solves this. With `PrefetchScalarGridSpec`:\n",
    "- Small arrays are loaded into **SMEM** (scalar memory — separate from\n",
    "  **VMEM**, the vector memory where tile data lives) before the kernel\n",
    "- Index maps receive these SMEM refs as extra arguments\n",
    "- The kernel also receives them as leading arguments\n",
    "\n",
    "```python\n",
    "PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=1,  # first 1 arg is scalar-prefetched\n",
    "    in_specs=[...],\n",
    "    out_specs=...,\n",
    "    grid=(...),\n",
    ")\n",
    "```\n",
    "\n",
    "Index map signature becomes: `lambda grid_idx0, ..., *prefetch_refs: (...)`\n",
    "\n",
    "The kernel signature becomes: `kernel(prefetch_ref0, ..., in_ref0, ..., out_ref, *scratch)`\n",
    "\n",
    "**Reminder from basics.py Puzzle 10**: Using `None` in `BlockSpec`\n",
    "block_shape squeezes that dimension — the ref won't have that dim.\n",
    "`BlockSpec((None, M, K), lambda b: (b, 0, 0))` gives the kernel a\n",
    "ref of shape `(M, K)`, not `(1, M, K)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31b528",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "G = 4\n",
    "M, K, N = 64, 64, 64\n",
    "\n",
    "# --- Reference ---\n",
    "def permuted_matmul_spec(lhs, rhs, perm):\n",
    "    \"\"\"lhs: (G, M, K), rhs: (G, K, N), perm: (G,) -> (G, M, N)\n",
    "    out[i] = lhs[i] @ rhs[perm[i]]\n",
    "    \"\"\"\n",
    "    return jnp.stack([lhs[i] @ rhs[perm[i]] for i in range(G)])\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def permuted_matmul_kernel(perm_ref, lhs_ref, rhs_ref, o_ref):\n",
    "    # perm_ref: scalar-prefetched permutation array (in SMEM)\n",
    "    # lhs_ref: (M, K) — current group's lhs (batch dim squeezed by None)\n",
    "    # rhs_ref: (K, N) — permuted group's rhs (loaded via index map)\n",
    "    # o_ref: (M, N) — output tile\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "# --- Index maps ---\n",
    "def lhs_index_map(g, perm_ref):\n",
    "    return (g, 0, 0)\n",
    "\n",
    "def rhs_index_map(g, perm_ref):\n",
    "    # Use the scalar-prefetched perm to look up which rhs group to load\n",
    "    return (perm_ref[g], 0, 0)\n",
    "\n",
    "def out_index_map(g, perm_ref):\n",
    "    return (g, 0, 0)\n",
    "\n",
    "# --- Tests ---\n",
    "lhs = jax.random.normal(jax.random.key(14), (G, M, K))\n",
    "rhs = jax.random.normal(jax.random.key(15), (G, K, N))\n",
    "perm = jnp.array([2, 0, 3, 1], dtype=jnp.int32)\n",
    "\n",
    "expected = permuted_matmul_spec(lhs, rhs, perm)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    permuted_matmul_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=1,  # first arg (perm) is prefetched to SMEM\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((None, M, K), lhs_index_map),\n",
    "            pl.BlockSpec((None, K, N), rhs_index_map),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((None, M, N), out_index_map),\n",
    "        grid=(G,),\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((G, M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(perm, lhs, rhs)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-3):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    max_err = float(jnp.max(jnp.abs(actual - expected)))\n",
    "    print(f\"FAILED ✗  max error: {max_err:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc9c83",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Approach</summary>\n",
    "\n",
    "The index maps handle the permutation using `perm_ref[g]`. By the time the kernel runs, `rhs_ref` already points to the correct permuted group. So the kernel body is identical to basics.py Puzzle 10 — just a matmul.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "o_ref[...] = lhs_ref[...] @ rhs_ref[...]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ea4a2",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 2: Group Metadata — Tile-to-Group Mapping\n",
    "\n",
    "**Goal**: Implement the metadata functions that map fixed-size tiles to\n",
    "variable-size groups. This is **pure JAX** — not a kernel puzzle.\n",
    "\n",
    "### Theory\n",
    "\n",
    "In grouped matmul, `lhs` has shape `(M, K)` where rows are divided into\n",
    "`G` groups of variable sizes. We need to figure out which **tiles**\n",
    "belong to which **groups**.\n",
    "\n",
    "Given `group_sizes = [300, 212, 512]` with `bm = 128`:\n",
    "\n",
    "![Groups and tiles](https://raw.githubusercontent.com/vorushin/pallas_puzzles/master/images/ragged-dot-puzzle2.drawio.svg)\n",
    "\n",
    "Tile at row 256 straddles the group boundary at row 300. It gets visited\n",
    "**twice**: once for group 0 (rows 256-299 are valid) and once for group 1\n",
    "(rows 300-383 are valid). The kernel uses a **mask** to only store the\n",
    "valid rows for each visit.\n",
    "\n",
    "**Rule of thumb**: `num_tiles = tiles_m + (number of non-aligned group\n",
    "boundaries)`. Aligned boundaries don't cause extra visits.\n",
    "\n",
    "**Output arrays**:\n",
    "- `group_offsets`: `[0, 300, 512, 1024]` — cumsum with leading 0\n",
    "- `group_ids`: maps each grid index to a group id\n",
    "- `m_tile_ids`: maps each grid index to which m-tile to process\n",
    "- `num_tiles`: total number of grid iterations needed\n",
    "\n",
    "The arrays can be longer than `num_tiles` (padded with the last group)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca1a9b",
   "metadata": {},
   "source": [
    "### Step 2a: Group Offsets (provided)\n",
    "\n",
    "CSR-style prefix sum: `[0, cumsum(group_sizes)]`.\n",
    "\n",
    "```\n",
    "group_sizes = [300, 212, 512]\n",
    "group_offsets = [0, 300, 512, 1024]\n",
    "                 ^    ^    ^     ^\n",
    "                 g0   g1   g2   end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ab6fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_group_offsets(group_sizes):\n",
    "    \"\"\"[0, cumsum(group_sizes)] — maps group id to start row.\"\"\"\n",
    "    return jnp.concatenate([jnp.zeros(1, dtype=jnp.int32), jnp.cumsum(group_sizes)])\n",
    "\n",
    "# Quick check\n",
    "assert jnp.array_equal(\n",
    "    compute_group_offsets(jnp.array([300, 212, 512], dtype=jnp.int32)),\n",
    "    jnp.array([0, 300, 512, 1024], dtype=jnp.int32))\n",
    "print(\"Step 2a — compute_group_offsets: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef8288",
   "metadata": {},
   "source": [
    "### Step 2b: Tiles per Group — STUDENT IMPLEMENTS\n",
    "\n",
    "**Goal**: Compute how many tile visits each group needs.\n",
    "\n",
    "The key insight: group boundaries don't align with tile boundaries.\n",
    "Round group starts **down** to tile boundaries, round group ends **up**,\n",
    "then compute how many tiles that covers. Handle zero-size groups\n",
    "(they need 0 tiles).\n",
    "\n",
    "```\n",
    "group_sizes = [300, 212, 512], bm = 128\n",
    "\n",
    "Group 0: rows 0–299   → tiles 0,1,2   (3 tiles: 0→256 covers 0-255, plus tile 2 covers 256-383)\n",
    "Group 1: rows 300–511 → tiles 2,3     (2 tiles: tile 2 for 300-383, tile 3 for 384-511)\n",
    "Group 2: rows 512–1023 → tiles 4,5,6,7 (4 tiles)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e696c00",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_group_tiles(group_sizes, group_offsets, bm):\n",
    "    \"\"\"Number of tile visits per group (boundary tiles counted by both neighbors).\n",
    "\n",
    "    Args:\n",
    "        group_sizes: (G,) int32\n",
    "        group_offsets: (G+1,) int32 from compute_group_offsets\n",
    "        bm: tile size\n",
    "    Returns:\n",
    "        (G,) int32\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Extract group starts and ends from offsets\n",
    "    # 2. Round starts DOWN and ends UP to tile boundaries\n",
    "    # 3. Handle zero-size groups\n",
    "    # 4. Convert rounded range sizes to tile counts\n",
    "\n",
    "# --- Tests ---\n",
    "assert jnp.array_equal(\n",
    "    compute_group_tiles(jnp.array([256, 256, 256, 256], dtype=jnp.int32),\n",
    "                        jnp.array([0, 256, 512, 768, 1024], dtype=jnp.int32), 128),\n",
    "    jnp.array([2, 2, 2, 2]))\n",
    "assert jnp.array_equal(\n",
    "    compute_group_tiles(jnp.array([300, 212, 512], dtype=jnp.int32),\n",
    "                        jnp.array([0, 300, 512, 1024], dtype=jnp.int32), 128),\n",
    "    jnp.array([3, 2, 4]))\n",
    "assert jnp.array_equal(\n",
    "    compute_group_tiles(jnp.array([512, 0, 512], dtype=jnp.int32),\n",
    "                        jnp.array([0, 512, 512, 1024], dtype=jnp.int32), 128),\n",
    "    jnp.array([4, 0, 4]))\n",
    "assert jnp.array_equal(\n",
    "    compute_group_tiles(jnp.array([300, 0, 724], dtype=jnp.int32),\n",
    "                        jnp.array([0, 300, 300, 1024], dtype=jnp.int32), 128),\n",
    "    jnp.array([3, 0, 6]))\n",
    "print(\"Step 2b — compute_group_tiles: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2af081",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group_starts = group_offsets[:-1]\n",
    "group_ends = group_offsets[1:]\n",
    "rounded_starts = (group_starts // bm * bm).astype(jnp.int32)\n",
    "rounded_ends = ((group_ends + bm - 1) // bm * bm).astype(jnp.int32)\n",
    "rounded_sizes = jnp.where(group_sizes == 0, 0, rounded_ends - rounded_starts)\n",
    "return rounded_sizes // bm\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c713c",
   "metadata": {},
   "source": [
    "### Step 2c: Group IDs (provided)\n",
    "\n",
    "Flat array mapping grid index to group id.\n",
    "```\n",
    "group_tiles = [3, 2, 4]  →  group_ids = [0,0,0, 1,1, 2,2,2,2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15b2e4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_group_ids(group_tiles, num_groups, max_len):\n",
    "    \"\"\"Flat array mapping grid index to group id.\"\"\"\n",
    "    return jnp.repeat(\n",
    "        jnp.arange(num_groups, dtype=jnp.int32),\n",
    "        group_tiles,\n",
    "        total_repeat_length=max_len,\n",
    "    )\n",
    "\n",
    "# Quick check\n",
    "assert compute_group_ids(jnp.array([3, 2, 4]), 3, 10)[:9].tolist() == [0, 0, 0, 1, 1, 2, 2, 2, 2]\n",
    "print(\"Step 2c — compute_group_ids: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524277f",
   "metadata": {},
   "source": [
    "### Step 2d: Tile Visits — STUDENT IMPLEMENTS\n",
    "\n",
    "**Goal**: Count how many times each tile is visited.\n",
    "\n",
    "Every tile is visited at least once. When a group boundary falls in the\n",
    "**middle** of a tile (not aligned to `bm`), that tile gets an extra visit.\n",
    "\n",
    "```\n",
    "group_offsets = [0, 300, 512, 1024],  bm = 128\n",
    "Group 1 starts at row 300 → inside tile 2 → extra visit\n",
    "tile_visits = [1, 1, 2, 1, 1, 1, 1, 1]\n",
    "```\n",
    "\n",
    "Use `jnp.histogram` to count how many non-aligned boundaries land in each tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a58e1c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_tile_visits(group_sizes, group_offsets, tiles_m, bm):\n",
    "    \"\"\"Visit count per tile (1 + extra for each mid-tile group boundary).\n",
    "\n",
    "    Args:\n",
    "        group_sizes: (G,) int32\n",
    "        group_offsets: (G+1,) int32\n",
    "        tiles_m: M // bm\n",
    "        bm: tile size\n",
    "    Returns:\n",
    "        (tiles_m,) int32\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Find group start positions (from offsets, skip the leading 0)\n",
    "    # 2. Identify which starts are non-aligned (start % bm != 0)\n",
    "    #    AND belong to non-empty groups\n",
    "    # 3. For non-aligned starts, compute which tile they land in (start // bm)\n",
    "    # 4. Count how many non-aligned boundaries per tile (jnp.histogram)\n",
    "    # 5. Result = 1 + extra_visits_per_tile\n",
    "\n",
    "# --- Tests ---\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([256, 256, 256, 256], dtype=jnp.int32),\n",
    "    jnp.array([0, 256, 512, 768, 1024], dtype=jnp.int32), 8, 128\n",
    ").tolist() == [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([300, 212, 512], dtype=jnp.int32),\n",
    "    jnp.array([0, 300, 512, 1024], dtype=jnp.int32), 8, 128\n",
    ").tolist() == [1, 1, 2, 1, 1, 1, 1, 1]\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([512, 0, 512], dtype=jnp.int32),\n",
    "    jnp.array([0, 512, 512, 1024], dtype=jnp.int32), 8, 128\n",
    ").tolist() == [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([300, 0, 724], dtype=jnp.int32),\n",
    "    jnp.array([0, 300, 300, 1024], dtype=jnp.int32), 8, 128\n",
    ").tolist() == [1, 1, 2, 1, 1, 1, 1, 1]\n",
    "assert compute_tile_visits(\n",
    "    jnp.array([300, 212, 512], dtype=jnp.int32),\n",
    "    jnp.array([0, 300, 512, 1024], dtype=jnp.int32), 8, 128\n",
    ").dtype == jnp.int32, \"compute_tile_visits must return int32, not float32\"\n",
    "print(\"Step 2d — compute_tile_visits: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b03230",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group_starts = group_offsets[:-1]\n",
    "aligned_or_empty = ((group_starts % bm) == 0) | (group_sizes == 0)\n",
    "partial_tile_ids = jnp.where(aligned_or_empty, tiles_m + 1, group_starts // bm)\n",
    "extra_visits = jnp.histogram(\n",
    "    partial_tile_ids, bins=tiles_m, range=(0, tiles_m)\n",
    ")[0]\n",
    "return (extra_visits + 1).astype(jnp.int32)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd58fc",
   "metadata": {},
   "source": [
    "### Step 2e: M-tile IDs (provided)\n",
    "\n",
    "Flat array mapping grid index to m-tile id.\n",
    "```\n",
    "tile_visits = [1,1,2,1,1,1,1,1]  →  m_tile_ids = [0,1,2,2,3,4,5,6,7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8330b00",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_m_tile_ids(tile_visits, tiles_m, max_len):\n",
    "    \"\"\"Flat array mapping grid index to m-tile id.\"\"\"\n",
    "    return jnp.repeat(\n",
    "        jnp.arange(tiles_m, dtype=jnp.int32),\n",
    "        tile_visits,\n",
    "        total_repeat_length=max_len,\n",
    "    )\n",
    "\n",
    "# Quick check\n",
    "assert compute_m_tile_ids(jnp.array([1,1,2,1,1,1,1,1]), 8, 10)[:9].tolist() == [0,1,2,2,3,4,5,6,7]\n",
    "print(\"Step 2e — compute_m_tile_ids: PASSED ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de5d4d",
   "metadata": {},
   "source": [
    "### Step 2f: Combined `make_group_metadata` — STUDENT CHAINS steps 2a–2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f8bf1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_group_metadata_yours(group_sizes, m, bm):\n",
    "    \"\"\"Vectorized group metadata — chains steps 2a-2e.\n",
    "\n",
    "    Args:\n",
    "        group_sizes: jnp.array of shape (num_groups,), dtype int32\n",
    "        m: total number of rows\n",
    "        bm: tile size for m dimension\n",
    "\n",
    "    Returns:\n",
    "        (group_offsets, group_ids, m_tile_ids), num_tiles\n",
    "    \"\"\"\n",
    "    num_groups = group_sizes.shape[0]\n",
    "    tiles_m = m // bm\n",
    "    max_len = tiles_m + num_groups - 1\n",
    "\n",
    "    # YOUR CODE HERE — chain steps 2a-2e, then compute num_tiles\n",
    "    # Replace this raise with your implementation:\n",
    "    raise NotImplementedError(\"Chain compute_group_offsets -> ... -> compute_m_tile_ids\")\n",
    "\n",
    "    return (group_offsets, group_ids, m_tile_ids), num_tiles\n",
    "\n",
    "# --- Reference implementation for testing ---\n",
    "def make_group_metadata_reference(group_sizes, m, bm):\n",
    "    \"\"\"Simple reference implementation — O(m) but correct.\"\"\"\n",
    "    num_groups = len(group_sizes)\n",
    "    group_offsets = jnp.concatenate([jnp.array([0]), jnp.cumsum(group_sizes)])\n",
    "\n",
    "    row_to_group = jnp.zeros(m, dtype=jnp.int32)\n",
    "    for g in range(num_groups):\n",
    "        start = int(group_offsets[g])\n",
    "        end = int(group_offsets[g + 1])\n",
    "        row_to_group = row_to_group.at[start:end].set(g)\n",
    "\n",
    "    tiles_m = m // bm\n",
    "    group_ids_list = []\n",
    "    m_tile_ids_list = []\n",
    "\n",
    "    for t in range(tiles_m):\n",
    "        tile_start = t * bm\n",
    "        tile_end = (t + 1) * bm\n",
    "        groups_in_tile = jnp.unique(row_to_group[tile_start:tile_end])\n",
    "        for g in groups_in_tile:\n",
    "            group_ids_list.append(int(g))\n",
    "            m_tile_ids_list.append(t)\n",
    "\n",
    "    num_tiles = len(group_ids_list)\n",
    "\n",
    "    max_len = tiles_m + num_groups - 1\n",
    "    group_ids = jnp.zeros(max_len, dtype=jnp.int32)\n",
    "    m_tile_ids = jnp.zeros(max_len, dtype=jnp.int32)\n",
    "    group_ids = group_ids.at[:num_tiles].set(jnp.array(group_ids_list, dtype=jnp.int32))\n",
    "    m_tile_ids = m_tile_ids.at[:num_tiles].set(jnp.array(m_tile_ids_list, dtype=jnp.int32))\n",
    "    if num_tiles < max_len:\n",
    "        group_ids = group_ids.at[num_tiles:].set(group_ids_list[-1])\n",
    "        m_tile_ids = m_tile_ids.at[num_tiles:].set(m_tile_ids_list[-1])\n",
    "\n",
    "    return (group_offsets.astype(jnp.int32), group_ids, m_tile_ids), num_tiles\n",
    "\n",
    "\n",
    "# --- Integration tests ---\n",
    "def check_metadata(name, group_sizes, m, bm):\n",
    "    ref, ref_nt = make_group_metadata_reference(group_sizes, m, bm)\n",
    "    yours, your_nt = make_group_metadata_yours(group_sizes, m, bm)\n",
    "    ok = (ref_nt == your_nt\n",
    "          and bool(jnp.array_equal(ref[0], yours[0]))\n",
    "          and bool(jnp.array_equal(ref[1][:ref_nt], yours[1][:your_nt]))\n",
    "          and bool(jnp.array_equal(ref[2][:ref_nt], yours[2][:your_nt])))\n",
    "    status = \"PASSED ✓\" if ok else \"FAILED ✗\"\n",
    "    print(f\"  {name}: {status}  (num_tiles: ref={ref_nt}, yours={your_nt})\")\n",
    "    if not ok:\n",
    "        print(f\"    group_ids ref:   {ref[1][:ref_nt].tolist()}\")\n",
    "        print(f\"    group_ids yours: {yours[1][:your_nt].tolist()}\")\n",
    "        print(f\"    m_tile_ids ref:   {ref[2][:ref_nt].tolist()}\")\n",
    "        print(f\"    m_tile_ids yours: {yours[2][:your_nt].tolist()}\")\n",
    "\n",
    "print(\"=== Integration tests ===\")\n",
    "check_metadata(\"Aligned groups\",\n",
    "               jnp.array([256, 256, 256, 256], dtype=jnp.int32), 1024, 128)\n",
    "check_metadata(\"Unaligned groups\",\n",
    "               jnp.array([300, 212, 512], dtype=jnp.int32), 1024, 128)\n",
    "check_metadata(\"Zero-size group (aligned)\",\n",
    "               jnp.array([512, 0, 512], dtype=jnp.int32), 1024, 128)\n",
    "check_metadata(\"Zero-size group (non-aligned)\",\n",
    "               jnp.array([300, 0, 724], dtype=jnp.int32), 1024, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1298bef",
   "metadata": {},
   "source": [
    "<details><summary>Hint — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group_offsets = compute_group_offsets(group_sizes)\n",
    "group_tiles = compute_group_tiles(group_sizes, group_offsets, bm)\n",
    "group_ids = compute_group_ids(group_tiles, num_groups, max_len)\n",
    "tile_visits = compute_tile_visits(group_sizes, group_offsets, tiles_m, bm)\n",
    "m_tile_ids = compute_m_tile_ids(tile_visits, tiles_m, max_len)\n",
    "num_tiles = int(group_tiles.sum())\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263023d",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II: Grouped Matmul Kernels (Puzzles 3–6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d0a1c",
   "metadata": {},
   "source": [
    "### Provided utilities\n",
    "\n",
    "These are the building blocks from Part I. The production\n",
    "`make_group_metadata` is provided so you can focus on kernel logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd561335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_group_metadata(group_sizes, m, bm):\n",
    "    \"\"\"Compute tile-to-group mapping for grouped matmul.\n",
    "\n",
    "    Returns:\n",
    "        (group_offsets, group_ids, m_tile_ids), num_tiles\n",
    "    \"\"\"\n",
    "    num_groups = group_sizes.shape[0]\n",
    "    tiles_m = m // bm\n",
    "\n",
    "    group_ends = jnp.cumsum(group_sizes)\n",
    "    group_offsets = jnp.concatenate([jnp.zeros(1, dtype=jnp.int32), group_ends])\n",
    "\n",
    "    group_starts = jnp.concatenate([jnp.zeros(1, dtype=jnp.int32), group_ends[:-1]])\n",
    "    rounded_ends = ((group_ends + bm - 1) // bm * bm).astype(jnp.int32)\n",
    "    rounded_starts = (group_starts // bm * bm).astype(jnp.int32)\n",
    "    rounded_sizes = rounded_ends - rounded_starts\n",
    "    rounded_sizes = jnp.where(group_sizes == 0, 0, rounded_sizes)\n",
    "    group_tiles = rounded_sizes // bm\n",
    "\n",
    "    group_ids = jnp.repeat(\n",
    "        jnp.arange(num_groups, dtype=jnp.int32),\n",
    "        group_tiles,\n",
    "        total_repeat_length=tiles_m + num_groups - 1,\n",
    "    )\n",
    "\n",
    "    partial_mask = ((group_offsets[:-1] % bm) == 0) | (group_sizes == 0)\n",
    "    partial_tile_ids = jnp.where(partial_mask, tiles_m + 1, group_offsets[:-1] // bm)\n",
    "    tile_visits = (\n",
    "        jnp.histogram(partial_tile_ids, bins=tiles_m, range=(0, tiles_m))[0] + 1\n",
    "    )\n",
    "    m_tile_ids = jnp.repeat(\n",
    "        jnp.arange(tiles_m, dtype=jnp.int32),\n",
    "        tile_visits.astype(jnp.int32),\n",
    "        total_repeat_length=tiles_m + num_groups - 1,\n",
    "    )\n",
    "\n",
    "    num_tiles = int(group_tiles.sum())\n",
    "    return (group_offsets, group_ids, m_tile_ids), num_tiles\n",
    "\n",
    "\n",
    "def get_store_mask(grid_id, group_offsets, group_ids, m_tile_ids, bm, bn):\n",
    "    \"\"\"Build a (bm, bn) boolean mask for rows belonging to the current group.\"\"\"\n",
    "    group_id = group_ids[grid_id]\n",
    "    group_start = group_offsets[group_id]\n",
    "    group_end = group_offsets[group_id + 1]\n",
    "    m_id = m_tile_ids[grid_id] * bm\n",
    "    iota = jax.lax.broadcasted_iota(jnp.int32, (bm, bn), 0) + m_id\n",
    "    return (iota >= group_start) & (iota < group_end)\n",
    "\n",
    "\n",
    "# --- Shared index maps for grouped matmul ---\n",
    "def lhs_imap(n_i, grid_id, k_i, group_meta_ref, group_offset_ref):\n",
    "    _, _, m_tile_ids = group_meta_ref\n",
    "    return (m_tile_ids[grid_id], k_i)\n",
    "\n",
    "def rhs_imap(n_i, grid_id, k_i, group_meta_ref, group_offset_ref):\n",
    "    _, group_ids, _ = group_meta_ref\n",
    "    return (group_ids[grid_id], k_i, n_i)\n",
    "\n",
    "def out_imap(n_i, grid_id, k_i, group_meta_ref, group_offset_ref):\n",
    "    _, _, m_tile_ids = group_meta_ref\n",
    "    return (m_tile_ids[grid_id], n_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12125b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 3: Masked Store with Group Boundaries\n",
    "\n",
    "**Goal**: Write a kernel that copies input rows to output, but **masks**\n",
    "writes based on group boundaries. Only rows belonging to the current\n",
    "group are written; other rows retain their previous value (zero).\n",
    "\n",
    "### Theory\n",
    "\n",
    "When a tile straddles a group boundary, some rows belong to group `g`\n",
    "and others to group `g+1`. The kernel must only store the rows that\n",
    "belong to the **current group** being processed.\n",
    "\n",
    "The mask is built from:\n",
    "- `group_offsets[group_id]` — start row of current group\n",
    "- `group_offsets[group_id + 1]` — end row of current group\n",
    "- `m_tile_ids[grid_id] * bm` — first row of current tile\n",
    "\n",
    "For a 2D mask `(bm, N)`, use `jax.lax.broadcasted_iota(dtype, shape, dim)`\n",
    "— it creates an array where values along `dim` are `0, 1, 2, ...` and\n",
    "all other dimensions are broadcast. Think of it as a multi-dimensional\n",
    "`jnp.arange`:\n",
    "```python\n",
    "broadcasted_iota(int32, (4, 3), 0) -> [[0,0,0], [1,1,1], [2,2,2], [3,3,3]]\n",
    "broadcasted_iota(int32, (4, 3), 1) -> [[0,1,2], [0,1,2], [0,1,2], [0,1,2]]\n",
    "```\n",
    "\n",
    "```\n",
    "TODO diagram: A tile straddling a group boundary. Show rows 256-383\n",
    "with rows 256-299 belonging to group 0 (highlighted) and rows 300-383\n",
    "belonging to group 1. Two visits: visit 1 masks top rows, visit 2\n",
    "masks bottom rows.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e875a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1024\n",
    "N = 64\n",
    "bm = 128\n",
    "G = 3\n",
    "\n",
    "group_sizes = jnp.array([300, 212, 512], dtype=jnp.int32)\n",
    "\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata(group_sizes, M, bm)\n",
    "\n",
    "# --- Reference ---\n",
    "def masked_copy_spec(x, group_offsets, group_ids, m_tile_ids):\n",
    "    \"\"\"Copy x to output, but only rows within their assigned group.\"\"\"\n",
    "    out = jnp.zeros_like(x)\n",
    "    for grid_id in range(num_tiles):\n",
    "        g = int(group_ids[grid_id])\n",
    "        tile_id = int(m_tile_ids[grid_id])\n",
    "        g_start = int(group_offsets[g])\n",
    "        g_end = int(group_offsets[g + 1])\n",
    "        t_start = tile_id * bm\n",
    "        t_end = t_start + bm\n",
    "        for row in range(t_start, t_end):\n",
    "            if g_start <= row < g_end:\n",
    "                out = out.at[row].set(x[row])\n",
    "    return out\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def masked_copy_kernel(group_offsets_ref, group_ids_ref, m_tile_ids_ref,\n",
    "                       x_ref, o_ref):\n",
    "    # group_offsets_ref, group_ids_ref, m_tile_ids_ref: metadata in SMEM\n",
    "    # x_ref: (bm, N) — tile of input\n",
    "    # o_ref: (bm, N) — tile of output\n",
    "    grid_id = pl.program_id(0)\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Look up which group and tile this grid iteration processes\n",
    "    # 2. Get the group's row boundaries\n",
    "    # 3. Build a 2D boolean mask for rows inside this group\n",
    "    # 4. Masked store: only write rows belonging to this group\n",
    "\n",
    "# --- Tests ---\n",
    "x = jax.random.normal(jax.random.key(27), (M, N))\n",
    "expected = masked_copy_spec(x, group_offsets, group_ids, m_tile_ids)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    masked_copy_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=3,\n",
    "        in_specs=[pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0))],\n",
    "        out_specs=pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0)),\n",
    "        grid=(num_tiles,),\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_offsets, group_ids, m_tile_ids, x)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-5):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    nan_count = int(jnp.isnan(actual).sum())\n",
    "    if nan_count > 0:\n",
    "        nan_rows = jnp.where(jnp.isnan(actual).any(axis=1))[0]\n",
    "        print(f\"FAILED ✗  {nan_count} NaN values in output (rows: {nan_rows.tolist()[:8]}...)\")\n",
    "        print(f\"  Common cause: indexing group_offsets_ref with grid_id instead of group_id\")\n",
    "    else:\n",
    "        diff = jnp.abs(actual - expected)\n",
    "        worst_row = int(jnp.argmax(diff.max(axis=1)))\n",
    "        max_err = float(diff[worst_row].max())\n",
    "        print(f\"FAILED ✗  max error: {max_err:.6f} at row {worst_row}\")\n",
    "        g_boundaries = group_offsets.tolist()\n",
    "        print(f\"  Group boundaries at rows: {g_boundaries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468fd8f",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Key pattern</summary>\n",
    "\n",
    "```python\n",
    "group_id = group_ids_ref[grid_id]\n",
    "m_tile = m_tile_ids_ref[grid_id]\n",
    "group_start = group_offsets_ref[group_id]\n",
    "group_end = group_offsets_ref[group_id + 1]\n",
    "tile_start = m_tile * bm\n",
    "\n",
    "# Build a (bm, N) mask where row_index in [group_start, group_end)\n",
    "# Tip: jax.lax.broadcasted_iota(dtype, shape, dimension)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "group_id = group_ids_ref[grid_id]\n",
    "m_tile = m_tile_ids_ref[grid_id]\n",
    "group_start = group_offsets_ref[group_id]\n",
    "group_end = group_offsets_ref[group_id + 1]\n",
    "tile_start = m_tile * bm\n",
    "\n",
    "row_ids = tile_start + jax.lax.broadcasted_iota(jnp.int32, (bm, N), 0)\n",
    "mask = (row_ids >= group_start) & (row_ids < group_end)\n",
    "\n",
    "o_ref[...] = jnp.where(mask, x_ref[...], o_ref[...])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d88fc",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 4: Configure Your Own Scalar-Prefetch `pallas_call`\n",
    "\n",
    "**Goal**: Given the working kernel from Puzzle 3, write the **entire**\n",
    "`pl.pallas_call` invocation from scratch.\n",
    "\n",
    "### Theory\n",
    "\n",
    "You need to understand:\n",
    "- **`num_scalar_prefetch=3`**: the first 3 call args (`group_offsets`,\n",
    "  `group_ids`, `m_tile_ids`) are prefetched to SMEM. They appear as\n",
    "  leading refs in the kernel signature.\n",
    "- **Index map signature**: each index map gets the grid index first,\n",
    "  then all prefetch refs. E.g.,\n",
    "  `lambda i, go, gi, mt: (mt[i], 0)` — `i` is the grid index,\n",
    "  `go/gi/mt` are the three scalar-prefetched refs.\n",
    "- **Grid size**: `(num_tiles,)` — iterates over tile visits, not\n",
    "  just `tiles_m`.\n",
    "- **Call argument order**: scalar-prefetched args come first, then\n",
    "  regular inputs:\n",
    "  `(group_offsets, group_ids, m_tile_ids, x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 1024, 64\n",
    "bm = 128\n",
    "G = 3\n",
    "\n",
    "group_sizes = jnp.array([300, 212, 512], dtype=jnp.int32)\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata(group_sizes, M, bm)\n",
    "\n",
    "# The kernel is provided (solved from Puzzle 3):\n",
    "def masked_copy_kernel_solved(group_offsets_ref, group_ids_ref, m_tile_ids_ref,\n",
    "                               x_ref, o_ref):\n",
    "    \"\"\"Copy rows from x to output, masked by group boundaries.\"\"\"\n",
    "    grid_id = pl.program_id(0)\n",
    "    group_id = group_ids_ref[grid_id]\n",
    "    m_tile = m_tile_ids_ref[grid_id]\n",
    "    group_start = group_offsets_ref[group_id]\n",
    "    group_end = group_offsets_ref[group_id + 1]\n",
    "    tile_start = m_tile * bm\n",
    "\n",
    "    row_ids = tile_start + jax.lax.broadcasted_iota(jnp.int32, (bm, N), 0)\n",
    "    mask = (row_ids >= group_start) & (row_ids < group_end)\n",
    "    o_ref[...] = jnp.where(mask, x_ref[...], o_ref[...])\n",
    "\n",
    "# Reference spec\n",
    "def masked_copy_spec4(x, group_offsets, group_ids, m_tile_ids):\n",
    "    out = jnp.zeros_like(x)\n",
    "    for grid_id in range(num_tiles):\n",
    "        g = int(group_ids[grid_id])\n",
    "        tile_id = int(m_tile_ids[grid_id])\n",
    "        g_start = int(group_offsets[g])\n",
    "        g_end = int(group_offsets[g + 1])\n",
    "        t_start = tile_id * bm\n",
    "        t_end = t_start + bm\n",
    "        for row in range(t_start, t_end):\n",
    "            if g_start <= row < g_end:\n",
    "                out = out.at[row].set(x[row])\n",
    "    return out\n",
    "\n",
    "# --- Tests ---\n",
    "x = jax.random.normal(jax.random.key(26), (M, N))\n",
    "expected = masked_copy_spec4(x, group_offsets, group_ids, m_tile_ids)\n",
    "\n",
    "# YOUR TASK: Write the complete pl.pallas_call invocation.\n",
    "# Replace `None` with your working code.\n",
    "#\n",
    "# You need:\n",
    "# - PrefetchScalarGridSpec with num_scalar_prefetch=3\n",
    "# - in_specs: BlockSpec that uses m_tile_ids to route tiles\n",
    "# - out_specs: BlockSpec that uses m_tile_ids to route tiles\n",
    "# - grid=(num_tiles,)\n",
    "# - Call args: (group_offsets, group_ids, m_tile_ids, x) — scalar prefetch first!\n",
    "actual = None  # Replace with pl.pallas_call(...)(...) invocation\n",
    "\n",
    "# --- Tests ---\n",
    "if actual is not None and jnp.allclose(actual, expected, atol=1e-5):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    print(\"FAILED ✗  (fill in the cell above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef136a9",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Structure</summary>\n",
    "\n",
    "```python\n",
    "actual = pl.pallas_call(\n",
    "    masked_copy_kernel_solved,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=3,\n",
    "        in_specs=[pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0))],\n",
    "        out_specs=pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0)),\n",
    "        grid=(num_tiles,),\n",
    "    ),\n",
    "    out_shape=...,\n",
    "    interpret=True,\n",
    ")(...)  # scalar-prefetched args first, then regular inputs\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "actual = pl.pallas_call(\n",
    "    masked_copy_kernel_solved,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=3,\n",
    "        in_specs=[pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0))],\n",
    "        out_specs=pl.BlockSpec((bm, N), lambda i, go, gi, mt: (mt[i], 0)),\n",
    "        grid=(num_tiles,),\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_offsets, group_ids, m_tile_ids, x)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de73676",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 5: Simple Grouped Matmul — Equal, Tile-Aligned Groups\n",
    "\n",
    "**Goal**: Implement grouped matmul for the simplest case: all groups have\n",
    "equal size and group sizes are divisible by the tile size.\n",
    "\n",
    "### Theory\n",
    "\n",
    "This is the \"easy mode\" grouped matmul. With equal, tile-aligned groups:\n",
    "- No partial tiles (every tile belongs to exactly one group)\n",
    "- `group_ids` is a simple repeat: `[0,0,1,1,2,2,3,3]`\n",
    "- `m_tile_ids` = `[0,1,2,3,4,5,6,7]` (just sequential)\n",
    "- No masking needed on stores\n",
    "\n",
    "**Grid**: `(tiles_n, num_tiles, tiles_k)`\n",
    "- `tiles_n`: N dimension (parallel — independent output columns)\n",
    "- `num_tiles`: M tiles across all groups (may revisit same tile)\n",
    "- `tiles_k`: K reduction dimension (accumulates)\n",
    "\n",
    "N is outermost because all N tiles are independent — this matters for\n",
    "pipelining later.\n",
    "\n",
    "**Index maps** (provided — study them!):\n",
    "- `lhs_imap`: `(n_i, grid_id, k_i) -> (m_tile_ids[grid_id], k_i)`\n",
    "- `rhs_imap`: `(n_i, grid_id, k_i) -> (group_ids[grid_id], k_i, n_i)`\n",
    "- `out_imap`: `(n_i, grid_id, k_i) -> (m_tile_ids[grid_id], n_i)`\n",
    "\n",
    "The `group_ids` lookup in `rhs_imap` routes each tile to the correct\n",
    "group's weight matrix.\n",
    "\n",
    "**`num_scalar_prefetch=2`**: metadata packed as a tuple\n",
    "`(group_offsets, group_ids, m_tile_ids)` in slot 1, sharding offset\n",
    "`[0]` in slot 2. The sharding offset is used for multi-device setups;\n",
    "for single-device it's always `[0]`.\n",
    "\n",
    "**Recall from basics.py Puzzle 8**: the zero/accumulate/store pattern:\n",
    "```python\n",
    "@pl.when(k_i == 0)           # ZERO on first K tile\n",
    "def _(): acc[...] = zeros\n",
    "\n",
    "acc[...] += a @ b             # ACCUMULATE on every tile\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1) # STORE on last K tile\n",
    "def _(): out[...] = acc[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 4\n",
    "M, K, N = 512, 256, 128\n",
    "bm, bk, bn = 128, 128, 128\n",
    "\n",
    "group_sizes = jnp.array([M // G] * G, dtype=jnp.int32)\n",
    "tiles_k = K // bk\n",
    "tiles_n = N // bn\n",
    "\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata(group_sizes, M, bm)\n",
    "\n",
    "# --- Reference ---\n",
    "def simple_gmm_spec(lhs, rhs, group_sizes):\n",
    "    \"\"\"lhs: (M, K), rhs: (G, K, N), group_sizes: (G,) -> (M, N)\"\"\"\n",
    "    offsets = jnp.concatenate([jnp.array([0]), jnp.cumsum(group_sizes)])\n",
    "    out = jnp.zeros((lhs.shape[0], rhs.shape[2]), dtype=jnp.float32)\n",
    "    for g in range(len(group_sizes)):\n",
    "        s, e = int(offsets[g]), int(offsets[g + 1])\n",
    "        out = out.at[s:e].set(lhs[s:e] @ rhs[g])\n",
    "    return out\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def simple_gmm_kernel(group_metadata_ref, group_offset_ref,\n",
    "                      lhs_ref, rhs_ref, o_ref, acc_ref):\n",
    "    # group_metadata_ref: (group_offsets, group_ids, m_tile_ids) in SMEM\n",
    "    # group_offset_ref: unused here (for sharding), always [0]\n",
    "    # lhs_ref: (bm, bk) — tile of lhs\n",
    "    # rhs_ref: (bk, bn) — tile of rhs (group dim squeezed by None)\n",
    "    # o_ref: (bm, bn) — output tile\n",
    "    # acc_ref: (bm, bn) — scratch accumulator (VMEM on TPU)\n",
    "    grid_id = pl.program_id(1)\n",
    "    k_i = pl.program_id(2)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Zero accumulator on first K tile\n",
    "    # 2. Accumulate tile matmul\n",
    "    # 3. Store result on last K tile\n",
    "\n",
    "# --- Tests ---\n",
    "lhs = jax.random.normal(jax.random.key(30), (M, K))\n",
    "rhs = jax.random.normal(jax.random.key(31), (G, K, N))\n",
    "expected = simple_gmm_spec(lhs, rhs, group_sizes)\n",
    "\n",
    "group_metadata = (group_offsets, group_ids, m_tile_ids)\n",
    "group_offset = jnp.array([0], dtype=jnp.int32)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    simple_gmm_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=2,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((bm, bk), lhs_imap),\n",
    "            pl.BlockSpec((None, bk, bn), rhs_imap),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((bm, bn), out_imap),\n",
    "        grid=(tiles_n, num_tiles, tiles_k),\n",
    "        scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_metadata, group_offset, lhs, rhs)\n",
    "\n",
    "if jnp.allclose(actual, expected, atol=1e-2, rtol=1e-2):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    max_err = float(jnp.max(jnp.abs(actual - expected)))\n",
    "    print(f\"FAILED ✗  max error: {max_err:.6f}\")\n",
    "    print(f\"  Expected[:2,:4]:\\n{expected[:2,:4]}\")\n",
    "    print(f\"  Actual[:2,:4]:\\n{actual[:2,:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bdd41",
   "metadata": {},
   "source": [
    "> **AHA moment**: The kernel body you just wrote is identical to basics.py\n",
    "> Puzzle 8. All the complexity of grouped matmul — variable group sizes,\n",
    "> tile routing, boundary handling — lives in the *metadata* and *index\n",
    "> maps*. The kernel itself is oblivious to groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c766db4",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — Approach</summary>\n",
    "\n",
    "The kernel body is identical to basics.py Puzzle 8: zero / accumulate / store with `@pl.when`. The index maps (already provided) handle all the group-to-tile routing via `group_ids` and `m_tile_ids`. With `None` in the rhs BlockSpec, the group dimension is squeezed — `rhs_ref` is just `(bk, bn)`.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "\n",
    "acc_ref[...] += lhs_ref[...] @ rhs_ref[...]\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    o_ref[...] = acc_ref[...]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7a5c3",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 6: Full Ragged Dot — Variable Group Sizes\n",
    "\n",
    "**Goal**: Handle **variable group sizes** where tiles can straddle group\n",
    "boundaries. This is the real grouped matmul.\n",
    "\n",
    "### Theory\n",
    "\n",
    "The only difference from Puzzle 5: when groups are unequal, a tile may\n",
    "be visited **multiple times** (once per group it straddles). On each visit,\n",
    "the kernel must **mask** the store so only rows belonging to the current\n",
    "group are written.\n",
    "\n",
    "`make_group_metadata` handles all the complexity — the `group_ids` and\n",
    "`m_tile_ids` arrays already encode the repeated visits. The kernel just\n",
    "needs to add the mask at store time using `get_store_mask` from Puzzle 3:\n",
    "\n",
    "```python\n",
    "mask = get_store_mask(grid_id, group_offsets, group_ids, m_tile_ids, bm, bn)\n",
    "o_ref[...] = jnp.where(mask, acc[...], o_ref[...])\n",
    "```\n",
    "\n",
    "Why `o_ref[...]` in the else branch? Boundary tiles are visited twice —\n",
    "the first visit writes some rows, and the second visit must preserve\n",
    "those rows while writing others.\n",
    "\n",
    "```\n",
    "Tile at row 256, bm=128:\n",
    "+------------------------+\n",
    "| rows 256-299: group 0  | <- Visit 1: mask=True for rows 256-299\n",
    "| rows 300-383: group 1  | <- Visit 2: mask=True for rows 300-383\n",
    "+------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "M, K, N = 1024, 256, 128\n",
    "bm, bk, bn = 128, 128, 128\n",
    "\n",
    "group_sizes = jnp.array([300, 212, 512], dtype=jnp.int32)\n",
    "tiles_k = K // bk\n",
    "tiles_n = N // bn\n",
    "\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata(group_sizes, M, bm)\n",
    "\n",
    "print(f\"M={M}, G={G}, group_sizes={group_sizes.tolist()}\")\n",
    "print(f\"num_tiles={num_tiles} (vs {M//bm} base tiles)\")\n",
    "print(f\"group_ids[:num_tiles]={group_ids[:num_tiles].tolist()}\")\n",
    "print(f\"m_tile_ids[:num_tiles]={m_tile_ids[:num_tiles].tolist()}\")\n",
    "\n",
    "# --- Reference ---\n",
    "def ragged_dot_spec(lhs, rhs, group_sizes):\n",
    "    \"\"\"Same as jax.lax.ragged_dot but explicit for clarity.\"\"\"\n",
    "    offsets = jnp.concatenate([jnp.array([0]), jnp.cumsum(group_sizes)])\n",
    "    out = jnp.zeros((lhs.shape[0], rhs.shape[2]), dtype=jnp.float32)\n",
    "    for g in range(len(group_sizes)):\n",
    "        s, e = int(offsets[g]), int(offsets[g + 1])\n",
    "        if s < e:\n",
    "            out = out.at[s:e].set(lhs[s:e] @ rhs[g])\n",
    "    return out\n",
    "\n",
    "# --- Kernel skeleton ---\n",
    "def ragged_dot_kernel(group_metadata_ref, group_offset_ref,\n",
    "                      lhs_ref, rhs_ref, o_ref, acc_ref):\n",
    "    group_offsets, group_ids, m_tile_ids = group_metadata_ref\n",
    "    grid_id = pl.program_id(1)\n",
    "    k_i = pl.program_id(2)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Same as Puzzle 5, but on the last K tile, apply a masked store\n",
    "    # so only rows belonging to the current group are written.\n",
    "    # Use get_store_mask(grid_id, group_offsets, group_ids, m_tile_ids, bm, bn)\n",
    "\n",
    "# --- Tests ---\n",
    "lhs = jax.random.normal(jax.random.key(40), (M, K))\n",
    "rhs = jax.random.normal(jax.random.key(41), (G, K, N))\n",
    "expected = ragged_dot_spec(lhs, rhs, group_sizes)\n",
    "\n",
    "group_metadata = (group_offsets, group_ids, m_tile_ids)\n",
    "group_offset = jnp.array([0], dtype=jnp.int32)\n",
    "\n",
    "actual = pl.pallas_call(\n",
    "    ragged_dot_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=2,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((bm, bk), lhs_imap),\n",
    "            pl.BlockSpec((None, bk, bn), rhs_imap),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((bm, bn), out_imap),\n",
    "        grid=(tiles_n, num_tiles, tiles_k),\n",
    "        scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_metadata, group_offset, lhs, rhs)\n",
    "\n",
    "total_rows = int(group_sizes.sum())\n",
    "if jnp.allclose(actual[:total_rows], expected[:total_rows], atol=1e-2, rtol=1e-2):\n",
    "    print(f\"PASSED ✓  (shape={actual.shape})\")\n",
    "    print(f\"  Verified {total_rows} active rows\")\n",
    "else:\n",
    "    max_err = float(jnp.max(jnp.abs(actual[:total_rows] - expected[:total_rows])))\n",
    "    print(f\"FAILED ✗  max error: {max_err:.6f}\")\n",
    "\n",
    "# Test with equal groups too (masked kernel is a strict generalization)\n",
    "equal_sizes = jnp.array([256, 256, 512], dtype=jnp.int32)\n",
    "(go2, gi2, mt2), nt2 = make_group_metadata(equal_sizes, M, bm)\n",
    "expected2 = ragged_dot_spec(lhs, rhs, equal_sizes)\n",
    "gm2 = (go2, gi2, mt2)\n",
    "\n",
    "actual2 = pl.pallas_call(\n",
    "    ragged_dot_kernel,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=2,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((bm, bk), lhs_imap),\n",
    "            pl.BlockSpec((None, bk, bn), rhs_imap),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((bm, bn), out_imap),\n",
    "        grid=(tiles_n, nt2, tiles_k),\n",
    "        scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(gm2, group_offset, lhs, rhs)\n",
    "\n",
    "if jnp.allclose(actual2, expected2, atol=1e-2, rtol=1e-2):\n",
    "    print(f\"  Equal groups also PASSED ✓  (strict generalization)\")\n",
    "else:\n",
    "    max_err2 = float(jnp.max(jnp.abs(actual2 - expected2)))\n",
    "    print(f\"  Equal groups FAILED ✗  max error: {max_err2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1941f",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 2 — The masked store block</summary>\n",
    "\n",
    "```python\n",
    "# Steps 1-2 are the same as Puzzle 5 (zero + accumulate)\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    mask = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                          m_tile_ids, bm, bn)\n",
    "    acc = acc_ref[...]\n",
    "    o_ref[...] = jnp.where(mask, acc, o_ref[...].astype(acc.dtype))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 2 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "@pl.when(k_i == 0)\n",
    "def _zero():\n",
    "    acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "\n",
    "acc_ref[...] += lhs_ref[...] @ rhs_ref[...]\n",
    "\n",
    "@pl.when(k_i == tiles_k - 1)\n",
    "def _store():\n",
    "    mask = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                          m_tile_ids, bm, bn)\n",
    "    acc = acc_ref[...]\n",
    "    o_ref[...] = jnp.where(mask, acc, o_ref[...].astype(acc.dtype))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0765e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part III: Putting It All Together (Puzzle 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501f320",
   "metadata": {},
   "source": [
    "---\n",
    "## Puzzle 7: Configure Your Own Grouped Matmul\n",
    "\n",
    "**Goal**: Capstone puzzle — write the complete `pl.pallas_call(...)` invocation\n",
    "for grouped matmul. The kernel from Puzzle 6 is provided.\n",
    "\n",
    "### Theory\n",
    "\n",
    "You need to assemble all the pieces:\n",
    "- `PrefetchScalarGridSpec` with `num_scalar_prefetch=2`\n",
    "- `grid=(tiles_n, num_tiles, tiles_k)` — 3D grid\n",
    "- `in_specs`: `lhs` BlockSpec with `m_tile_ids` routing, `rhs` BlockSpec\n",
    "  with `group_ids` routing and `None` dimension squeeze\n",
    "- `out_specs`: BlockSpec with `m_tile_ids` routing\n",
    "- `scratch_shapes`: VMEM accumulator `(bm, bn)`\n",
    "- `out_shape`: full output shape `(M, N)`\n",
    "- Correct argument order: `(group_metadata, group_offset, lhs, rhs)`\n",
    "\n",
    "The index maps are the same shared `lhs_imap`, `rhs_imap`, `out_imap`\n",
    "from the utilities section. Study them — they show exactly how grid\n",
    "indices and scalar-prefetched metadata combine to route tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 3\n",
    "M, K, N = 1024, 256, 128\n",
    "bm, bk, bn = 128, 128, 128\n",
    "tiles_k = K // bk\n",
    "tiles_n = N // bn\n",
    "\n",
    "# The kernel is provided (solved from Puzzle 6):\n",
    "def gmm_kernel_solved(group_metadata_ref, group_offset_ref,\n",
    "                      lhs_ref, rhs_ref, o_ref, acc_ref):\n",
    "    group_offsets, group_ids, m_tile_ids = group_metadata_ref\n",
    "    grid_id = pl.program_id(1)\n",
    "    k_i = pl.program_id(2)\n",
    "\n",
    "    @pl.when(k_i == 0)\n",
    "    def _zero():\n",
    "        acc_ref[...] = jnp.zeros((bm, bn), dtype=jnp.float32)\n",
    "\n",
    "    acc_ref[...] += lhs_ref[...] @ rhs_ref[...]\n",
    "\n",
    "    @pl.when(k_i == tiles_k - 1)\n",
    "    def _store():\n",
    "        mask = get_store_mask(grid_id, group_offsets, group_ids,\n",
    "                              m_tile_ids, bm, bn)\n",
    "        acc = acc_ref[...]\n",
    "        o_ref[...] = jnp.where(mask, acc, o_ref[...].astype(acc.dtype))\n",
    "\n",
    "# --- Test with variable group sizes ---\n",
    "group_sizes = jnp.array([300, 212, 512], dtype=jnp.int32)\n",
    "(group_offsets, group_ids, m_tile_ids), num_tiles = \\\n",
    "    make_group_metadata(group_sizes, M, bm)\n",
    "\n",
    "lhs = jax.random.normal(jax.random.key(50), (M, K))\n",
    "rhs = jax.random.normal(jax.random.key(51), (G, K, N))\n",
    "expected = grouped_matmul_spec(lhs, rhs, group_sizes)\n",
    "\n",
    "group_metadata = (group_offsets, group_ids, m_tile_ids)\n",
    "group_offset = jnp.array([0], dtype=jnp.int32)\n",
    "\n",
    "# YOUR TASK: Write the complete pl.pallas_call invocation.\n",
    "# Replace `None` with your working code.\n",
    "actual = None  # Replace with pl.pallas_call(...)(...) invocation\n",
    "\n",
    "# --- Tests ---\n",
    "total_rows = int(group_sizes.sum())\n",
    "if actual is not None and jnp.allclose(actual[:total_rows], expected[:total_rows], atol=1e-2, rtol=1e-2):\n",
    "    print(f\"Variable groups: PASSED ✓  (shape={actual.shape})\")\n",
    "else:\n",
    "    print(\"Variable groups: FAILED ✗  (fill in the cell above)\")\n",
    "\n",
    "# Test with equal groups\n",
    "equal_sizes = jnp.array([256, 512, 256], dtype=jnp.int32)\n",
    "(go2, gi2, mt2), nt2 = make_group_metadata(equal_sizes, M, bm)\n",
    "expected2 = grouped_matmul_spec(lhs, rhs, equal_sizes)\n",
    "gm2 = (go2, gi2, mt2)\n",
    "\n",
    "# Re-run with equal groups (copy your pallas_call, updating metadata + grid)\n",
    "actual2 = None  # Replace with pl.pallas_call(...)(...) invocation\n",
    "\n",
    "if actual2 is not None and jnp.allclose(actual2, expected2, atol=1e-2, rtol=1e-2):\n",
    "    print(f\"Equal groups:    PASSED ✓\")\n",
    "else:\n",
    "    print(\"Equal groups:    FAILED ✗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c12259",
   "metadata": {},
   "source": [
    "<details><summary>Hint 1 of 3 — Grid and scalar prefetch</summary>\n",
    "\n",
    "```python\n",
    "grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "    num_scalar_prefetch=2,\n",
    "    grid=(tiles_n, num_tiles, tiles_k),\n",
    "    ...\n",
    ")\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2 of 3 — in_specs and out_specs</summary>\n",
    "\n",
    "```python\n",
    "in_specs=[\n",
    "    pl.BlockSpec((bm, bk), lhs_imap),       # lhs: route by m_tile_ids\n",
    "    pl.BlockSpec((None, bk, bn), rhs_imap),  # rhs: route by group_ids, squeeze group dim\n",
    "],\n",
    "out_specs=pl.BlockSpec((bm, bn), out_imap),  # out: route by m_tile_ids\n",
    "scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3 of 3 — Full solution</summary>\n",
    "\n",
    "```python\n",
    "actual = pl.pallas_call(\n",
    "    gmm_kernel_solved,\n",
    "    grid_spec=pltpu.PrefetchScalarGridSpec(\n",
    "        num_scalar_prefetch=2,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((bm, bk), lhs_imap),\n",
    "            pl.BlockSpec((None, bk, bn), rhs_imap),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((bm, bn), out_imap),\n",
    "        grid=(tiles_n, num_tiles, tiles_k),\n",
    "        scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],\n",
    "    ),\n",
    "    out_shape=jax.ShapeDtypeStruct((M, N), jnp.float32),\n",
    "    interpret=True,\n",
    ")(group_metadata, group_offset, lhs, rhs)\n",
    "```\n",
    "\n",
    "For the equal-groups test, replace `group_metadata` with `gm2`,\n",
    "`num_tiles` with `nt2` in the grid.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2ea1a",
   "metadata": {},
   "source": [
    "---\n",
    "# What's Next\n",
    "\n",
    "This notebook covered the forward-pass grouped matmul — the core of MoE\n",
    "dispatch. Production MoE kernels add several more layers:\n",
    "\n",
    "1. **Backward pass (tgmm)**: Transpose grouped matmul computes gradients\n",
    "   w.r.t. expert weights. Accumulation is over M tiles (group rows) instead\n",
    "   of K tiles. Uses prologue/epilogue detection for group transitions.\n",
    "\n",
    "2. **Software pipelining (`emit_pipeline`)**: Overlaps async DMA with\n",
    "   compute on TPU. Double-buffered loading hides HBM latency.\n",
    "   `dimension_semantics` tells the compiler which axes can be reordered.\n",
    "\n",
    "3. **Production implementations**: The kernel pattern from this notebook\n",
    "   maps directly to the [tokamax](https://github.com/jax-ml/jax-triton)\n",
    "   `gmm` implementation (TPU) and\n",
    "   [MegaBlocks](https://github.com/databricks/megablocks) (GPU)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
